{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":403,"status":"ok","timestamp":1647217664245,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"},"user_tz":420},"id":"NRc61dK9pUNa","outputId":"3161a6f1-0366-4705-b0c9-f7964a708de9","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mon Mar 14 00:27:43 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0    24W / 300W |      0MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["#@title check GPU type\n","!nvidia-smi"]},{"cell_type":"code","execution_count":1,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1623,"status":"ok","timestamp":1647551747416,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"},"user_tz":420},"id":"yuD0Fd14k1B7","outputId":"8b8609b3-e581-4759-f1bc-1db2b035d143"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#@title Mount google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4033,"status":"ok","timestamp":1647551751710,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"},"user_tz":420},"id":"9fwC17Oyis-e","outputId":"c39cdcff-b498-4dae-c40d-40c5a0b9aaa4","cellView":"form"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Proj4_DPI/scripts/model1_MolTrans\n"]}],"source":["#@title import libraries\n","import torch\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch.utils import data\n","from torch import nn \n","import copy\n","\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from time import time\n","\n","from sklearn import metrics\n","from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, roc_curve, confusion_matrix, precision_score, recall_score, auc\n","from sklearn.model_selection import KFold, train_test_split\n","torch.manual_seed(2)    # reproducible torch:2 np:3\n","np.random.seed(3)\n","\n","%cd /content/drive/MyDrive/Proj4_DPI/scripts/model1_MolTrans/\n","from config import BIN_config_DBPE\n","from models import BIN_Interaction_Flat\n","from stream import BIN_Data_Encoder\n","\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"CDy2SpNhis-i","executionInfo":{"status":"ok","timestamp":1647551779877,"user_tz":420,"elapsed":596,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}}},"outputs":[],"source":["#@title define functions\n","def test(data_generator, model, use_cuda):\n","    y_pred = []\n","    y_label = []\n","    model.eval()\n","    loss_accumulate = 0.0\n","    count = 0.0\n","    for i, (d, p, d_mask, p_mask, label) in enumerate(data_generator):\n","\n","        label = Variable(torch.from_numpy(np.array(label)).float())\n","        if use_cuda:\n","            d = d.cuda()\n","            p = p.cuda()\n","            d_mask = d_mask.cuda()\n","            p_mask = p_mask.cuda()\n","            label = label.cuda()\n","        \n","        score = model(d, p, d_mask, p_mask)\n","        \n","        m = torch.nn.Sigmoid()\n","        logits = torch.squeeze(m(score))\n","        loss_fct = torch.nn.BCELoss()           \n","\n","        loss = loss_fct(logits, label)\n","        \n","        loss_accumulate += loss\n","        count += 1\n","        \n","        logits = logits.detach().cpu().numpy()\n","        \n","        label_ids = label.to('cpu').numpy()\n","        y_label = y_label + label_ids.flatten().tolist()\n","        y_pred = y_pred + logits.flatten().tolist()\n","        \n","    loss = loss_accumulate/count\n","    \n","    fpr, tpr, thresholds = roc_curve(y_label, y_pred)\n","\n","    precision = tpr / (tpr + fpr)\n","\n","    f1 = 2 * precision * tpr / (tpr + precision + 0.00001)\n","\n","    # thred_optim = thresholds[5:][np.argmax(f1[5:])]\n","    thred_optim = 0.5\n","\n","    print(\"optimal threshold: \" + str(thred_optim))\n","\n","    # y_pred_s = [1 if i else 0 for i in (y_pred >= thred_optim)]\n","    y_pred_s = [1 if i>thred_optim else 0 for i in y_pred]\n","\n","    auc_k = metrics.auc(fpr, tpr)\n","    print(\"AUROC:\" + str(auc_k))\n","    print(\"AUPRC: \"+ str(average_precision_score(y_label, y_pred)))\n","\n","    cm1 = confusion_matrix(y_label, y_pred_s)\n","    print('Confusion Matrix : \\n', cm1)\n","    print('Recall : ', recall_score(y_label, y_pred_s))\n","    print('Precision : ', precision_score(y_label, y_pred_s))\n","\n","    total1=sum(sum(cm1))\n","    #####from confusion matrix calculate accuracy\n","    accuracy1=(cm1[0,0]+cm1[1,1])/total1\n","    print ('Accuracy : ', accuracy1)\n","\n","    sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n","    print('Sensitivity : ', sensitivity1 )\n","\n","    specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n","    print('Specificity : ', specificity1)\n","\n","    outputs = np.asarray([1 if i else 0 for i in (np.asarray(y_pred) >= 0.5)])\n","    return accuracy1, roc_auc_score(y_label, y_pred), average_precision_score(y_label, y_pred), f1_score(y_label, outputs), y_pred, loss.item()\n","\n","\n","def main(use_cuda):\n","    config = BIN_config_DBPE()\n","    \n","    lr = config['learning_rate']\n","    # lr = 5e-5\n","    BATCH_SIZE = config['batch_size']\n","    train_epoch = config['train_epoch']\n","    accumulation_steps = config['grad_accumul_steps']\n","    max_d = config['max_dna_seq']\n","    max_p = config['max_protein_seq']\n","    \n","    print(\"learning rate:\",lr)\n","    print(\"batch size:\",BATCH_SIZE)\n","    print(\"training epoch:\",train_epoch)\n","    print(\"accumulation steps:\", accumulation_steps)\n","\n","    loss_history = []\n","    loss_history_val = []\n","    acc_train = []\n","    acc_val = []\n","    \n","    model = BIN_Interaction_Flat(**config)\n","    \n","    if use_cuda:\n","        model = model.cuda()\n","\n","    if torch.cuda.device_count() > 1:\n","        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","        model = nn.DataParallel(model, dim = 0)\n","    elif torch.cuda.device_count() < 1:\n","        print(\"Let's use cpu!\")\n","            \n","    opt = torch.optim.Adam(model.parameters(), lr = lr)\n","    #opt = torch.optim.SGD(model.parameters(), lr = lr, momentum=0.9)\n","    \n","    print('--- Data Preparation ---')\n","    \n","    params = {'batch_size': BATCH_SIZE,\n","              'shuffle': True,\n","              'num_workers': 6, \n","              'drop_last': True}\n","\n","    dataFolder = '/content/drive/MyDrive/Proj4_DPI/data/data_with_embedding/'\n","    # df_train = pd.read_pickle(dataFolder + '/train_loc0.pkl')\n","    # df_train = df_train[0:10000]\n","    # df_val = pd.read_pickle(dataFolder + '/val_loc0.pkl')\n","    # df_val = df_val[0:2000]\n","    # df_test = pd.read_pickle(dataFolder + '/test_loc0.pkl')\n","    # df = pd.read_pickle(dataFolder + '/JUNB_test.pkl')\n","    df = pd.read_pickle(dataFolder + '/AP2A_train.pkl')\n","    \n","    df_train, df_val = train_test_split(df, test_size=0.2)\n","    print(\"training size: \", df_train.shape[0])\n","    print(\"validation size: \", df_val.shape[0])\n","\n","    training_set = BIN_Data_Encoder(np.array([i for i in range(df_train.shape[0])]), df_train.label.values, df_train, max_d, max_p)\n","    training_generator = data.DataLoader(training_set, **params)\n","\n","    validation_set = BIN_Data_Encoder(np.array([i for i in range(df_val.shape[0])]), df_val.label.values, df_val, max_d, max_p)\n","    validation_generator = data.DataLoader(validation_set, **params)\n","    \n","    # testing_set = BIN_Data_Encoder(np.array([i for i in range(df_test.shape[0])]), df_test.label.values, df_test, max_d, max_p)\n","    # testing_generator = data.DataLoader(testing_set, **params)\n","    \n","    # early stopping\n","    max_auc = 0\n","    model_max = copy.deepcopy(model)\n","    \n","    print('--- Go for Training ---')\n","    torch.backends.cudnn.benchmark = True\n","    for epo in range(train_epoch):\n","        model.train()\n","        for i, (d, p, d_mask, p_mask, label) in enumerate(training_generator):\n","            \n","            label = Variable(torch.from_numpy(np.array(label)).float())\n","            if use_cuda:\n","                d = d.cuda()\n","                p = p.cuda()\n","                d_mask = d_mask.cuda()\n","                p_mask = p_mask.cuda()\n","                label = label.cuda()\n","\n","            \n","            score = model(d, p, d_mask, p_mask)\n","            loss_fct = torch.nn.BCELoss()\n","            m = torch.nn.Sigmoid()\n","            n = torch.squeeze(m(score))\n","            # print(d.isnan().any())\n","            # print(p.isnan().any())\n","            # print(p)\n","            \n","            loss = loss_fct(n, label)\n","            loss_history.append(loss)\n","            loss.backward()\n","\n","            if ((i+1) % accumulation_steps == 0):\n","              opt.step()\n","              opt.zero_grad()\n","            \n","            if ((i+1) % 10 == 0):\n","                print('Training at Epoch ' + str(epo + 1) + ' iteration ' + str(i+1) + ' with loss ' + str(loss.cpu().detach().numpy()))\n","            \n","        # every epoch test\n","        with torch.set_grad_enabled(False):\n","            accuracy, auc, auprc, f1, logits, loss = test(validation_generator, model, use_cuda)\n","            loss_history_val.append(loss)\n","            acc_val.append(accuracy)\n","            if auc > max_auc:\n","                model_max = copy.deepcopy(model)\n","                max_auc = auc\n","                # # save the model\n","                # save_path = './epoch' + str(epo+1) + '.pt'\n","                # torch.save(model.state_dict(), save_path)\n","                # print('Saving model to',save_path)\n","\n","            print('Validation at Epoch '+ str(epo + 1) + ' , AUROC: '+ str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Val loss: '+str(loss) + ' , Accuracy: '+str(accuracy))\n","            accuracy, auc, auprc, f1, logits, loss = test(training_generator, model, use_cuda)\n","            acc_train.append(accuracy)\n","            print('Training at Epoch '+ str(epo + 1) + ' , AUROC: '+ str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Training loss: '+str(loss) + ' , Accuracy: '+str(accuracy))\n","    \n","    # print('--- Go for Testing ---')\n","    # try:\n","    #     with torch.set_grad_enabled(False):\n","    #         auc, auprc, f1, logits, loss = test(testing_generator, model_max, use_cuda)\n","    #         print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Test loss: '+str(loss))\n","    # except:\n","    #     print('testing failed')\n","    return model_max, model, loss_history, loss_history_val, acc_train, acc_val"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"zWPwwfnNis-k","colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"13p9d7Q5lLy_6FG637nEkWKOLHWd1lkiR"},"executionInfo":{"status":"ok","timestamp":1647586840358,"user_tz":420,"elapsed":16413334,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"539bc7a2-8c50-4082-93ff-86e423559908"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["#@title run the model\n","# fold 1\n","s = time()\n","model_max, model_last, loss_history, loss_val, acc_train, acc_val = main(use_cuda)\n","e = time()\n","print(e-s)\n"]},{"cell_type":"code","source":["config = BIN_config_DBPE()\n","lr = config['learning_rate']\n","BATCH_SIZE = config['batch_size']\n","train_epoch = config['train_epoch']\n","accumulation_steps = config['grad_accumul_steps']\n","max_d = config['max_dna_seq']\n","max_p = config['max_protein_seq']\n","params = {'batch_size': BATCH_SIZE,\n","  'shuffle': True,\n","  'num_workers': 6, \n","  'drop_last': True}\n","\n","dataFolder = '/content/drive/MyDrive/Proj4_DPI/data/data_with_embedding/'\n","df_train = pd.read_pickle(dataFolder + '/train_loc0.pkl')[128:200]\n","training_set = BIN_Data_Encoder(np.array([i for i in range(df_train.shape[0])]), df_train.label.values, df_train, max_d, max_p)\n","training_generator = data.DataLoader(training_set, **params)\n","\n","try:\n","    with torch.set_grad_enabled(False):\n","        auc, auprc, f1, logits, loss = test(training_generator, model_max, use_cuda)\n","        print('Testing AUROC: ' + str(auc) + ' , AUPRC: ' + str(auprc) + ' , F1: '+str(f1) + ' , Test loss: '+str(loss))\n","except:\n","    print('testing failed')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Cqhx-z3osfS","executionInfo":{"status":"ok","timestamp":1647215653596,"user_tz":420,"elapsed":13314,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"99345892-d054-47ea-e41d-f63623decc0f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["optimal threshold: 0.5\n","AUROC:0.8204365079365079\n","AUPRC: 0.9034588637207163\n","Confusion Matrix : \n"," [[ 0 28]\n"," [ 0 36]]\n","Recall :  1.0\n","Precision :  0.5625\n","Accuracy :  0.5625\n","Sensitivity :  0.0\n","Specificity :  1.0\n","Testing AUROC: 0.8204365079365079 , AUPRC: 0.9034588637207163 , F1: 0.72 , Test loss: 3.117576837539673\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"]}]},{"cell_type":"code","source":["#@title total parameters\n","from prettytable import PrettyTable\n","\n","def count_parameters(model):\n","    table = PrettyTable([\"Modules\", \"Parameters\"])\n","    total_params = 0\n","    for name, parameter in model.named_parameters():\n","        if not parameter.requires_grad: continue\n","        params = parameter.numel()\n","        table.add_row([name, params])\n","        total_params+=params\n","    print(table)\n","    print(f\"Total Trainable Params: {total_params}\")\n","    return total_params\n","    \n","count_parameters(model_max)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dIL9D-xJywyC","executionInfo":{"status":"ok","timestamp":1647217218846,"user_tz":420,"elapsed":295,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"758bb0b3-b3e5-4311-d917-e31e623fcad3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+----------------------------------------------------+------------+\n","|                      Modules                       | Parameters |\n","+----------------------------------------------------+------------+\n","|               demb.layernorm.weight                |    768     |\n","|                demb.layernorm.bias                 |    768     |\n","|               demb.transform.weight                |   294912   |\n","|                demb.transform.bias                 |    384     |\n","|               pemb.layernorm.weight                |    384     |\n","|                pemb.layernorm.bias                 |    384     |\n","|               pemb.transform.weight                |   147456   |\n","|                pemb.transform.bias                 |    384     |\n","|   d_encoder.layer.0.attention.self.query.weight    |   147456   |\n","|    d_encoder.layer.0.attention.self.query.bias     |    384     |\n","|    d_encoder.layer.0.attention.self.key.weight     |   147456   |\n","|     d_encoder.layer.0.attention.self.key.bias      |    384     |\n","|   d_encoder.layer.0.attention.self.value.weight    |   147456   |\n","|    d_encoder.layer.0.attention.self.value.bias     |    384     |\n","|  d_encoder.layer.0.attention.output.dense.weight   |   147456   |\n","|   d_encoder.layer.0.attention.output.dense.bias    |    384     |\n","| d_encoder.layer.0.attention.output.LayerNorm.gamma |    384     |\n","| d_encoder.layer.0.attention.output.LayerNorm.beta  |    384     |\n","|    d_encoder.layer.0.intermediate.dense.weight     |   589824   |\n","|     d_encoder.layer.0.intermediate.dense.bias      |    1536    |\n","|       d_encoder.layer.0.output.dense.weight        |   589824   |\n","|        d_encoder.layer.0.output.dense.bias         |    384     |\n","|      d_encoder.layer.0.output.LayerNorm.gamma      |    384     |\n","|      d_encoder.layer.0.output.LayerNorm.beta       |    384     |\n","|   d_encoder.layer.1.attention.self.query.weight    |   147456   |\n","|    d_encoder.layer.1.attention.self.query.bias     |    384     |\n","|    d_encoder.layer.1.attention.self.key.weight     |   147456   |\n","|     d_encoder.layer.1.attention.self.key.bias      |    384     |\n","|   d_encoder.layer.1.attention.self.value.weight    |   147456   |\n","|    d_encoder.layer.1.attention.self.value.bias     |    384     |\n","|  d_encoder.layer.1.attention.output.dense.weight   |   147456   |\n","|   d_encoder.layer.1.attention.output.dense.bias    |    384     |\n","| d_encoder.layer.1.attention.output.LayerNorm.gamma |    384     |\n","| d_encoder.layer.1.attention.output.LayerNorm.beta  |    384     |\n","|    d_encoder.layer.1.intermediate.dense.weight     |   589824   |\n","|     d_encoder.layer.1.intermediate.dense.bias      |    1536    |\n","|       d_encoder.layer.1.output.dense.weight        |   589824   |\n","|        d_encoder.layer.1.output.dense.bias         |    384     |\n","|      d_encoder.layer.1.output.LayerNorm.gamma      |    384     |\n","|      d_encoder.layer.1.output.LayerNorm.beta       |    384     |\n","|   p_encoder.layer.0.attention.self.query.weight    |   147456   |\n","|    p_encoder.layer.0.attention.self.query.bias     |    384     |\n","|    p_encoder.layer.0.attention.self.key.weight     |   147456   |\n","|     p_encoder.layer.0.attention.self.key.bias      |    384     |\n","|   p_encoder.layer.0.attention.self.value.weight    |   147456   |\n","|    p_encoder.layer.0.attention.self.value.bias     |    384     |\n","|  p_encoder.layer.0.attention.output.dense.weight   |   147456   |\n","|   p_encoder.layer.0.attention.output.dense.bias    |    384     |\n","| p_encoder.layer.0.attention.output.LayerNorm.gamma |    384     |\n","| p_encoder.layer.0.attention.output.LayerNorm.beta  |    384     |\n","|    p_encoder.layer.0.intermediate.dense.weight     |   589824   |\n","|     p_encoder.layer.0.intermediate.dense.bias      |    1536    |\n","|       p_encoder.layer.0.output.dense.weight        |   589824   |\n","|        p_encoder.layer.0.output.dense.bias         |    384     |\n","|      p_encoder.layer.0.output.LayerNorm.gamma      |    384     |\n","|      p_encoder.layer.0.output.LayerNorm.beta       |    384     |\n","|   p_encoder.layer.1.attention.self.query.weight    |   147456   |\n","|    p_encoder.layer.1.attention.self.query.bias     |    384     |\n","|    p_encoder.layer.1.attention.self.key.weight     |   147456   |\n","|     p_encoder.layer.1.attention.self.key.bias      |    384     |\n","|   p_encoder.layer.1.attention.self.value.weight    |   147456   |\n","|    p_encoder.layer.1.attention.self.value.bias     |    384     |\n","|  p_encoder.layer.1.attention.output.dense.weight   |   147456   |\n","|   p_encoder.layer.1.attention.output.dense.bias    |    384     |\n","| p_encoder.layer.1.attention.output.LayerNorm.gamma |    384     |\n","| p_encoder.layer.1.attention.output.LayerNorm.beta  |    384     |\n","|    p_encoder.layer.1.intermediate.dense.weight     |   589824   |\n","|     p_encoder.layer.1.intermediate.dense.bias      |    1536    |\n","|       p_encoder.layer.1.output.dense.weight        |   589824   |\n","|        p_encoder.layer.1.output.dense.bias         |    384     |\n","|      p_encoder.layer.1.output.LayerNorm.gamma      |    384     |\n","|      p_encoder.layer.1.output.LayerNorm.beta       |    384     |\n","|                    icnn.weight                     |     27     |\n","|                     icnn.bias                      |     3      |\n","|                  decoder.0.weight                  |  13317120  |\n","|                   decoder.0.bias                   |    512     |\n","|                  decoder.2.weight                  |    512     |\n","|                   decoder.2.bias                   |    512     |\n","|                  decoder.3.weight                  |   32768    |\n","|                   decoder.3.bias                   |     64     |\n","|                  decoder.5.weight                  |     64     |\n","|                   decoder.5.bias                   |     64     |\n","|                  decoder.6.weight                  |    2048    |\n","|                   decoder.6.bias                   |     32     |\n","|                  decoder.8.weight                  |     32     |\n","|                   decoder.8.bias                   |     1      |\n","+----------------------------------------------------+------------+\n","Total Trainable Params: 20897055\n"]},{"output_type":"execute_result","data":{"text/plain":["20897055"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["#@title loss plot\n","plt.rcParams['figure.figsize'] = [10,5]\n","fig,(ax1,ax2) = plt.subplots(1,2)\n","fig.suptitle(\"model1;lr:5e-5;batch_size:64*1;train_size:23660\",fontsize=16)\n","loss_train =[ loss.cpu().detach().numpy() for loss in loss_history]\n","\n","\n","lh = list(filter(lambda x: x < 1, loss_train))\n","lh = [lh[i] for i in list(range(0,len(lh),369))]\n","# lh = list(filter(lambda x: x < 1, loss_val))\n","ax1.plot(lh)\n","ax1.set_title('train/val loss')\n","ax1.plot(loss_val,color='tab:orange')\n","ax2.plot(acc_train)\n","ax2.set_title('train/val accuracy')\n","ax2.plot(acc_val,color='tab:orange')\n"],"metadata":{"id":"drsXfISAv5vD","colab":{"base_uri":"https://localhost:8080/","height":72},"executionInfo":{"status":"ok","timestamp":1647586845890,"user_tz":420,"elapsed":3819,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"b95788dc-0258-4281-bcc6-b080485ddebf"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fc6d1d7bb90>]"]},"metadata":{},"execution_count":6},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["plt.rcParams['figure.figsize'] = [10,5]\n","fig,(ax1,ax2) = plt.subplots(1,2)\n","fig.suptitle(\"lr:5e-5;batch_size:64*2\",fontsize=16)\n","\n","lh = list(filter(lambda x: x < 1, loss_history))\n","lh = [lh[i] for i in list(range(0,len(lh),3))]\n","# lh = list(filter(lambda x: x < 1, loss_val))\n","ax1.plot(lh)\n","ax1.set_title('train loss')\n","ax2.plot(loss_val,color='tab:orange')\n","ax2.set_title('val loss')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"2HmMtvZqdJaO","executionInfo":{"status":"ok","timestamp":1647213336556,"user_tz":420,"elapsed":772,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"afc76781-8904-4087-93f9-2445910b7644"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0.5, 1.0, 'val loss')"]},"metadata":{},"execution_count":49},{"output_type":"display_data","data":{"text/plain":["<Figure size 720x360 with 2 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":[""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"id":"0NgepkUXbG75","executionInfo":{"status":"error","timestamp":1647213791210,"user_tz":420,"elapsed":29554,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"026cebb3-4d12-44f2-f9d4-f2f7fc7c8abe"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-51-adf7f220e2c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataFolder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Proj4_DPI/data/data_with_embedding/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataFolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train_loc0.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtraining_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBIN_Data_Encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtraining_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'max_d' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zi-Myq9laEll"},"outputs":[],"source":["#@title define torchtest functions\n","# !pip install --upgrade torchtest\n","# install from source because I need to make some changes to the source code\n","# %%bash\n","# git clone https://github.com/suriyadeepan/torchtest.git --quiet\n","class VariablesChangeException(Exception):\n","  pass\n","def assert_vars_change(model, loss_fn, optim, batch, device, params=None):\n","  \"\"\"Make sure that the given parameters (params) DO change during training\n","  If parameters (params) aren't provided, check all parameters.\n","  Parameters\n","  ----------\n","  model : torch.nn.Module\n","    torch model, an instance of torch.nn.Module\n","  loss_fn : function\n","    a loss function from torch.nn.functional \n","  optim : torch.optim.Optimizer\n","    an optimizer instance\n","  batch : list\n","    a 2 element list of inputs and labels, to be fed to the model\n","  params : list, optional\n","    list of parameters of form (name, variable)\n","  Raises\n","  ------\n","  VariablesChangeException\n","    If params do not change during training\n","  \"\"\"\n","\n","  _var_change_helper(True, model, loss_fn, optim, batch, device, params)\n","\n","def _var_change_helper(vars_change, model, loss_fn, optim, batch, device, params=None): \n","  \"\"\"Check if given variables (params) change or not during training\n","  If parameters (params) aren't provided, check all parameters.\n","  Parameters\n","  ----------\n","  vars_change : bool\n","    a flag which controls the check for change or not change\n","  model : torch.nn.Module\n","    torch model, an instance of torch.nn.Module\n","  loss_fn : function\n","    a loss function from torch.nn.functional \n","  optim : torch.optim.Optimizer\n","    an optimizer instance\n","  batch : list\n","    a 2 ele## ment list of inputs and labels, to be fed to the model\n","  params : list, optional\n","    list of parameters of form (name, variable)\n","  Raises\n","  ------\n","  VariablesChangeException\n","    if vars_change is True and params DO NOT change during training\n","    if vars_change is False and params DO change during training\n","  \"\"\"\n","\n","  if params is None:\n","    # get a list of params that are allowed to change\n","    params = [ np for np in model.named_parameters() if np[1].requires_grad ]\n","\n","  # take a copy\n","  initial_params = [ (name, p.clone()) for (name, p) in params ]\n","\n","  # run a training step\n","  _train_step(model, loss_fn, optim, batch, device)\n","\n","  # check if variables have changed\n","  for (_, p0), (name, p1) in zip(initial_params, params):\n","    try:\n","      if vars_change:\n","        assert not torch.equal(p0.to(device), p1.to(device))\n","      else:\n","        assert torch.equal(p0.to(device), p1.to(device))\n","    except AssertionError:\n","      raise VariablesChangeException( # error message\n","          \"{var_name} {msg}\".format(\n","            var_name=name, \n","            msg='did not change!' if vars_change else 'changed!' \n","            )\n","          )\n","\n","def _train_step(model, loss_fn, optim, batch, device):\n","  \"\"\"Run a training step on model for a given batch of data\n","  Parameters of the model accumulate gradients and the optimizer performs\n","  a gradient update on the parameters\n","  Parameters\n","  ----------\n","  model : torch.nn.Module\n","    torch model, an instance of torch.nn.Module\n","  loss_fn : function\n","    a loss function from torch.nn.functional \n","  optim : torch.optim.Optimizer\n","    an optimizer instance\n","  batch : list\n","    a 2 element list of inputs and labels, to be fed to the model\n","  \"\"\"\n","\n","  # put model in train mode\n","  model.train()\n","  model.to(device)\n","\n","  #  run one forward + backward step\n","  # clear gradient\n","  optim.zero_grad()\n","  # inputs and targets\n","  d, p, d_mask, p_mask, targets = batch[0], batch[1], batch[2], batch[3], batch[4]\n","  # move data to DEVICE\n","  d = d.to(device)\n","  p = p.to(device)\n","  d_mask = d_mask.to(device)\n","  p_mask = p_mask.to(device)\n","  targets = targets.to(device)\n","  # targets = targets.unsqueeze(1)\n","  targets = Variable(torch.from_numpy(np.array(targets)).float())\n","  # forward\n","  likelihood = model(d, p, d_mask, p_mask)\n","  m = torch.nn.Sigmoid()\n","  likelihood = torch.squeeze(m(likelihood))\n","  print(likelihood)\n","  # calc loss\n","  loss = loss_fn(likelihood, targets)\n","  # backward\n","  loss.backward()\n","  # optimization step\n","  optim.step()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1647128823248,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"},"user_tz":480},"id":"Ps8-X3pvnCnz","outputId":"74df1d11-03f1-4c99-9269-d545e525b090"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch size: 64\n","The actual training batch size: 640\n","Let's use cpu!\n","total parameters:  20897055\n","trainable parameters:  20897055\n"]}],"source":["#@title define configurations\n","config = BIN_config_DBPE()\n","\n","lr = 0.01\n","batch_size = config['batch_size']\n","accumulation_steps = config['grad_accumul_steps']\n","print(\"Batch size:\",batch_size)\n","print(\"The actual training batch size:\",batch_size*accumulation_steps)\n","\n","\n","train_epoch = 500\n","max_d = config['max_dna_seq']\n","max_p = config['max_protein_seq']\n","\n","loss_history = []\n","\n","model = BIN_Interaction_Flat(**config)\n","\n","if use_cuda:\n","  model = model.cuda()\n","\n","if torch.cuda.device_count() > 1:\n","  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","  model = nn.DataParallel(model, dim = 0)\n","elif torch.cuda.device_count() < 1:\n","  print(\"Let's use cpu!\")\n","\n","opt = torch.optim.Adam(model.parameters(), lr = lr)\n","pytorch_total_params = sum(p.numel() for p in model.parameters())\n","pytorch_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","print(\"total parameters: \", pytorch_total_params)\n","print(\"trainable parameters: \", pytorch_trainable_params)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64198,"status":"ok","timestamp":1647129747555,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"},"user_tz":480},"id":"FQvCzN4IiaSi","outputId":"fdc86b1d-4b8d-4e68-d98c-96de98c8ec54"},"outputs":[{"output_type":"stream","name":"stdout","text":["--- Data Preparation ---\n"]}],"source":["## import data\n","print('--- Data Preparation ---')\n","params = {'batch_size': batch_size,\n","  'shuffle': True,\n","  'num_workers': 6, \n","  'drop_last': True}\n","dataFolder = '/content/drive/MyDrive/Proj4_DPI/data/data_with_embedding/'\n","df_train = pd.read_pickle(dataFolder + '/train_loc0.pkl')[0:10000]\n","training_set = BIN_Data_Encoder(np.array([i for i in range(df_train.shape[0])]), df_train.label.values, df_train, max_d, max_p)\n","training_generator = data.DataLoader(training_set, **params)\n","df_val = pd.read_pickle(dataFolder + '/val_loc0.pkl')\n","df_val = df_val[1:5000]\n"]},{"cell_type":"code","source":["df_train.label.values.sum()\n","# df_val.label.values.sum()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"571ICuf2iFbn","executionInfo":{"status":"ok","timestamp":1647130116184,"user_tz":480,"elapsed":284,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"191c6e23-06b7-41d2-e7ad-8879a08bcdfa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5035"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["np.intersect1d(df_train.protein.values,df_val.protein.values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uKnsw3bjj65n","executionInfo":{"status":"ok","timestamp":1647130121954,"user_tz":480,"elapsed":137,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"b89e4d5b-9bec-4313-ffcb-77ea53b948f6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['AP2A', 'AP2C', 'ARID3A', 'ATF1', 'ATF2', 'ATF3', 'BACH1',\n","       'BAF155', 'BAF170', 'BATF', 'BCL11A', 'BCL3', 'BCLAF1', 'BDP1',\n","       'BHLHE40', 'BRCA1', 'BRG1', 'CBX3', 'CCNT2', 'CEBPB', 'CEBPD',\n","       'CHD1', 'CHD2', 'CREB1', 'CTBP2', 'CTCF', 'CTCFL', 'E2F1', 'E2F4',\n","       'E2F6', 'EBF1', 'EGR1', 'ELF1', 'ELK1', 'ELK4', 'ERR1', 'ESR1',\n","       'ETS1', 'EZH2', 'FAM48A', 'FOS', 'FOSL1', 'FOSL2', 'FOXA1',\n","       'FOXA2', 'FOXM1', 'FOXP2', 'GABPA', 'GATA1', 'GATA2', 'GATA3',\n","       'GR', 'GTF2B', 'GTF2F1', 'GTF3C2', 'HDAC1', 'HDAC2', 'HMGN3',\n","       'HNF4A', 'HNF4G', 'HSF1', 'IKZF1', 'INI1', 'IRF1', 'IRF3', 'IRF4',\n","       'JARID1A', 'JUNB', 'JUND', 'KAP1', 'MAFF', 'MAFK', 'MAX', 'MAZ',\n","       'MBD4', 'MEF2A', 'MEF2C', 'MTA3', 'MXI1', 'MYBL2', 'MYC', 'NANOG',\n","       'NFATC1', 'NFE2', 'NFIC', 'NFYA', 'NFYB', 'NR2F2', 'NRF1', 'NRSF',\n","       'P300', 'PAX5', 'PBX3', 'PGC1A', 'PHF8', 'PLU1', 'PML', 'POLR2A',\n","       'POLR3A', 'POLR3G', 'POU2F2', 'POU5F1', 'PRDM1', 'RAD21', 'RBBP5',\n","       'RCOR1', 'RELA', 'RFX5', 'RUNX3', 'RXRA', 'SAP30', 'SETDB1',\n","       'SIN3A', 'SIRT6', 'SIX5', 'SMC3', 'SP1', 'SP2', 'SP4', 'SPI1',\n","       'SREBP1', 'SRF', 'STAT1', 'STAT2', 'STAT3', 'STAT5A', 'SUZ12',\n","       'TAF1', 'TAF7', 'TAL1', 'TBLR1', 'TBP', 'TCF12', 'TCF3', 'TCF7L2',\n","       'TEAD4', 'THAP1', 'TR4', 'TRIM28', 'UBF', 'UBTF', 'USF1', 'USF2',\n","       'WHIP', 'YY1', 'ZBTB33', 'ZBTB7A', 'ZEB1', 'ZKSCAN1', 'ZNF143',\n","       'ZNF217', 'ZNF263', 'ZNF274'], dtype=object)"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["np.unique(df_val.protein.values)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3qfKQu8LmyeK","executionInfo":{"status":"ok","timestamp":1647130136069,"user_tz":480,"elapsed":166,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"}},"outputId":"239bd0e0-7aff-40e8-c9fc-1b282ef3b1fd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['AP2A', 'AP2C', 'ARID3A', 'ATF1', 'ATF2', 'ATF3', 'BACH1',\n","       'BAF155', 'BAF170', 'BATF', 'BCL11A', 'BCL3', 'BCLAF1', 'BDP1',\n","       'BHLHE40', 'BRCA1', 'BRG1', 'CBX3', 'CCNT2', 'CEBPB', 'CEBPD',\n","       'CHD1', 'CHD2', 'CREB1', 'CTBP2', 'CTCF', 'CTCFL', 'E2F1', 'E2F4',\n","       'E2F6', 'EBF1', 'EGR1', 'ELF1', 'ELK1', 'ELK4', 'ERR1', 'ESR1',\n","       'ETS1', 'EZH2', 'FAM48A', 'FOS', 'FOSL1', 'FOSL2', 'FOXA1',\n","       'FOXA2', 'FOXM1', 'FOXP2', 'GABPA', 'GATA1', 'GATA2', 'GATA3',\n","       'GR', 'GTF2B', 'GTF2F1', 'GTF3C2', 'HDAC1', 'HDAC2', 'HDAC8',\n","       'HMGN3', 'HNF4A', 'HNF4G', 'HSF1', 'IKZF1', 'INI1', 'IRF1', 'IRF3',\n","       'IRF4', 'JARID1A', 'JUNB', 'JUND', 'KAP1', 'MAFF', 'MAFK', 'MAX',\n","       'MAZ', 'MBD4', 'MEF2A', 'MEF2C', 'MTA3', 'MXI1', 'MYBL2', 'MYC',\n","       'NANOG', 'NFATC1', 'NFE2', 'NFIC', 'NFYA', 'NFYB', 'NR2F2', 'NRF1',\n","       'NRSF', 'P300', 'PAX5', 'PBX3', 'PGC1A', 'PHF8', 'PLU1', 'PML',\n","       'POLR2A', 'POLR3A', 'POLR3G', 'POU2F2', 'POU5F1', 'PRDM1', 'RAD21',\n","       'RBBP5', 'RCOR1', 'RELA', 'RFX5', 'RUNX3', 'RXRA', 'SAP30',\n","       'SETDB1', 'SIN3A', 'SIRT6', 'SIX5', 'SMC3', 'SP1', 'SP2', 'SP4',\n","       'SPI1', 'SREBP1', 'SRF', 'STAT1', 'STAT2', 'STAT3', 'STAT5A',\n","       'SUZ12', 'TAF1', 'TAF7', 'TAL1', 'TBLR1', 'TBP', 'TCF12', 'TCF3',\n","       'TCF7L2', 'TEAD4', 'THAP1', 'TR4', 'TRIM28', 'UBF', 'UBTF', 'USF1',\n","       'USF2', 'WHIP', 'YY1', 'ZBTB33', 'ZBTB7A', 'ZEB1', 'ZKSCAN1',\n","       'ZNF143', 'ZNF217', 'ZNF263', 'ZNF274'], dtype=object)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BnJrluaQjkmZ","outputId":"bb5be6d6-0cb0-4359-88f7-baf344e0ec5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Training at Epoch 246 iteration 20 with loss 0.07087783\n","Training at Epoch 246 iteration 21 with loss 0.067757346\n","Training at Epoch 246 iteration 22 with loss 0.06879651\n","Training at Epoch 246 iteration 23 with loss 0.06903021\n","Training at Epoch 246 iteration 24 with loss 0.06833451\n","Training at Epoch 246 iteration 25 with loss 0.070829764\n","Training at Epoch 246 iteration 26 with loss 0.06778493\n","Training at Epoch 246 iteration 27 with loss 0.069937214\n","Training at Epoch 246 iteration 28 with loss 0.06874999\n","Training at Epoch 246 iteration 29 with loss 0.0689032\n","Training at Epoch 246 iteration 30 with loss 0.07038142\n","Training at Epoch 247 iteration 0 with loss 0.06916329\n","Training at Epoch 247 iteration 1 with loss 0.06947997\n","Training at Epoch 247 iteration 2 with loss 0.06865624\n","Training at Epoch 247 iteration 3 with loss 0.06991817\n","Training at Epoch 247 iteration 4 with loss 0.068928495\n","Training at Epoch 247 iteration 5 with loss 0.07005451\n","Training at Epoch 247 iteration 6 with loss 0.070867926\n","Training at Epoch 247 iteration 7 with loss 0.07071259\n","Training at Epoch 247 iteration 8 with loss 0.06864257\n","Training at Epoch 247 iteration 9 with loss 0.06700249\n","Training at Epoch 247 iteration 10 with loss 0.06768763\n","Training at Epoch 247 iteration 11 with loss 0.06797788\n","Training at Epoch 247 iteration 12 with loss 0.06870433\n","Training at Epoch 247 iteration 13 with loss 0.06922847\n","Training at Epoch 247 iteration 14 with loss 0.06952101\n","Training at Epoch 247 iteration 15 with loss 0.06921916\n","Training at Epoch 247 iteration 16 with loss 0.06929095\n","Training at Epoch 247 iteration 17 with loss 0.069942355\n","Training at Epoch 247 iteration 18 with loss 0.06598599\n","Training at Epoch 247 iteration 19 with loss 0.070621505\n","Training at Epoch 247 iteration 20 with loss 0.06861706\n","Training at Epoch 247 iteration 21 with loss 0.068919584\n","Training at Epoch 247 iteration 22 with loss 0.0684651\n","Training at Epoch 247 iteration 23 with loss 0.069377236\n","Training at Epoch 247 iteration 24 with loss 0.06828138\n","Training at Epoch 247 iteration 25 with loss 0.06911956\n","Training at Epoch 247 iteration 26 with loss 0.06844633\n","Training at Epoch 247 iteration 27 with loss 0.06859279\n","Training at Epoch 247 iteration 28 with loss 0.06946001\n","Training at Epoch 247 iteration 29 with loss 0.069097616\n","Training at Epoch 247 iteration 30 with loss 0.07158969\n","Training at Epoch 248 iteration 0 with loss 0.067689605\n","Training at Epoch 248 iteration 1 with loss 0.06937428\n","Training at Epoch 248 iteration 2 with loss 0.06865074\n","Training at Epoch 248 iteration 3 with loss 0.06888265\n","Training at Epoch 248 iteration 4 with loss 0.06821789\n","Training at Epoch 248 iteration 5 with loss 0.06840804\n","Training at Epoch 248 iteration 6 with loss 0.070223644\n","Training at Epoch 248 iteration 7 with loss 0.06817708\n","Training at Epoch 248 iteration 8 with loss 0.070155814\n","Training at Epoch 248 iteration 9 with loss 0.06838721\n","Training at Epoch 248 iteration 10 with loss 0.067934774\n","Training at Epoch 248 iteration 11 with loss 0.06983125\n","Training at Epoch 248 iteration 12 with loss 0.06942469\n","Training at Epoch 248 iteration 13 with loss 0.06961861\n","Training at Epoch 248 iteration 14 with loss 0.06767937\n","Training at Epoch 248 iteration 15 with loss 0.0710883\n","Training at Epoch 248 iteration 16 with loss 0.0688127\n","Training at Epoch 248 iteration 17 with loss 0.070077874\n","Training at Epoch 248 iteration 18 with loss 0.06902458\n","Training at Epoch 248 iteration 19 with loss 0.06875898\n","Training at Epoch 248 iteration 20 with loss 0.06826394\n","Training at Epoch 248 iteration 21 with loss 0.069000505\n","Training at Epoch 248 iteration 22 with loss 0.06952666\n","Training at Epoch 248 iteration 23 with loss 0.06888615\n","Training at Epoch 248 iteration 24 with loss 0.06880821\n","Training at Epoch 248 iteration 25 with loss 0.067681715\n","Training at Epoch 248 iteration 26 with loss 0.06827722\n","Training at Epoch 248 iteration 27 with loss 0.06986847\n","Training at Epoch 248 iteration 28 with loss 0.06847589\n","Training at Epoch 248 iteration 29 with loss 0.069413856\n","Training at Epoch 248 iteration 30 with loss 0.07057748\n","Training at Epoch 249 iteration 0 with loss 0.06920804\n","Training at Epoch 249 iteration 1 with loss 0.06990956\n","Training at Epoch 249 iteration 2 with loss 0.06914054\n","Training at Epoch 249 iteration 3 with loss 0.07080738\n","Training at Epoch 249 iteration 4 with loss 0.06778537\n","Training at Epoch 249 iteration 5 with loss 0.070557065\n","Training at Epoch 249 iteration 6 with loss 0.06654153\n","Training at Epoch 249 iteration 7 with loss 0.06860242\n","Training at Epoch 249 iteration 8 with loss 0.07005367\n","Training at Epoch 249 iteration 9 with loss 0.06943\n","Training at Epoch 249 iteration 10 with loss 0.0683312\n","Training at Epoch 249 iteration 11 with loss 0.06934352\n","Training at Epoch 249 iteration 12 with loss 0.06935724\n","Training at Epoch 249 iteration 13 with loss 0.06976733\n","Training at Epoch 249 iteration 14 with loss 0.068248406\n","Training at Epoch 249 iteration 15 with loss 0.066194475\n","Training at Epoch 249 iteration 16 with loss 0.070066676\n","Training at Epoch 249 iteration 17 with loss 0.07080953\n","Training at Epoch 249 iteration 18 with loss 0.07049039\n","Training at Epoch 249 iteration 19 with loss 0.0687811\n","Training at Epoch 249 iteration 20 with loss 0.068304434\n","Training at Epoch 249 iteration 21 with loss 0.07034193\n","Training at Epoch 249 iteration 22 with loss 0.069159925\n","Training at Epoch 249 iteration 23 with loss 0.06699425\n","Training at Epoch 249 iteration 24 with loss 0.06796825\n","Training at Epoch 249 iteration 25 with loss 0.068908915\n","Training at Epoch 249 iteration 26 with loss 0.06897809\n","Training at Epoch 249 iteration 27 with loss 0.06966233\n","Training at Epoch 249 iteration 28 with loss 0.0688874\n","Training at Epoch 249 iteration 29 with loss 0.0707203\n","Training at Epoch 249 iteration 30 with loss 0.069883645\n","Training at Epoch 250 iteration 0 with loss 0.06922798\n","Training at Epoch 250 iteration 1 with loss 0.06938125\n","Training at Epoch 250 iteration 2 with loss 0.07010875\n","Training at Epoch 250 iteration 3 with loss 0.06910924\n","Training at Epoch 250 iteration 4 with loss 0.06811507\n","Training at Epoch 250 iteration 5 with loss 0.07037395\n","Training at Epoch 250 iteration 6 with loss 0.069693126\n","Training at Epoch 250 iteration 7 with loss 0.06852776\n","Training at Epoch 250 iteration 8 with loss 0.06886121\n","Training at Epoch 250 iteration 9 with loss 0.06994972\n","Training at Epoch 250 iteration 10 with loss 0.0694108\n","Training at Epoch 250 iteration 11 with loss 0.069474205\n","Training at Epoch 250 iteration 12 with loss 0.06767797\n","Training at Epoch 250 iteration 13 with loss 0.06805643\n","Training at Epoch 250 iteration 14 with loss 0.06963931\n","Training at Epoch 250 iteration 15 with loss 0.06930742\n","Training at Epoch 250 iteration 16 with loss 0.069424696\n","Training at Epoch 250 iteration 17 with loss 0.06847729\n","Training at Epoch 250 iteration 18 with loss 0.06819333\n","Training at Epoch 250 iteration 19 with loss 0.069469824\n","Training at Epoch 250 iteration 20 with loss 0.06840792\n","Training at Epoch 250 iteration 21 with loss 0.06862693\n","Training at Epoch 250 iteration 22 with loss 0.06753749\n","Training at Epoch 250 iteration 23 with loss 0.07062198\n","Training at Epoch 250 iteration 24 with loss 0.06989075\n","Training at Epoch 250 iteration 25 with loss 0.06947375\n","Training at Epoch 250 iteration 26 with loss 0.06804669\n","Training at Epoch 250 iteration 27 with loss 0.0665874\n","Training at Epoch 250 iteration 28 with loss 0.06882008\n","Training at Epoch 250 iteration 29 with loss 0.06962757\n","Training at Epoch 250 iteration 30 with loss 0.07006266\n","Training at Epoch 251 iteration 0 with loss 0.07106898\n","Training at Epoch 251 iteration 1 with loss 0.06772345\n","Training at Epoch 251 iteration 2 with loss 0.06801242\n","Training at Epoch 251 iteration 3 with loss 0.07070385\n","Training at Epoch 251 iteration 4 with loss 0.068515144\n","Training at Epoch 251 iteration 5 with loss 0.06912769\n","Training at Epoch 251 iteration 6 with loss 0.06883821\n","Training at Epoch 251 iteration 7 with loss 0.07296257\n","Training at Epoch 251 iteration 8 with loss 0.06950729\n","Training at Epoch 251 iteration 9 with loss 0.06930692\n","Training at Epoch 251 iteration 10 with loss 0.07118347\n","Training at Epoch 251 iteration 11 with loss 0.06862293\n","Training at Epoch 251 iteration 12 with loss 0.06992432\n","Training at Epoch 251 iteration 13 with loss 0.06915273\n","Training at Epoch 251 iteration 14 with loss 0.067242816\n","Training at Epoch 251 iteration 15 with loss 0.07021569\n","Training at Epoch 251 iteration 16 with loss 0.0701021\n","Training at Epoch 251 iteration 17 with loss 0.07031808\n","Training at Epoch 251 iteration 18 with loss 0.068070844\n","Training at Epoch 251 iteration 19 with loss 0.067692086\n","Training at Epoch 251 iteration 20 with loss 0.069818035\n","Training at Epoch 251 iteration 21 with loss 0.0665027\n","Training at Epoch 251 iteration 22 with loss 0.06775284\n","Training at Epoch 251 iteration 23 with loss 0.068760425\n","Training at Epoch 251 iteration 24 with loss 0.06854711\n","Training at Epoch 251 iteration 25 with loss 0.06904821\n","Training at Epoch 251 iteration 26 with loss 0.069346026\n","Training at Epoch 251 iteration 27 with loss 0.06878924\n","Training at Epoch 251 iteration 28 with loss 0.0681441\n","Training at Epoch 251 iteration 29 with loss 0.06829723\n","Training at Epoch 251 iteration 30 with loss 0.06878686\n","Training at Epoch 252 iteration 0 with loss 0.06884126\n","Training at Epoch 252 iteration 1 with loss 0.06795119\n","Training at Epoch 252 iteration 2 with loss 0.07040424\n","Training at Epoch 252 iteration 3 with loss 0.0709634\n","Training at Epoch 252 iteration 4 with loss 0.06830941\n","Training at Epoch 252 iteration 5 with loss 0.07142573\n","Training at Epoch 252 iteration 6 with loss 0.06914646\n","Training at Epoch 252 iteration 7 with loss 0.06945067\n","Training at Epoch 252 iteration 8 with loss 0.07080497\n","Training at Epoch 252 iteration 9 with loss 0.06876321\n","Training at Epoch 252 iteration 10 with loss 0.06898039\n","Training at Epoch 252 iteration 11 with loss 0.06951468\n","Training at Epoch 252 iteration 12 with loss 0.068229765\n","Training at Epoch 252 iteration 13 with loss 0.06899691\n","Training at Epoch 252 iteration 14 with loss 0.068522714\n","Training at Epoch 252 iteration 15 with loss 0.069104165\n","Training at Epoch 252 iteration 16 with loss 0.069272585\n","Training at Epoch 252 iteration 17 with loss 0.06774391\n","Training at Epoch 252 iteration 18 with loss 0.06839593\n","Training at Epoch 252 iteration 19 with loss 0.069185\n","Training at Epoch 252 iteration 20 with loss 0.06779962\n","Training at Epoch 252 iteration 21 with loss 0.0699379\n","Training at Epoch 252 iteration 22 with loss 0.06757818\n","Training at Epoch 252 iteration 23 with loss 0.06729446\n","Training at Epoch 252 iteration 24 with loss 0.069441244\n","Training at Epoch 252 iteration 25 with loss 0.0691944\n","Training at Epoch 252 iteration 26 with loss 0.07014652\n","Training at Epoch 252 iteration 27 with loss 0.069339894\n","Training at Epoch 252 iteration 28 with loss 0.069411054\n","Training at Epoch 252 iteration 29 with loss 0.06777641\n","Training at Epoch 252 iteration 30 with loss 0.07056979\n","Training at Epoch 253 iteration 0 with loss 0.069073886\n","Training at Epoch 253 iteration 1 with loss 0.06859207\n","Training at Epoch 253 iteration 2 with loss 0.07124202\n","Training at Epoch 253 iteration 3 with loss 0.06756493\n","Training at Epoch 253 iteration 4 with loss 0.06922863\n","Training at Epoch 253 iteration 5 with loss 0.07105298\n","Training at Epoch 253 iteration 6 with loss 0.070042744\n","Training at Epoch 253 iteration 7 with loss 0.06932107\n","Training at Epoch 253 iteration 8 with loss 0.068338536\n","Training at Epoch 253 iteration 9 with loss 0.07058102\n","Training at Epoch 253 iteration 10 with loss 0.06928028\n","Training at Epoch 253 iteration 11 with loss 0.0706664\n","Training at Epoch 253 iteration 12 with loss 0.068767145\n","Training at Epoch 253 iteration 13 with loss 0.07060586\n","Training at Epoch 253 iteration 14 with loss 0.06764408\n","Training at Epoch 253 iteration 15 with loss 0.069195196\n","Training at Epoch 253 iteration 16 with loss 0.06850424\n","Training at Epoch 253 iteration 17 with loss 0.0685526\n","Training at Epoch 253 iteration 18 with loss 0.067742184\n","Training at Epoch 253 iteration 19 with loss 0.06639249\n","Training at Epoch 253 iteration 20 with loss 0.06868239\n","Training at Epoch 253 iteration 21 with loss 0.068081215\n","Training at Epoch 253 iteration 22 with loss 0.07013318\n","Training at Epoch 253 iteration 23 with loss 0.06961187\n","Training at Epoch 253 iteration 24 with loss 0.070148446\n","Training at Epoch 253 iteration 25 with loss 0.06961179\n","Training at Epoch 253 iteration 26 with loss 0.06810586\n","Training at Epoch 253 iteration 27 with loss 0.068138845\n","Training at Epoch 253 iteration 28 with loss 0.07017369\n","Training at Epoch 253 iteration 29 with loss 0.06767734\n","Training at Epoch 253 iteration 30 with loss 0.06961439\n","Training at Epoch 254 iteration 0 with loss 0.07073041\n","Training at Epoch 254 iteration 1 with loss 0.0707063\n","Training at Epoch 254 iteration 2 with loss 0.06615125\n","Training at Epoch 254 iteration 3 with loss 0.070797816\n","Training at Epoch 254 iteration 4 with loss 0.06956319\n","Training at Epoch 254 iteration 5 with loss 0.069023505\n","Training at Epoch 254 iteration 6 with loss 0.067003205\n","Training at Epoch 254 iteration 7 with loss 0.06760818\n","Training at Epoch 254 iteration 8 with loss 0.068746224\n","Training at Epoch 254 iteration 9 with loss 0.0680275\n","Training at Epoch 254 iteration 10 with loss 0.07167257\n","Training at Epoch 254 iteration 11 with loss 0.06961028\n","Training at Epoch 254 iteration 12 with loss 0.06532147\n","Training at Epoch 254 iteration 13 with loss 0.067083105\n","Training at Epoch 254 iteration 14 with loss 0.070419475\n","Training at Epoch 254 iteration 15 with loss 0.06955721\n","Training at Epoch 254 iteration 16 with loss 0.06961992\n","Training at Epoch 254 iteration 17 with loss 0.06987036\n","Training at Epoch 254 iteration 18 with loss 0.069103666\n","Training at Epoch 254 iteration 19 with loss 0.070248544\n","Training at Epoch 254 iteration 20 with loss 0.07157637\n","Training at Epoch 254 iteration 21 with loss 0.06748957\n","Training at Epoch 254 iteration 22 with loss 0.071367845\n","Training at Epoch 254 iteration 23 with loss 0.072098896\n","Training at Epoch 254 iteration 24 with loss 0.06887011\n","Training at Epoch 254 iteration 25 with loss 0.06902456\n","Training at Epoch 254 iteration 26 with loss 0.06733094\n","Training at Epoch 254 iteration 27 with loss 0.06619837\n","Training at Epoch 254 iteration 28 with loss 0.06737407\n","Training at Epoch 254 iteration 29 with loss 0.070176676\n","Training at Epoch 254 iteration 30 with loss 0.06978901\n","Training at Epoch 255 iteration 0 with loss 0.0696685\n","Training at Epoch 255 iteration 1 with loss 0.066835746\n","Training at Epoch 255 iteration 2 with loss 0.068045065\n","Training at Epoch 255 iteration 3 with loss 0.067613095\n","Training at Epoch 255 iteration 4 with loss 0.0705322\n","Training at Epoch 255 iteration 5 with loss 0.06581518\n","Training at Epoch 255 iteration 6 with loss 0.069522455\n","Training at Epoch 255 iteration 7 with loss 0.068737306\n","Training at Epoch 255 iteration 8 with loss 0.06977149\n","Training at Epoch 255 iteration 9 with loss 0.0677508\n","Training at Epoch 255 iteration 10 with loss 0.06699786\n","Training at Epoch 255 iteration 11 with loss 0.0680398\n","Training at Epoch 255 iteration 12 with loss 0.07076119\n","Training at Epoch 255 iteration 13 with loss 0.07085417\n","Training at Epoch 255 iteration 14 with loss 0.07102387\n","Training at Epoch 255 iteration 15 with loss 0.06874461\n","Training at Epoch 255 iteration 16 with loss 0.06990689\n","Training at Epoch 255 iteration 17 with loss 0.072055995\n","Training at Epoch 255 iteration 18 with loss 0.06998881\n","Training at Epoch 255 iteration 19 with loss 0.07045908\n","Training at Epoch 255 iteration 20 with loss 0.06861635\n","Training at Epoch 255 iteration 21 with loss 0.069065355\n","Training at Epoch 255 iteration 22 with loss 0.07142051\n","Training at Epoch 255 iteration 23 with loss 0.07155481\n","Training at Epoch 255 iteration 24 with loss 0.06854762\n","Training at Epoch 255 iteration 25 with loss 0.06966676\n","Training at Epoch 255 iteration 26 with loss 0.068156235\n","Training at Epoch 255 iteration 27 with loss 0.06542738\n","Training at Epoch 255 iteration 28 with loss 0.06955093\n","Training at Epoch 255 iteration 29 with loss 0.06691763\n","Training at Epoch 255 iteration 30 with loss 0.06937246\n","Training at Epoch 256 iteration 0 with loss 0.07012453\n","Training at Epoch 256 iteration 1 with loss 0.06877916\n","Training at Epoch 256 iteration 2 with loss 0.071687356\n","Training at Epoch 256 iteration 3 with loss 0.06888984\n","Training at Epoch 256 iteration 4 with loss 0.06758555\n","Training at Epoch 256 iteration 5 with loss 0.07102887\n","Training at Epoch 256 iteration 6 with loss 0.06738619\n","Training at Epoch 256 iteration 7 with loss 0.06749853\n","Training at Epoch 256 iteration 8 with loss 0.07095735\n","Training at Epoch 256 iteration 9 with loss 0.0704291\n","Training at Epoch 256 iteration 10 with loss 0.06794144\n","Training at Epoch 256 iteration 11 with loss 0.07221633\n","Training at Epoch 256 iteration 12 with loss 0.06968992\n","Training at Epoch 256 iteration 13 with loss 0.0656287\n","Training at Epoch 256 iteration 14 with loss 0.070057884\n","Training at Epoch 256 iteration 15 with loss 0.07020445\n","Training at Epoch 256 iteration 16 with loss 0.06993677\n","Training at Epoch 256 iteration 17 with loss 0.06791611\n","Training at Epoch 256 iteration 18 with loss 0.07003044\n","Training at Epoch 256 iteration 19 with loss 0.06811984\n","Training at Epoch 256 iteration 20 with loss 0.06822738\n","Training at Epoch 256 iteration 21 with loss 0.06772348\n","Training at Epoch 256 iteration 22 with loss 0.06915082\n","Training at Epoch 256 iteration 23 with loss 0.06938566\n","Training at Epoch 256 iteration 24 with loss 0.069103144\n","Training at Epoch 256 iteration 25 with loss 0.06912245\n","Training at Epoch 256 iteration 26 with loss 0.067067236\n","Training at Epoch 256 iteration 27 with loss 0.07009733\n","Training at Epoch 256 iteration 28 with loss 0.06979221\n","Training at Epoch 256 iteration 29 with loss 0.07091652\n","Training at Epoch 256 iteration 30 with loss 0.066325046\n","Training at Epoch 257 iteration 0 with loss 0.06822157\n","Training at Epoch 257 iteration 1 with loss 0.069307014\n","Training at Epoch 257 iteration 2 with loss 0.068719424\n","Training at Epoch 257 iteration 3 with loss 0.06915855\n","Training at Epoch 257 iteration 4 with loss 0.06857608\n","Training at Epoch 257 iteration 5 with loss 0.06883098\n","Training at Epoch 257 iteration 6 with loss 0.06993083\n","Training at Epoch 257 iteration 7 with loss 0.06568505\n","Training at Epoch 257 iteration 8 with loss 0.0702623\n","Training at Epoch 257 iteration 9 with loss 0.07130801\n","Training at Epoch 257 iteration 10 with loss 0.069564775\n","Training at Epoch 257 iteration 11 with loss 0.0677887\n","Training at Epoch 257 iteration 12 with loss 0.0692359\n","Training at Epoch 257 iteration 13 with loss 0.07061781\n","Training at Epoch 257 iteration 14 with loss 0.06910241\n","Training at Epoch 257 iteration 15 with loss 0.07222277\n","Training at Epoch 257 iteration 16 with loss 0.06662397\n","Training at Epoch 257 iteration 17 with loss 0.067533754\n","Training at Epoch 257 iteration 18 with loss 0.069063485\n","Training at Epoch 257 iteration 19 with loss 0.07004173\n","Training at Epoch 257 iteration 20 with loss 0.06961767\n","Training at Epoch 257 iteration 21 with loss 0.06815463\n","Training at Epoch 257 iteration 22 with loss 0.07251339\n","Training at Epoch 257 iteration 23 with loss 0.0681837\n","Training at Epoch 257 iteration 24 with loss 0.068238296\n","Training at Epoch 257 iteration 25 with loss 0.06820464\n","Training at Epoch 257 iteration 26 with loss 0.06908502\n","Training at Epoch 257 iteration 27 with loss 0.0715948\n","Training at Epoch 257 iteration 28 with loss 0.06962784\n","Training at Epoch 257 iteration 29 with loss 0.06809412\n","Training at Epoch 257 iteration 30 with loss 0.068595484\n","Training at Epoch 258 iteration 0 with loss 0.070073985\n","Training at Epoch 258 iteration 1 with loss 0.06623829\n","Training at Epoch 258 iteration 2 with loss 0.06674496\n","Training at Epoch 258 iteration 3 with loss 0.06811199\n","Training at Epoch 258 iteration 4 with loss 0.06820776\n","Training at Epoch 258 iteration 5 with loss 0.07043634\n","Training at Epoch 258 iteration 6 with loss 0.0686581\n","Training at Epoch 258 iteration 7 with loss 0.069112554\n","Training at Epoch 258 iteration 8 with loss 0.06859056\n","Training at Epoch 258 iteration 9 with loss 0.06960588\n","Training at Epoch 258 iteration 10 with loss 0.0696135\n","Training at Epoch 258 iteration 11 with loss 0.069597766\n","Training at Epoch 258 iteration 12 with loss 0.06770926\n","Training at Epoch 258 iteration 13 with loss 0.06865539\n","Training at Epoch 258 iteration 14 with loss 0.07242886\n","Training at Epoch 258 iteration 15 with loss 0.06824516\n","Training at Epoch 258 iteration 16 with loss 0.06951198\n","Training at Epoch 258 iteration 17 with loss 0.06908882\n","Training at Epoch 258 iteration 18 with loss 0.066283464\n","Training at Epoch 258 iteration 19 with loss 0.071872935\n","Training at Epoch 258 iteration 20 with loss 0.068643406\n","Training at Epoch 258 iteration 21 with loss 0.06963073\n","Training at Epoch 258 iteration 22 with loss 0.06881001\n","Training at Epoch 258 iteration 23 with loss 0.069614924\n","Training at Epoch 258 iteration 24 with loss 0.069582246\n","Training at Epoch 258 iteration 25 with loss 0.07046303\n","Training at Epoch 258 iteration 26 with loss 0.06905083\n","Training at Epoch 258 iteration 27 with loss 0.06912937\n","Training at Epoch 258 iteration 28 with loss 0.06963854\n","Training at Epoch 258 iteration 29 with loss 0.06908797\n","Training at Epoch 258 iteration 30 with loss 0.06865134\n","Training at Epoch 259 iteration 0 with loss 0.069089845\n","Training at Epoch 259 iteration 1 with loss 0.06914775\n","Training at Epoch 259 iteration 2 with loss 0.06729114\n","Training at Epoch 259 iteration 3 with loss 0.07010593\n","Training at Epoch 259 iteration 4 with loss 0.069542274\n","Training at Epoch 259 iteration 5 with loss 0.06865282\n","Training at Epoch 259 iteration 6 with loss 0.06815411\n","Training at Epoch 259 iteration 7 with loss 0.068216205\n","Training at Epoch 259 iteration 8 with loss 0.06918027\n","Training at Epoch 259 iteration 9 with loss 0.07099289\n","Training at Epoch 259 iteration 10 with loss 0.06819163\n","Training at Epoch 259 iteration 11 with loss 0.06771502\n","Training at Epoch 259 iteration 12 with loss 0.06822683\n","Training at Epoch 259 iteration 13 with loss 0.06953561\n","Training at Epoch 259 iteration 14 with loss 0.069598325\n","Training at Epoch 259 iteration 15 with loss 0.07002259\n","Training at Epoch 259 iteration 16 with loss 0.07095604\n","Training at Epoch 259 iteration 17 with loss 0.070979\n","Training at Epoch 259 iteration 18 with loss 0.068620756\n","Training at Epoch 259 iteration 19 with loss 0.06919219\n","Training at Epoch 259 iteration 20 with loss 0.06873119\n","Training at Epoch 259 iteration 21 with loss 0.07134422\n","Training at Epoch 259 iteration 22 with loss 0.06825562\n","Training at Epoch 259 iteration 23 with loss 0.06909592\n","Training at Epoch 259 iteration 24 with loss 0.06642711\n","Training at Epoch 259 iteration 25 with loss 0.0709259\n","Training at Epoch 259 iteration 26 with loss 0.07182627\n","Training at Epoch 259 iteration 27 with loss 0.06731741\n","Training at Epoch 259 iteration 28 with loss 0.06863121\n","Training at Epoch 259 iteration 29 with loss 0.06788822\n","Training at Epoch 259 iteration 30 with loss 0.06868422\n","Training at Epoch 260 iteration 0 with loss 0.06914476\n","Training at Epoch 260 iteration 1 with loss 0.06772969\n","Training at Epoch 260 iteration 2 with loss 0.06915431\n","Training at Epoch 260 iteration 3 with loss 0.068247035\n","Training at Epoch 260 iteration 4 with loss 0.06952763\n","Training at Epoch 260 iteration 5 with loss 0.070405826\n","Training at Epoch 260 iteration 6 with loss 0.06916606\n","Training at Epoch 260 iteration 7 with loss 0.069166414\n","Training at Epoch 260 iteration 8 with loss 0.06862567\n","Training at Epoch 260 iteration 9 with loss 0.070920914\n","Training at Epoch 260 iteration 10 with loss 0.067824766\n","Training at Epoch 260 iteration 11 with loss 0.06962679\n","Training at Epoch 260 iteration 12 with loss 0.07050416\n","Training at Epoch 260 iteration 13 with loss 0.06866096\n","Training at Epoch 260 iteration 14 with loss 0.06611407\n","Training at Epoch 260 iteration 15 with loss 0.06787279\n","Training at Epoch 260 iteration 16 with loss 0.06826591\n","Training at Epoch 260 iteration 17 with loss 0.069097675\n","Training at Epoch 260 iteration 18 with loss 0.06825842\n","Training at Epoch 260 iteration 19 with loss 0.06908949\n","Training at Epoch 260 iteration 20 with loss 0.06874348\n","Training at Epoch 260 iteration 21 with loss 0.06735542\n","Training at Epoch 260 iteration 22 with loss 0.06864126\n","Training at Epoch 260 iteration 23 with loss 0.07174206\n","Training at Epoch 260 iteration 24 with loss 0.07170894\n","Training at Epoch 260 iteration 25 with loss 0.0696221\n","Training at Epoch 260 iteration 26 with loss 0.07001543\n","Training at Epoch 260 iteration 27 with loss 0.06875034\n","Training at Epoch 260 iteration 28 with loss 0.070354\n","Training at Epoch 260 iteration 29 with loss 0.06868275\n","Training at Epoch 260 iteration 30 with loss 0.06922501\n","Training at Epoch 261 iteration 0 with loss 0.07037211\n","Training at Epoch 261 iteration 1 with loss 0.068326935\n","Training at Epoch 261 iteration 2 with loss 0.069059685\n","Training at Epoch 261 iteration 3 with loss 0.069555484\n","Training at Epoch 261 iteration 4 with loss 0.06657188\n","Training at Epoch 261 iteration 5 with loss 0.06872419\n","Training at Epoch 261 iteration 6 with loss 0.06823105\n","Training at Epoch 261 iteration 7 with loss 0.0687323\n","Training at Epoch 261 iteration 8 with loss 0.069172926\n","Training at Epoch 261 iteration 9 with loss 0.07126348\n","Training at Epoch 261 iteration 10 with loss 0.071574785\n","Training at Epoch 261 iteration 11 with loss 0.06948844\n","Training at Epoch 261 iteration 12 with loss 0.06779773\n","Training at Epoch 261 iteration 13 with loss 0.067926034\n","Training at Epoch 261 iteration 14 with loss 0.07081964\n","Training at Epoch 261 iteration 15 with loss 0.06822507\n","Training at Epoch 261 iteration 16 with loss 0.06999795\n","Training at Epoch 261 iteration 17 with loss 0.06915659\n","Training at Epoch 261 iteration 18 with loss 0.068281315\n","Training at Epoch 261 iteration 19 with loss 0.06698677\n","Training at Epoch 261 iteration 20 with loss 0.06870769\n","Training at Epoch 261 iteration 21 with loss 0.069029264\n","Training at Epoch 261 iteration 22 with loss 0.069556184\n","Training at Epoch 261 iteration 23 with loss 0.06873654\n","Training at Epoch 261 iteration 24 with loss 0.069542125\n","Training at Epoch 261 iteration 25 with loss 0.068653874\n","Training at Epoch 261 iteration 26 with loss 0.06775151\n","Training at Epoch 261 iteration 27 with loss 0.06993556\n","Training at Epoch 261 iteration 28 with loss 0.06863471\n","Training at Epoch 261 iteration 29 with loss 0.06994612\n","Training at Epoch 261 iteration 30 with loss 0.07089588\n","Training at Epoch 262 iteration 0 with loss 0.069177754\n","Training at Epoch 262 iteration 1 with loss 0.0695906\n","Training at Epoch 262 iteration 2 with loss 0.069987595\n","Training at Epoch 262 iteration 3 with loss 0.066670135\n","Training at Epoch 262 iteration 4 with loss 0.06822801\n","Training at Epoch 262 iteration 5 with loss 0.06791634\n","Training at Epoch 262 iteration 6 with loss 0.06822865\n","Training at Epoch 262 iteration 7 with loss 0.06875165\n","Training at Epoch 262 iteration 8 with loss 0.06864287\n","Training at Epoch 262 iteration 9 with loss 0.06905465\n","Training at Epoch 262 iteration 10 with loss 0.06782377\n","Training at Epoch 262 iteration 11 with loss 0.06783139\n","Training at Epoch 262 iteration 12 with loss 0.068638325\n","Training at Epoch 262 iteration 13 with loss 0.069952235\n","Training at Epoch 262 iteration 14 with loss 0.06868122\n","Training at Epoch 262 iteration 15 with loss 0.066959254\n","Training at Epoch 262 iteration 16 with loss 0.06868411\n","Training at Epoch 262 iteration 17 with loss 0.07137964\n","Training at Epoch 262 iteration 18 with loss 0.06874636\n","Training at Epoch 262 iteration 19 with loss 0.070872605\n","Training at Epoch 262 iteration 20 with loss 0.07126468\n","Training at Epoch 262 iteration 21 with loss 0.06790225\n","Training at Epoch 262 iteration 22 with loss 0.070505485\n","Training at Epoch 262 iteration 23 with loss 0.070901886\n","Training at Epoch 262 iteration 24 with loss 0.06996029\n","Training at Epoch 262 iteration 25 with loss 0.06867099\n","Training at Epoch 262 iteration 26 with loss 0.067701206\n","Training at Epoch 262 iteration 27 with loss 0.068742186\n","Training at Epoch 262 iteration 28 with loss 0.06992911\n","Training at Epoch 262 iteration 29 with loss 0.06865252\n","Training at Epoch 262 iteration 30 with loss 0.07085513\n","Training at Epoch 263 iteration 0 with loss 0.070051596\n","Training at Epoch 263 iteration 1 with loss 0.06949021\n","Training at Epoch 263 iteration 2 with loss 0.06871413\n","Training at Epoch 263 iteration 3 with loss 0.071313836\n","Training at Epoch 263 iteration 4 with loss 0.0696483\n","Training at Epoch 263 iteration 5 with loss 0.06781073\n","Training at Epoch 263 iteration 6 with loss 0.06789799\n","Training at Epoch 263 iteration 7 with loss 0.0690186\n","Training at Epoch 263 iteration 8 with loss 0.06613989\n","Training at Epoch 263 iteration 9 with loss 0.06910092\n","Training at Epoch 263 iteration 10 with loss 0.06668441\n","Training at Epoch 263 iteration 11 with loss 0.070006214\n","Training at Epoch 263 iteration 12 with loss 0.06875958\n","Training at Epoch 263 iteration 13 with loss 0.0691974\n","Training at Epoch 263 iteration 14 with loss 0.068617225\n","Training at Epoch 263 iteration 15 with loss 0.069535516\n","Training at Epoch 263 iteration 16 with loss 0.06917864\n","Training at Epoch 263 iteration 17 with loss 0.06962935\n","Training at Epoch 263 iteration 18 with loss 0.06653559\n","Training at Epoch 263 iteration 19 with loss 0.06951477\n","Training at Epoch 263 iteration 20 with loss 0.06835792\n","Training at Epoch 263 iteration 21 with loss 0.06873696\n","Training at Epoch 263 iteration 22 with loss 0.07210578\n","Training at Epoch 263 iteration 23 with loss 0.069058456\n","Training at Epoch 263 iteration 24 with loss 0.071315795\n","Training at Epoch 263 iteration 25 with loss 0.06836091\n","Training at Epoch 263 iteration 26 with loss 0.06943466\n","Training at Epoch 263 iteration 27 with loss 0.06815611\n","Training at Epoch 263 iteration 28 with loss 0.07045698\n","Training at Epoch 263 iteration 29 with loss 0.0713526\n","Training at Epoch 263 iteration 30 with loss 0.06784351\n","Training at Epoch 264 iteration 0 with loss 0.0709206\n","Training at Epoch 264 iteration 1 with loss 0.067813106\n","Training at Epoch 264 iteration 2 with loss 0.06822805\n","Training at Epoch 264 iteration 3 with loss 0.06945141\n","Training at Epoch 264 iteration 4 with loss 0.070442215\n","Training at Epoch 264 iteration 5 with loss 0.06949426\n","Training at Epoch 264 iteration 6 with loss 0.06860227\n","Training at Epoch 264 iteration 7 with loss 0.067000724\n","Training at Epoch 264 iteration 8 with loss 0.069185756\n","Training at Epoch 264 iteration 9 with loss 0.07079782\n","Training at Epoch 264 iteration 10 with loss 0.06960255\n","Training at Epoch 264 iteration 11 with loss 0.06835359\n","Training at Epoch 264 iteration 12 with loss 0.06915421\n","Training at Epoch 264 iteration 13 with loss 0.069053456\n","Training at Epoch 264 iteration 14 with loss 0.0698759\n","Training at Epoch 264 iteration 15 with loss 0.06702323\n","Training at Epoch 264 iteration 16 with loss 0.06857843\n","Training at Epoch 264 iteration 17 with loss 0.068726666\n","Training at Epoch 264 iteration 18 with loss 0.06903105\n","Training at Epoch 264 iteration 19 with loss 0.06866191\n","Training at Epoch 264 iteration 20 with loss 0.06915977\n","Training at Epoch 264 iteration 21 with loss 0.0673929\n","Training at Epoch 264 iteration 22 with loss 0.07059737\n","Training at Epoch 264 iteration 23 with loss 0.069297\n","Training at Epoch 264 iteration 24 with loss 0.067757055\n","Training at Epoch 264 iteration 25 with loss 0.06907825\n","Training at Epoch 264 iteration 26 with loss 0.06958701\n","Training at Epoch 264 iteration 27 with loss 0.070455536\n","Training at Epoch 264 iteration 28 with loss 0.0699687\n","Training at Epoch 264 iteration 29 with loss 0.06906929\n","Training at Epoch 264 iteration 30 with loss 0.06830017\n","Training at Epoch 265 iteration 0 with loss 0.06957552\n","Training at Epoch 265 iteration 1 with loss 0.06983402\n","Training at Epoch 265 iteration 2 with loss 0.06669123\n","Training at Epoch 265 iteration 3 with loss 0.06996656\n","Training at Epoch 265 iteration 4 with loss 0.06837846\n","Training at Epoch 265 iteration 5 with loss 0.069112696\n","Training at Epoch 265 iteration 6 with loss 0.06868671\n","Training at Epoch 265 iteration 7 with loss 0.07039057\n","Training at Epoch 265 iteration 8 with loss 0.067889154\n","Training at Epoch 265 iteration 9 with loss 0.06794381\n","Training at Epoch 265 iteration 10 with loss 0.07077156\n","Training at Epoch 265 iteration 11 with loss 0.06874616\n","Training at Epoch 265 iteration 12 with loss 0.071116574\n","Training at Epoch 265 iteration 13 with loss 0.06781204\n","Training at Epoch 265 iteration 14 with loss 0.06739159\n","Training at Epoch 265 iteration 15 with loss 0.06818306\n","Training at Epoch 265 iteration 16 with loss 0.070054926\n","Training at Epoch 265 iteration 17 with loss 0.06880499\n","Training at Epoch 265 iteration 18 with loss 0.06949575\n","Training at Epoch 265 iteration 19 with loss 0.06866034\n","Training at Epoch 265 iteration 20 with loss 0.06838514\n","Training at Epoch 265 iteration 21 with loss 0.06951954\n","Training at Epoch 265 iteration 22 with loss 0.068713024\n","Training at Epoch 265 iteration 23 with loss 0.070101775\n","Training at Epoch 265 iteration 24 with loss 0.06983794\n","Training at Epoch 265 iteration 25 with loss 0.070029855\n","Training at Epoch 265 iteration 26 with loss 0.0694494\n","Training at Epoch 265 iteration 27 with loss 0.06997688\n","Training at Epoch 265 iteration 28 with loss 0.069917254\n","Training at Epoch 265 iteration 29 with loss 0.06873341\n","Training at Epoch 265 iteration 30 with loss 0.06925861\n","Training at Epoch 266 iteration 0 with loss 0.06869961\n","Training at Epoch 266 iteration 1 with loss 0.069408104\n","Training at Epoch 266 iteration 2 with loss 0.06834858\n","Training at Epoch 266 iteration 3 with loss 0.06786556\n","Training at Epoch 266 iteration 4 with loss 0.06993091\n","Training at Epoch 266 iteration 5 with loss 0.07165377\n","Training at Epoch 266 iteration 6 with loss 0.06911003\n","Training at Epoch 266 iteration 7 with loss 0.06959867\n","Training at Epoch 266 iteration 8 with loss 0.06752069\n","Training at Epoch 266 iteration 9 with loss 0.06791447\n","Training at Epoch 266 iteration 10 with loss 0.070402116\n","Training at Epoch 266 iteration 11 with loss 0.071185224\n","Training at Epoch 266 iteration 12 with loss 0.06833687\n","Training at Epoch 266 iteration 13 with loss 0.068686895\n","Training at Epoch 266 iteration 14 with loss 0.06829455\n","Training at Epoch 266 iteration 15 with loss 0.06786309\n","Training at Epoch 266 iteration 16 with loss 0.070680246\n","Training at Epoch 266 iteration 17 with loss 0.069094405\n","Training at Epoch 266 iteration 18 with loss 0.06957836\n","Training at Epoch 266 iteration 19 with loss 0.0690464\n","Training at Epoch 266 iteration 20 with loss 0.06957637\n","Training at Epoch 266 iteration 21 with loss 0.06750081\n","Training at Epoch 266 iteration 22 with loss 0.0699504\n","Training at Epoch 266 iteration 23 with loss 0.06831114\n","Training at Epoch 266 iteration 24 with loss 0.06980569\n","Training at Epoch 266 iteration 25 with loss 0.06989769\n","Training at Epoch 266 iteration 26 with loss 0.06911876\n","Training at Epoch 266 iteration 27 with loss 0.06916688\n","Training at Epoch 266 iteration 28 with loss 0.068705454\n","Training at Epoch 266 iteration 29 with loss 0.06960089\n","Training at Epoch 266 iteration 30 with loss 0.06787808\n","Training at Epoch 267 iteration 0 with loss 0.06960407\n","Training at Epoch 267 iteration 1 with loss 0.06916172\n","Training at Epoch 267 iteration 2 with loss 0.06904362\n","Training at Epoch 267 iteration 3 with loss 0.06991534\n","Training at Epoch 267 iteration 4 with loss 0.069878586\n","Training at Epoch 267 iteration 5 with loss 0.07105143\n","Training at Epoch 267 iteration 6 with loss 0.06952421\n","Training at Epoch 267 iteration 7 with loss 0.069946125\n","Training at Epoch 267 iteration 8 with loss 0.0690912\n","Training at Epoch 267 iteration 9 with loss 0.067513235\n","Training at Epoch 267 iteration 10 with loss 0.06832935\n","Training at Epoch 267 iteration 11 with loss 0.06835181\n","Training at Epoch 267 iteration 12 with loss 0.0680039\n","Training at Epoch 267 iteration 13 with loss 0.07017325\n","Training at Epoch 267 iteration 14 with loss 0.06906269\n","Training at Epoch 267 iteration 15 with loss 0.069468275\n","Training at Epoch 267 iteration 16 with loss 0.0687003\n","Training at Epoch 267 iteration 17 with loss 0.06711276\n","Training at Epoch 267 iteration 18 with loss 0.070311725\n","Training at Epoch 267 iteration 19 with loss 0.07065938\n","Training at Epoch 267 iteration 20 with loss 0.0671254\n","Training at Epoch 267 iteration 21 with loss 0.06797941\n","Training at Epoch 267 iteration 22 with loss 0.06838087\n","Training at Epoch 267 iteration 23 with loss 0.06671077\n","Training at Epoch 267 iteration 24 with loss 0.070269674\n","Training at Epoch 267 iteration 25 with loss 0.07024036\n","Training at Epoch 267 iteration 26 with loss 0.068749055\n","Training at Epoch 267 iteration 27 with loss 0.068304375\n","Training at Epoch 267 iteration 28 with loss 0.07097058\n","Training at Epoch 267 iteration 29 with loss 0.06991683\n","Training at Epoch 267 iteration 30 with loss 0.06836905\n","Training at Epoch 268 iteration 0 with loss 0.067942895\n","Training at Epoch 268 iteration 1 with loss 0.06990062\n","Training at Epoch 268 iteration 2 with loss 0.06991978\n","Training at Epoch 268 iteration 3 with loss 0.06716628\n","Training at Epoch 268 iteration 4 with loss 0.067603454\n","Training at Epoch 268 iteration 5 with loss 0.068385586\n","Training at Epoch 268 iteration 6 with loss 0.06794055\n","Training at Epoch 268 iteration 7 with loss 0.06985034\n","Training at Epoch 268 iteration 8 with loss 0.06799044\n","Training at Epoch 268 iteration 9 with loss 0.0697733\n","Training at Epoch 268 iteration 10 with loss 0.07028999\n","Training at Epoch 268 iteration 11 with loss 0.068632066\n","Training at Epoch 268 iteration 12 with loss 0.069594406\n","Training at Epoch 268 iteration 13 with loss 0.06665799\n","Training at Epoch 268 iteration 14 with loss 0.06992773\n","Training at Epoch 268 iteration 15 with loss 0.06949164\n","Training at Epoch 268 iteration 16 with loss 0.068265334\n","Training at Epoch 268 iteration 17 with loss 0.07028087\n","Training at Epoch 268 iteration 18 with loss 0.06952238\n","Training at Epoch 268 iteration 19 with loss 0.06829681\n","Training at Epoch 268 iteration 20 with loss 0.06988474\n","Training at Epoch 268 iteration 21 with loss 0.06786506\n","Training at Epoch 268 iteration 22 with loss 0.07107044\n","Training at Epoch 268 iteration 23 with loss 0.06873892\n","Training at Epoch 268 iteration 24 with loss 0.06826676\n","Training at Epoch 268 iteration 25 with loss 0.070024684\n","Training at Epoch 268 iteration 26 with loss 0.06986462\n","Training at Epoch 268 iteration 27 with loss 0.07003562\n","Training at Epoch 268 iteration 28 with loss 0.07113962\n","Training at Epoch 268 iteration 29 with loss 0.06824918\n","Training at Epoch 268 iteration 30 with loss 0.069483496\n","Training at Epoch 269 iteration 0 with loss 0.06878241\n","Training at Epoch 269 iteration 1 with loss 0.06794891\n","Training at Epoch 269 iteration 2 with loss 0.06949295\n","Training at Epoch 269 iteration 3 with loss 0.06827659\n","Training at Epoch 269 iteration 4 with loss 0.068878785\n","Training at Epoch 269 iteration 5 with loss 0.06714381\n","Training at Epoch 269 iteration 6 with loss 0.069607414\n","Training at Epoch 269 iteration 7 with loss 0.068280615\n","Training at Epoch 269 iteration 8 with loss 0.06782546\n","Training at Epoch 269 iteration 9 with loss 0.071259096\n","Training at Epoch 269 iteration 10 with loss 0.06945813\n","Training at Epoch 269 iteration 11 with loss 0.0695468\n","Training at Epoch 269 iteration 12 with loss 0.06942598\n","Training at Epoch 269 iteration 13 with loss 0.06994516\n","Training at Epoch 269 iteration 14 with loss 0.07115444\n","Training at Epoch 269 iteration 15 with loss 0.06993052\n","Training at Epoch 269 iteration 16 with loss 0.06804159\n","Training at Epoch 269 iteration 17 with loss 0.06872531\n","Training at Epoch 269 iteration 18 with loss 0.069052696\n","Training at Epoch 269 iteration 19 with loss 0.06740482\n","Training at Epoch 269 iteration 20 with loss 0.072626054\n","Training at Epoch 269 iteration 21 with loss 0.06917412\n","Training at Epoch 269 iteration 22 with loss 0.06942882\n","Training at Epoch 269 iteration 23 with loss 0.06951677\n","Training at Epoch 269 iteration 24 with loss 0.06912786\n","Training at Epoch 269 iteration 25 with loss 0.067414165\n","Training at Epoch 269 iteration 26 with loss 0.06859758\n","Training at Epoch 269 iteration 27 with loss 0.067401424\n","Training at Epoch 269 iteration 28 with loss 0.06835429\n","Training at Epoch 269 iteration 29 with loss 0.07018322\n","Training at Epoch 269 iteration 30 with loss 0.069549695\n","Training at Epoch 270 iteration 0 with loss 0.07075335\n","Training at Epoch 270 iteration 1 with loss 0.068803646\n","Training at Epoch 270 iteration 2 with loss 0.07077648\n","Training at Epoch 270 iteration 3 with loss 0.068694234\n","Training at Epoch 270 iteration 4 with loss 0.0688185\n","Training at Epoch 270 iteration 5 with loss 0.06925504\n","Training at Epoch 270 iteration 6 with loss 0.06999302\n","Training at Epoch 270 iteration 7 with loss 0.06837198\n","Training at Epoch 270 iteration 8 with loss 0.0683608\n","Training at Epoch 270 iteration 9 with loss 0.06829695\n","Training at Epoch 270 iteration 10 with loss 0.0696044\n","Training at Epoch 270 iteration 11 with loss 0.068616614\n","Training at Epoch 270 iteration 12 with loss 0.06991868\n","Training at Epoch 270 iteration 13 with loss 0.06962316\n","Training at Epoch 270 iteration 14 with loss 0.07036162\n","Training at Epoch 270 iteration 15 with loss 0.06860075\n","Training at Epoch 270 iteration 16 with loss 0.070069715\n","Training at Epoch 270 iteration 17 with loss 0.06867592\n","Training at Epoch 270 iteration 18 with loss 0.06762733\n","Training at Epoch 270 iteration 19 with loss 0.06808774\n","Training at Epoch 270 iteration 20 with loss 0.06915401\n","Training at Epoch 270 iteration 21 with loss 0.07028533\n","Training at Epoch 270 iteration 22 with loss 0.06906571\n","Training at Epoch 270 iteration 23 with loss 0.070209816\n","Training at Epoch 270 iteration 24 with loss 0.06881702\n","Training at Epoch 270 iteration 25 with loss 0.06841484\n","Training at Epoch 270 iteration 26 with loss 0.06779446\n","Training at Epoch 270 iteration 27 with loss 0.069011\n","Training at Epoch 270 iteration 28 with loss 0.06830917\n","Training at Epoch 270 iteration 29 with loss 0.069145374\n","Training at Epoch 270 iteration 30 with loss 0.06991348\n","Training at Epoch 271 iteration 0 with loss 0.06798704\n","Training at Epoch 271 iteration 1 with loss 0.06832136\n","Training at Epoch 271 iteration 2 with loss 0.06757712\n","Training at Epoch 271 iteration 3 with loss 0.06876578\n","Training at Epoch 271 iteration 4 with loss 0.06907243\n","Training at Epoch 271 iteration 5 with loss 0.069573894\n","Training at Epoch 271 iteration 6 with loss 0.06909063\n","Training at Epoch 271 iteration 7 with loss 0.07020585\n","Training at Epoch 271 iteration 8 with loss 0.07035314\n","Training at Epoch 271 iteration 9 with loss 0.06905169\n","Training at Epoch 271 iteration 10 with loss 0.06958498\n","Training at Epoch 271 iteration 11 with loss 0.069093674\n","Training at Epoch 271 iteration 12 with loss 0.069957696\n","Training at Epoch 271 iteration 13 with loss 0.06950062\n","Training at Epoch 271 iteration 14 with loss 0.06982049\n","Training at Epoch 271 iteration 15 with loss 0.06949495\n","Training at Epoch 271 iteration 16 with loss 0.068380065\n","Training at Epoch 271 iteration 17 with loss 0.06790443\n","Training at Epoch 271 iteration 18 with loss 0.068786144\n","Training at Epoch 271 iteration 19 with loss 0.06912927\n","Training at Epoch 271 iteration 20 with loss 0.07192682\n","Training at Epoch 271 iteration 21 with loss 0.06834271\n","Training at Epoch 271 iteration 22 with loss 0.067106284\n","Training at Epoch 271 iteration 23 with loss 0.07230641\n","Training at Epoch 271 iteration 24 with loss 0.06837396\n","Training at Epoch 271 iteration 25 with loss 0.06985266\n","Training at Epoch 271 iteration 26 with loss 0.06870589\n","Training at Epoch 271 iteration 27 with loss 0.06986705\n","Training at Epoch 271 iteration 28 with loss 0.0675362\n","Training at Epoch 271 iteration 29 with loss 0.067534104\n","Training at Epoch 271 iteration 30 with loss 0.069098614\n","Training at Epoch 272 iteration 0 with loss 0.07145859\n","Training at Epoch 272 iteration 1 with loss 0.06910051\n","Training at Epoch 272 iteration 2 with loss 0.06756881\n","Training at Epoch 272 iteration 3 with loss 0.068722025\n","Training at Epoch 272 iteration 4 with loss 0.06795327\n","Training at Epoch 272 iteration 5 with loss 0.06911289\n","Training at Epoch 272 iteration 6 with loss 0.07024721\n","Training at Epoch 272 iteration 7 with loss 0.06836372\n","Training at Epoch 272 iteration 8 with loss 0.06875652\n","Training at Epoch 272 iteration 9 with loss 0.06834581\n","Training at Epoch 272 iteration 10 with loss 0.06793291\n","Training at Epoch 272 iteration 11 with loss 0.06907793\n","Training at Epoch 272 iteration 12 with loss 0.06869006\n","Training at Epoch 272 iteration 13 with loss 0.06915876\n","Training at Epoch 272 iteration 14 with loss 0.06751193\n","Training at Epoch 272 iteration 15 with loss 0.06914896\n","Training at Epoch 272 iteration 16 with loss 0.068436556\n","Training at Epoch 272 iteration 17 with loss 0.070256785\n","Training at Epoch 272 iteration 18 with loss 0.06913154\n","Training at Epoch 272 iteration 19 with loss 0.06911367\n","Training at Epoch 272 iteration 20 with loss 0.07069388\n","Training at Epoch 272 iteration 21 with loss 0.06909015\n","Training at Epoch 272 iteration 22 with loss 0.06991351\n","Training at Epoch 272 iteration 23 with loss 0.06953827\n","Training at Epoch 272 iteration 24 with loss 0.069114044\n","Training at Epoch 272 iteration 25 with loss 0.07227287\n","Training at Epoch 272 iteration 26 with loss 0.06751884\n","Training at Epoch 272 iteration 27 with loss 0.06953315\n","Training at Epoch 272 iteration 28 with loss 0.06829883\n","Training at Epoch 272 iteration 29 with loss 0.070678934\n","Training at Epoch 272 iteration 30 with loss 0.067538425\n","Training at Epoch 273 iteration 0 with loss 0.067941055\n","Training at Epoch 273 iteration 1 with loss 0.067943946\n","Training at Epoch 273 iteration 2 with loss 0.06990321\n","Training at Epoch 273 iteration 3 with loss 0.069432445\n","Training at Epoch 273 iteration 4 with loss 0.066714585\n","Training at Epoch 273 iteration 5 with loss 0.06989668\n","Training at Epoch 273 iteration 6 with loss 0.0671453\n","Training at Epoch 273 iteration 7 with loss 0.069139875\n","Training at Epoch 273 iteration 8 with loss 0.067946814\n","Training at Epoch 273 iteration 9 with loss 0.06793939\n","Training at Epoch 273 iteration 10 with loss 0.07072737\n","Training at Epoch 273 iteration 11 with loss 0.06868924\n","Training at Epoch 273 iteration 12 with loss 0.07031391\n","Training at Epoch 273 iteration 13 with loss 0.06797347\n","Training at Epoch 273 iteration 14 with loss 0.06910603\n","Training at Epoch 273 iteration 15 with loss 0.069914\n","Training at Epoch 273 iteration 16 with loss 0.06875026\n","Training at Epoch 273 iteration 17 with loss 0.06788848\n","Training at Epoch 273 iteration 18 with loss 0.07034554\n","Training at Epoch 273 iteration 19 with loss 0.07070642\n","Training at Epoch 273 iteration 20 with loss 0.06871762\n","Training at Epoch 273 iteration 21 with loss 0.06912489\n","Training at Epoch 273 iteration 22 with loss 0.06833376\n","Training at Epoch 273 iteration 23 with loss 0.070348576\n","Training at Epoch 273 iteration 24 with loss 0.06912906\n","Training at Epoch 273 iteration 25 with loss 0.06994693\n","Training at Epoch 273 iteration 26 with loss 0.06955531\n","Training at Epoch 273 iteration 27 with loss 0.0703224\n","Training at Epoch 273 iteration 28 with loss 0.06744795\n","Training at Epoch 273 iteration 29 with loss 0.07035097\n","Training at Epoch 273 iteration 30 with loss 0.070704095\n","Training at Epoch 274 iteration 0 with loss 0.07038066\n","Training at Epoch 274 iteration 1 with loss 0.06996164\n","Training at Epoch 274 iteration 2 with loss 0.06790149\n","Training at Epoch 274 iteration 3 with loss 0.06991306\n","Training at Epoch 274 iteration 4 with loss 0.06833573\n","Training at Epoch 274 iteration 5 with loss 0.07033887\n","Training at Epoch 274 iteration 6 with loss 0.06909977\n","Training at Epoch 274 iteration 7 with loss 0.06875672\n","Training at Epoch 274 iteration 8 with loss 0.069146484\n","Training at Epoch 274 iteration 9 with loss 0.06832264\n","Training at Epoch 274 iteration 10 with loss 0.07033421\n","Training at Epoch 274 iteration 11 with loss 0.06831569\n","Training at Epoch 274 iteration 12 with loss 0.06793726\n","Training at Epoch 274 iteration 13 with loss 0.069091395\n","Training at Epoch 274 iteration 14 with loss 0.070309415\n","Training at Epoch 274 iteration 15 with loss 0.06913912\n","Training at Epoch 274 iteration 16 with loss 0.06790644\n","Training at Epoch 274 iteration 17 with loss 0.070285544\n","Training at Epoch 274 iteration 18 with loss 0.069897756\n","Training at Epoch 274 iteration 19 with loss 0.06833343\n","Training at Epoch 274 iteration 20 with loss 0.06874801\n","Training at Epoch 274 iteration 21 with loss 0.068303995\n","Training at Epoch 274 iteration 22 with loss 0.07031144\n","Training at Epoch 274 iteration 23 with loss 0.06831153\n","Training at Epoch 274 iteration 24 with loss 0.069128886\n","Training at Epoch 274 iteration 25 with loss 0.06871885\n","Training at Epoch 274 iteration 26 with loss 0.06987281\n","Training at Epoch 274 iteration 27 with loss 0.07070373\n","Training at Epoch 274 iteration 28 with loss 0.06677978\n","Training at Epoch 274 iteration 29 with loss 0.07034806\n","Training at Epoch 274 iteration 30 with loss 0.067934394\n","Training at Epoch 275 iteration 0 with loss 0.06792358\n","Training at Epoch 275 iteration 1 with loss 0.06793608\n","Training at Epoch 275 iteration 2 with loss 0.06991697\n","Training at Epoch 275 iteration 3 with loss 0.069489315\n","Training at Epoch 275 iteration 4 with loss 0.06829899\n","Training at Epoch 275 iteration 5 with loss 0.069069445\n","Training at Epoch 275 iteration 6 with loss 0.06713905\n","Training at Epoch 275 iteration 7 with loss 0.07152275\n","Training at Epoch 275 iteration 8 with loss 0.06756007\n","Training at Epoch 275 iteration 9 with loss 0.06958352\n","Training at Epoch 275 iteration 10 with loss 0.06831298\n","Training at Epoch 275 iteration 11 with loss 0.07069925\n","Training at Epoch 275 iteration 12 with loss 0.069563314\n","Training at Epoch 275 iteration 13 with loss 0.06753091\n","Training at Epoch 275 iteration 14 with loss 0.0686966\n","Training at Epoch 275 iteration 15 with loss 0.069520146\n","Training at Epoch 275 iteration 16 with loss 0.069903225\n","Training at Epoch 275 iteration 17 with loss 0.06993629\n","Training at Epoch 275 iteration 18 with loss 0.06868096\n","Training at Epoch 275 iteration 19 with loss 0.068737924\n","Training at Epoch 275 iteration 20 with loss 0.069934756\n","Training at Epoch 275 iteration 21 with loss 0.068732575\n","Training at Epoch 275 iteration 22 with loss 0.06993063\n","Training at Epoch 275 iteration 23 with loss 0.06832917\n","Training at Epoch 275 iteration 24 with loss 0.06872634\n","Training at Epoch 275 iteration 25 with loss 0.06908141\n","Training at Epoch 275 iteration 26 with loss 0.069912836\n","Training at Epoch 275 iteration 27 with loss 0.06910489\n","Training at Epoch 275 iteration 28 with loss 0.06911806\n","Training at Epoch 275 iteration 29 with loss 0.06950225\n","Training at Epoch 275 iteration 30 with loss 0.06949277\n","Training at Epoch 276 iteration 0 with loss 0.06833717\n","Training at Epoch 276 iteration 1 with loss 0.0667202\n","Training at Epoch 276 iteration 2 with loss 0.069936976\n","Training at Epoch 276 iteration 3 with loss 0.06872067\n","Training at Epoch 276 iteration 4 with loss 0.06951213\n","Training at Epoch 276 iteration 5 with loss 0.067961045\n","Training at Epoch 276 iteration 6 with loss 0.070711456\n","Training at Epoch 276 iteration 7 with loss 0.068714425\n","Training at Epoch 276 iteration 8 with loss 0.07075657\n","Training at Epoch 276 iteration 9 with loss 0.06990526\n","Training at Epoch 276 iteration 10 with loss 0.07109647\n","Training at Epoch 276 iteration 11 with loss 0.06752758\n","Training at Epoch 276 iteration 12 with loss 0.0699404\n","Training at Epoch 276 iteration 13 with loss 0.06793053\n","Training at Epoch 276 iteration 14 with loss 0.06949693\n","Training at Epoch 276 iteration 15 with loss 0.06869431\n","Training at Epoch 276 iteration 16 with loss 0.06872039\n","Training at Epoch 276 iteration 17 with loss 0.067505315\n","Training at Epoch 276 iteration 18 with loss 0.069926925\n","Training at Epoch 276 iteration 19 with loss 0.068333186\n","Training at Epoch 276 iteration 20 with loss 0.06913147\n","Training at Epoch 276 iteration 21 with loss 0.06790447\n","Training at Epoch 276 iteration 22 with loss 0.069502555\n","Training at Epoch 276 iteration 23 with loss 0.069501735\n","Training at Epoch 276 iteration 24 with loss 0.069963366\n","Training at Epoch 276 iteration 25 with loss 0.071505755\n","Training at Epoch 276 iteration 26 with loss 0.06632132\n","Training at Epoch 276 iteration 27 with loss 0.069120534\n","Training at Epoch 276 iteration 28 with loss 0.06754621\n","Training at Epoch 276 iteration 29 with loss 0.07027855\n","Training at Epoch 276 iteration 30 with loss 0.07073312\n","Training at Epoch 277 iteration 0 with loss 0.06753174\n","Training at Epoch 277 iteration 1 with loss 0.06952529\n","Training at Epoch 277 iteration 2 with loss 0.06914352\n","Training at Epoch 277 iteration 3 with loss 0.068242535\n","Training at Epoch 277 iteration 4 with loss 0.06871598\n","Training at Epoch 277 iteration 5 with loss 0.06956187\n","Training at Epoch 277 iteration 6 with loss 0.06995016\n","Training at Epoch 277 iteration 7 with loss 0.06912782\n","Training at Epoch 277 iteration 8 with loss 0.06791963\n","Training at Epoch 277 iteration 9 with loss 0.06952538\n","Training at Epoch 277 iteration 10 with loss 0.06831644\n","Training at Epoch 277 iteration 11 with loss 0.06790278\n","Training at Epoch 277 iteration 12 with loss 0.07035309\n","Training at Epoch 277 iteration 13 with loss 0.06952212\n","Training at Epoch 277 iteration 14 with loss 0.067115195\n","Training at Epoch 277 iteration 15 with loss 0.06916701\n","Training at Epoch 277 iteration 16 with loss 0.07074603\n","Training at Epoch 277 iteration 17 with loss 0.06911807\n","Training at Epoch 277 iteration 18 with loss 0.06791078\n","Training at Epoch 277 iteration 19 with loss 0.0698901\n","Training at Epoch 277 iteration 20 with loss 0.06991896\n","Training at Epoch 277 iteration 21 with loss 0.06954807\n","Training at Epoch 277 iteration 22 with loss 0.0710863\n","Training at Epoch 277 iteration 23 with loss 0.06874119\n","Training at Epoch 277 iteration 24 with loss 0.06914289\n","Training at Epoch 277 iteration 25 with loss 0.07113009\n","Training at Epoch 277 iteration 26 with loss 0.0670858\n","Training at Epoch 277 iteration 27 with loss 0.069100425\n","Training at Epoch 277 iteration 28 with loss 0.06791687\n","Training at Epoch 277 iteration 29 with loss 0.069948465\n","Training at Epoch 277 iteration 30 with loss 0.06830952\n","Training at Epoch 278 iteration 0 with loss 0.06748879\n","Training at Epoch 278 iteration 1 with loss 0.06952176\n","Training at Epoch 278 iteration 2 with loss 0.06912028\n","Training at Epoch 278 iteration 3 with loss 0.06751554\n","Training at Epoch 278 iteration 4 with loss 0.07034524\n","Training at Epoch 278 iteration 5 with loss 0.06669627\n","Training at Epoch 278 iteration 6 with loss 0.06832436\n","Training at Epoch 278 iteration 7 with loss 0.07030288\n","Training at Epoch 278 iteration 8 with loss 0.06913887\n","Training at Epoch 278 iteration 9 with loss 0.068281546\n","Training at Epoch 278 iteration 10 with loss 0.0691268\n","Training at Epoch 278 iteration 11 with loss 0.06994906\n","Training at Epoch 278 iteration 12 with loss 0.069946125\n","Training at Epoch 278 iteration 13 with loss 0.071599945\n","Training at Epoch 278 iteration 14 with loss 0.069507316\n","Training at Epoch 278 iteration 15 with loss 0.06747595\n","Training at Epoch 278 iteration 16 with loss 0.06748818\n","Training at Epoch 278 iteration 17 with loss 0.0699363\n","Training at Epoch 278 iteration 18 with loss 0.066639\n","Training at Epoch 278 iteration 19 with loss 0.06912486\n","Training at Epoch 278 iteration 20 with loss 0.06871674\n","Training at Epoch 278 iteration 21 with loss 0.070355944\n","Training at Epoch 278 iteration 22 with loss 0.06994505\n","Training at Epoch 278 iteration 23 with loss 0.06868801\n","Training at Epoch 278 iteration 24 with loss 0.06786822\n","Training at Epoch 278 iteration 25 with loss 0.07118397\n","Training at Epoch 278 iteration 26 with loss 0.068710126\n","Training at Epoch 278 iteration 27 with loss 0.06912139\n","Training at Epoch 278 iteration 28 with loss 0.069949746\n","Training at Epoch 278 iteration 29 with loss 0.07037555\n","Training at Epoch 278 iteration 30 with loss 0.069114126\n","Training at Epoch 279 iteration 0 with loss 0.069537565\n","Training at Epoch 279 iteration 1 with loss 0.069979854\n","Training at Epoch 279 iteration 2 with loss 0.06701285\n","Training at Epoch 279 iteration 3 with loss 0.069533095\n","Training at Epoch 279 iteration 4 with loss 0.06996541\n","Training at Epoch 279 iteration 5 with loss 0.06829561\n","Training at Epoch 279 iteration 6 with loss 0.06915229\n","Training at Epoch 279 iteration 7 with loss 0.06706747\n","Training at Epoch 279 iteration 8 with loss 0.06912873\n","Training at Epoch 279 iteration 9 with loss 0.068343\n","Training at Epoch 279 iteration 10 with loss 0.06905474\n","Training at Epoch 279 iteration 11 with loss 0.070398405\n","Training at Epoch 279 iteration 12 with loss 0.07033385\n","Training at Epoch 279 iteration 13 with loss 0.06867149\n","Training at Epoch 279 iteration 14 with loss 0.068758205\n","Training at Epoch 279 iteration 15 with loss 0.06868146\n","Training at Epoch 279 iteration 16 with loss 0.06787229\n","Training at Epoch 279 iteration 17 with loss 0.06826854\n","Training at Epoch 279 iteration 18 with loss 0.06908682\n","Training at Epoch 279 iteration 19 with loss 0.06870286\n","Training at Epoch 279 iteration 20 with loss 0.06998545\n","Training at Epoch 279 iteration 21 with loss 0.07037867\n","Training at Epoch 279 iteration 22 with loss 0.06830372\n","Training at Epoch 279 iteration 23 with loss 0.07164083\n","Training at Epoch 279 iteration 24 with loss 0.06914882\n","Training at Epoch 279 iteration 25 with loss 0.06996603\n","Training at Epoch 279 iteration 26 with loss 0.07036002\n","Training at Epoch 279 iteration 27 with loss 0.06826055\n","Training at Epoch 279 iteration 28 with loss 0.06865437\n","Training at Epoch 279 iteration 29 with loss 0.06783009\n","Training at Epoch 279 iteration 30 with loss 0.06962525\n","Training at Epoch 280 iteration 0 with loss 0.06782214\n","Training at Epoch 280 iteration 1 with loss 0.069137715\n","Training at Epoch 280 iteration 2 with loss 0.06991925\n","Training at Epoch 280 iteration 3 with loss 0.066582575\n","Training at Epoch 280 iteration 4 with loss 0.071233325\n","Training at Epoch 280 iteration 5 with loss 0.06997226\n","Training at Epoch 280 iteration 6 with loss 0.068725996\n","Training at Epoch 280 iteration 7 with loss 0.0678458\n","Training at Epoch 280 iteration 8 with loss 0.06949284\n","Training at Epoch 280 iteration 9 with loss 0.06993054\n","Training at Epoch 280 iteration 10 with loss 0.06910981\n","Training at Epoch 280 iteration 11 with loss 0.06908454\n","Training at Epoch 280 iteration 12 with loss 0.06784526\n","Training at Epoch 280 iteration 13 with loss 0.068722464\n","Training at Epoch 280 iteration 14 with loss 0.070803575\n","Training at Epoch 280 iteration 15 with loss 0.07167493\n","Training at Epoch 280 iteration 16 with loss 0.06911207\n","Training at Epoch 280 iteration 17 with loss 0.068679325\n","Training at Epoch 280 iteration 18 with loss 0.069116585\n","Training at Epoch 280 iteration 19 with loss 0.07121095\n","Training at Epoch 280 iteration 20 with loss 0.06736674\n","Training at Epoch 280 iteration 21 with loss 0.06742437\n","Training at Epoch 280 iteration 22 with loss 0.06654513\n","Training at Epoch 280 iteration 23 with loss 0.06989731\n","Training at Epoch 280 iteration 24 with loss 0.06660272\n","Training at Epoch 280 iteration 25 with loss 0.070351824\n","Training at Epoch 280 iteration 26 with loss 0.071191624\n","Training at Epoch 280 iteration 27 with loss 0.069560416\n","Training at Epoch 280 iteration 28 with loss 0.07042343\n","Training at Epoch 280 iteration 29 with loss 0.067471154\n","Training at Epoch 280 iteration 30 with loss 0.06954849\n","Training at Epoch 281 iteration 0 with loss 0.06654035\n","Training at Epoch 281 iteration 1 with loss 0.068771146\n","Training at Epoch 281 iteration 2 with loss 0.06870767\n","Training at Epoch 281 iteration 3 with loss 0.06867591\n","Training at Epoch 281 iteration 4 with loss 0.06955929\n","Training at Epoch 281 iteration 5 with loss 0.068278864\n","Training at Epoch 281 iteration 6 with loss 0.071704544\n","Training at Epoch 281 iteration 7 with loss 0.069950834\n","Training at Epoch 281 iteration 8 with loss 0.068290874\n","Training at Epoch 281 iteration 9 with loss 0.06708126\n","Training at Epoch 281 iteration 10 with loss 0.06912704\n","Training at Epoch 281 iteration 11 with loss 0.068716854\n","Training at Epoch 281 iteration 12 with loss 0.06957474\n","Training at Epoch 281 iteration 13 with loss 0.070344605\n","Training at Epoch 281 iteration 14 with loss 0.06954924\n","Training at Epoch 281 iteration 15 with loss 0.068649754\n","Training at Epoch 281 iteration 16 with loss 0.06742628\n","Training at Epoch 281 iteration 17 with loss 0.06949491\n","Training at Epoch 281 iteration 18 with loss 0.06868736\n","Training at Epoch 281 iteration 19 with loss 0.06995134\n","Training at Epoch 281 iteration 20 with loss 0.068298034\n","Training at Epoch 281 iteration 21 with loss 0.06861549\n","Training at Epoch 281 iteration 22 with loss 0.069045834\n","Training at Epoch 281 iteration 23 with loss 0.06781633\n","Training at Epoch 281 iteration 24 with loss 0.0707854\n","Training at Epoch 281 iteration 25 with loss 0.068723336\n","Training at Epoch 281 iteration 26 with loss 0.06960033\n","Training at Epoch 281 iteration 27 with loss 0.068774894\n","Training at Epoch 281 iteration 28 with loss 0.07212236\n","Training at Epoch 281 iteration 29 with loss 0.068713844\n","Training at Epoch 281 iteration 30 with loss 0.070371345\n","Training at Epoch 282 iteration 0 with loss 0.06936885\n","Training at Epoch 282 iteration 1 with loss 0.07089185\n","Training at Epoch 282 iteration 2 with loss 0.07077353\n","Training at Epoch 282 iteration 3 with loss 0.067411564\n","Training at Epoch 282 iteration 4 with loss 0.06998518\n","Training at Epoch 282 iteration 5 with loss 0.0687976\n","Training at Epoch 282 iteration 6 with loss 0.06924066\n","Training at Epoch 282 iteration 7 with loss 0.07135326\n","Training at Epoch 282 iteration 8 with loss 0.06865628\n","Training at Epoch 282 iteration 9 with loss 0.06831031\n","Training at Epoch 282 iteration 10 with loss 0.06987897\n","Training at Epoch 282 iteration 11 with loss 0.0678656\n","Training at Epoch 282 iteration 12 with loss 0.0679653\n","Training at Epoch 282 iteration 13 with loss 0.06900888\n","Training at Epoch 282 iteration 14 with loss 0.0690982\n","Training at Epoch 282 iteration 15 with loss 0.06823249\n","Training at Epoch 282 iteration 16 with loss 0.069595136\n","Training at Epoch 282 iteration 17 with loss 0.06614989\n","Training at Epoch 282 iteration 18 with loss 0.06917833\n","Training at Epoch 282 iteration 19 with loss 0.06871806\n","Training at Epoch 282 iteration 20 with loss 0.06866398\n","Training at Epoch 282 iteration 21 with loss 0.068678096\n","Training at Epoch 282 iteration 22 with loss 0.068252265\n","Training at Epoch 282 iteration 23 with loss 0.069561124\n","Training at Epoch 282 iteration 24 with loss 0.06779965\n","Training at Epoch 282 iteration 25 with loss 0.06995223\n","Training at Epoch 282 iteration 26 with loss 0.06776138\n","Training at Epoch 282 iteration 27 with loss 0.06868588\n","Training at Epoch 282 iteration 28 with loss 0.07094434\n","Training at Epoch 282 iteration 29 with loss 0.069941595\n","Training at Epoch 282 iteration 30 with loss 0.07178144\n","Training at Epoch 283 iteration 0 with loss 0.06914799\n","Training at Epoch 283 iteration 1 with loss 0.07034155\n","Training at Epoch 283 iteration 2 with loss 0.06689291\n","Training at Epoch 283 iteration 3 with loss 0.06858987\n","Training at Epoch 283 iteration 4 with loss 0.06995024\n","Training at Epoch 283 iteration 5 with loss 0.06870642\n","Training at Epoch 283 iteration 6 with loss 0.06950491\n","Training at Epoch 283 iteration 7 with loss 0.07005441\n","Training at Epoch 283 iteration 8 with loss 0.06841466\n","Training at Epoch 283 iteration 9 with loss 0.06787137\n","Training at Epoch 283 iteration 10 with loss 0.07048297\n","Training at Epoch 283 iteration 11 with loss 0.070908554\n","Training at Epoch 283 iteration 12 with loss 0.06976839\n","Training at Epoch 283 iteration 13 with loss 0.06783674\n","Training at Epoch 283 iteration 14 with loss 0.069107674\n","Training at Epoch 283 iteration 15 with loss 0.06955873\n","Training at Epoch 283 iteration 16 with loss 0.07039143\n","Training at Epoch 283 iteration 17 with loss 0.06915282\n","Training at Epoch 283 iteration 18 with loss 0.06791519\n","Training at Epoch 283 iteration 19 with loss 0.06943215\n","Training at Epoch 283 iteration 20 with loss 0.06952007\n","Training at Epoch 283 iteration 21 with loss 0.06950804\n","Training at Epoch 283 iteration 22 with loss 0.06873259\n","Training at Epoch 283 iteration 23 with loss 0.06942703\n","Training at Epoch 283 iteration 24 with loss 0.06753077\n","Training at Epoch 283 iteration 25 with loss 0.068460554\n","Training at Epoch 283 iteration 26 with loss 0.06776498\n","Training at Epoch 283 iteration 27 with loss 0.06912763\n","Training at Epoch 283 iteration 28 with loss 0.06963917\n","Training at Epoch 283 iteration 29 with loss 0.06860806\n","Training at Epoch 283 iteration 30 with loss 0.06906761\n","Training at Epoch 284 iteration 0 with loss 0.07118841\n","Training at Epoch 284 iteration 1 with loss 0.068290286\n","Training at Epoch 284 iteration 2 with loss 0.07177858\n","Training at Epoch 284 iteration 3 with loss 0.070055924\n","Training at Epoch 284 iteration 4 with loss 0.07001766\n","Training at Epoch 284 iteration 5 with loss 0.06819397\n","Training at Epoch 284 iteration 6 with loss 0.06715934\n","Training at Epoch 284 iteration 7 with loss 0.06818743\n","Training at Epoch 284 iteration 8 with loss 0.0682653\n","Training at Epoch 284 iteration 9 with loss 0.06951668\n","Training at Epoch 284 iteration 10 with loss 0.0694866\n","Training at Epoch 284 iteration 11 with loss 0.070406325\n","Training at Epoch 284 iteration 12 with loss 0.06926518\n","Training at Epoch 284 iteration 13 with loss 0.07072681\n","Training at Epoch 284 iteration 14 with loss 0.06862076\n","Training at Epoch 284 iteration 15 with loss 0.06822979\n","Training at Epoch 284 iteration 16 with loss 0.06821564\n","Training at Epoch 284 iteration 17 with loss 0.06830484\n","Training at Epoch 284 iteration 18 with loss 0.069824256\n","Training at Epoch 284 iteration 19 with loss 0.068709634\n","Training at Epoch 284 iteration 20 with loss 0.06831315\n","Training at Epoch 284 iteration 21 with loss 0.06814687\n","Training at Epoch 284 iteration 22 with loss 0.067034744\n","Training at Epoch 284 iteration 23 with loss 0.06922285\n","Training at Epoch 284 iteration 24 with loss 0.07073901\n","Training at Epoch 284 iteration 25 with loss 0.068656\n","Training at Epoch 284 iteration 26 with loss 0.069949895\n","Training at Epoch 284 iteration 27 with loss 0.071217336\n","Training at Epoch 284 iteration 28 with loss 0.06715877\n","Training at Epoch 284 iteration 29 with loss 0.06836547\n","Training at Epoch 284 iteration 30 with loss 0.06831519\n","Training at Epoch 285 iteration 0 with loss 0.069443926\n","Training at Epoch 285 iteration 1 with loss 0.070014685\n","Training at Epoch 285 iteration 2 with loss 0.067496784\n","Training at Epoch 285 iteration 3 with loss 0.067837715\n","Training at Epoch 285 iteration 4 with loss 0.06835954\n","Training at Epoch 285 iteration 5 with loss 0.0711665\n","Training at Epoch 285 iteration 6 with loss 0.07115166\n","Training at Epoch 285 iteration 7 with loss 0.06698439\n","Training at Epoch 285 iteration 8 with loss 0.06955027\n","Training at Epoch 285 iteration 9 with loss 0.06952126\n","Training at Epoch 285 iteration 10 with loss 0.06708537\n","Training at Epoch 285 iteration 11 with loss 0.06952361\n","Training at Epoch 285 iteration 12 with loss 0.069472924\n","Training at Epoch 285 iteration 13 with loss 0.07002835\n","Training at Epoch 285 iteration 14 with loss 0.06777291\n","Training at Epoch 285 iteration 15 with loss 0.0690142\n","Training at Epoch 285 iteration 16 with loss 0.068713605\n","Training at Epoch 285 iteration 17 with loss 0.069553174\n","Training at Epoch 285 iteration 18 with loss 0.06877695\n","Training at Epoch 285 iteration 19 with loss 0.06912046\n","Training at Epoch 285 iteration 20 with loss 0.07040048\n","Training at Epoch 285 iteration 21 with loss 0.069892354\n","Training at Epoch 285 iteration 22 with loss 0.07160846\n","Training at Epoch 285 iteration 23 with loss 0.06866105\n","Training at Epoch 285 iteration 24 with loss 0.069857016\n","Training at Epoch 285 iteration 25 with loss 0.069147885\n","Training at Epoch 285 iteration 26 with loss 0.06840837\n","Training at Epoch 285 iteration 27 with loss 0.06792301\n","Training at Epoch 285 iteration 28 with loss 0.06832374\n","Training at Epoch 285 iteration 29 with loss 0.0682512\n","Training at Epoch 285 iteration 30 with loss 0.06943064\n","Training at Epoch 286 iteration 0 with loss 0.06787057\n","Training at Epoch 286 iteration 1 with loss 0.067868575\n","Training at Epoch 286 iteration 2 with loss 0.06788288\n","Training at Epoch 286 iteration 3 with loss 0.06991543\n","Training at Epoch 286 iteration 4 with loss 0.069456615\n","Training at Epoch 286 iteration 5 with loss 0.06672014\n","Training at Epoch 286 iteration 6 with loss 0.06948886\n","Training at Epoch 286 iteration 7 with loss 0.06834777\n","Training at Epoch 286 iteration 8 with loss 0.0676687\n","Training at Epoch 286 iteration 9 with loss 0.071714014\n","Training at Epoch 286 iteration 10 with loss 0.07070949\n","Training at Epoch 286 iteration 11 with loss 0.07113216\n","Training at Epoch 286 iteration 12 with loss 0.0699265\n","Training at Epoch 286 iteration 13 with loss 0.07069341\n","Training at Epoch 286 iteration 14 with loss 0.069190584\n","Training at Epoch 286 iteration 15 with loss 0.06740915\n","Training at Epoch 286 iteration 16 with loss 0.06863375\n","Training at Epoch 286 iteration 17 with loss 0.069444194\n","Training at Epoch 286 iteration 18 with loss 0.068313144\n","Training at Epoch 286 iteration 19 with loss 0.07037198\n","Training at Epoch 286 iteration 20 with loss 0.0684083\n","Training at Epoch 286 iteration 21 with loss 0.0714954\n","Training at Epoch 286 iteration 22 with loss 0.06864678\n","Training at Epoch 286 iteration 23 with loss 0.06899607\n","Training at Epoch 286 iteration 24 with loss 0.06784989\n","Training at Epoch 286 iteration 25 with loss 0.06993038\n","Training at Epoch 286 iteration 26 with loss 0.0670147\n","Training at Epoch 286 iteration 27 with loss 0.069170535\n","Training at Epoch 286 iteration 28 with loss 0.06882633\n","Training at Epoch 286 iteration 29 with loss 0.06995149\n","Training at Epoch 286 iteration 30 with loss 0.06867491\n","Training at Epoch 287 iteration 0 with loss 0.067451015\n","Training at Epoch 287 iteration 1 with loss 0.06949942\n","Training at Epoch 287 iteration 2 with loss 0.06764312\n","Training at Epoch 287 iteration 3 with loss 0.06996601\n","Training at Epoch 287 iteration 4 with loss 0.06956998\n","Training at Epoch 287 iteration 5 with loss 0.06915531\n","Training at Epoch 287 iteration 6 with loss 0.07279735\n","Training at Epoch 287 iteration 7 with loss 0.06661089\n","Training at Epoch 287 iteration 8 with loss 0.071294144\n","Training at Epoch 287 iteration 9 with loss 0.07032006\n","Training at Epoch 287 iteration 10 with loss 0.06783604\n","Training at Epoch 287 iteration 11 with loss 0.069677785\n","Training at Epoch 287 iteration 12 with loss 0.06910389\n","Training at Epoch 287 iteration 13 with loss 0.0710225\n","Training at Epoch 287 iteration 14 with loss 0.068734005\n","Training at Epoch 287 iteration 15 with loss 0.06917244\n","Training at Epoch 287 iteration 16 with loss 0.06718583\n","Training at Epoch 287 iteration 17 with loss 0.06861824\n","Training at Epoch 287 iteration 18 with loss 0.06913638\n","Training at Epoch 287 iteration 19 with loss 0.06862638\n","Training at Epoch 287 iteration 20 with loss 0.06669341\n","Training at Epoch 287 iteration 21 with loss 0.06776425\n","Training at Epoch 287 iteration 22 with loss 0.06831473\n","Training at Epoch 287 iteration 23 with loss 0.069272466\n","Training at Epoch 287 iteration 24 with loss 0.06858804\n","Training at Epoch 287 iteration 25 with loss 0.06871252\n","Training at Epoch 287 iteration 26 with loss 0.06835599\n","Training at Epoch 287 iteration 27 with loss 0.069127694\n","Training at Epoch 287 iteration 28 with loss 0.07026939\n","Training at Epoch 287 iteration 29 with loss 0.07130025\n","Training at Epoch 287 iteration 30 with loss 0.06992008\n","Training at Epoch 288 iteration 0 with loss 0.069592096\n","Training at Epoch 288 iteration 1 with loss 0.0703277\n","Training at Epoch 288 iteration 2 with loss 0.06899492\n","Training at Epoch 288 iteration 3 with loss 0.069096275\n","Training at Epoch 288 iteration 4 with loss 0.070654586\n","Training at Epoch 288 iteration 5 with loss 0.06993686\n","Training at Epoch 288 iteration 6 with loss 0.0687086\n","Training at Epoch 288 iteration 7 with loss 0.069667295\n","Training at Epoch 288 iteration 8 with loss 0.06992163\n","Training at Epoch 288 iteration 9 with loss 0.067977436\n","Training at Epoch 288 iteration 10 with loss 0.07080177\n","Training at Epoch 288 iteration 11 with loss 0.067064255\n","Training at Epoch 288 iteration 12 with loss 0.06864223\n","Training at Epoch 288 iteration 13 with loss 0.06879379\n","Training at Epoch 288 iteration 14 with loss 0.06948994\n","Training at Epoch 288 iteration 15 with loss 0.06746155\n","Training at Epoch 288 iteration 16 with loss 0.06955831\n","Training at Epoch 288 iteration 17 with loss 0.066321515\n","Training at Epoch 288 iteration 18 with loss 0.070366666\n","Training at Epoch 288 iteration 19 with loss 0.07108864\n","Training at Epoch 288 iteration 20 with loss 0.06960519\n","Training at Epoch 288 iteration 21 with loss 0.06959621\n","Training at Epoch 288 iteration 22 with loss 0.07238679\n","Training at Epoch 288 iteration 23 with loss 0.06707145\n","Training at Epoch 288 iteration 24 with loss 0.06748275\n","Training at Epoch 288 iteration 25 with loss 0.07003112\n","Training at Epoch 288 iteration 26 with loss 0.067506\n","Training at Epoch 288 iteration 27 with loss 0.06794749\n","Training at Epoch 288 iteration 28 with loss 0.06918053\n","Training at Epoch 288 iteration 29 with loss 0.06870652\n","Training at Epoch 288 iteration 30 with loss 0.06803074\n","Training at Epoch 289 iteration 0 with loss 0.069547534\n","Training at Epoch 289 iteration 1 with loss 0.06943977\n","Training at Epoch 289 iteration 2 with loss 0.071513675\n","Training at Epoch 289 iteration 3 with loss 0.06995462\n","Training at Epoch 289 iteration 4 with loss 0.06956297\n","Training at Epoch 289 iteration 5 with loss 0.068726026\n","Training at Epoch 289 iteration 6 with loss 0.06988427\n","Training at Epoch 289 iteration 7 with loss 0.070670545\n","Training at Epoch 289 iteration 8 with loss 0.06962128\n","Training at Epoch 289 iteration 9 with loss 0.071914\n","Training at Epoch 289 iteration 10 with loss 0.06838794\n","Training at Epoch 289 iteration 11 with loss 0.06789696\n","Training at Epoch 289 iteration 12 with loss 0.06794752\n","Training at Epoch 289 iteration 13 with loss 0.0683506\n","Training at Epoch 289 iteration 14 with loss 0.06911032\n","Training at Epoch 289 iteration 15 with loss 0.07069367\n","Training at Epoch 289 iteration 16 with loss 0.069530785\n","Training at Epoch 289 iteration 17 with loss 0.07065592\n","Training at Epoch 289 iteration 18 with loss 0.068302155\n","Training at Epoch 289 iteration 19 with loss 0.068377785\n","Training at Epoch 289 iteration 20 with loss 0.06791879\n","Training at Epoch 289 iteration 21 with loss 0.06866537\n","Training at Epoch 289 iteration 22 with loss 0.06842397\n","Training at Epoch 289 iteration 23 with loss 0.06874538\n","Training at Epoch 289 iteration 24 with loss 0.06751944\n","Training at Epoch 289 iteration 25 with loss 0.069464326\n","Training at Epoch 289 iteration 26 with loss 0.06871916\n","Training at Epoch 289 iteration 27 with loss 0.06877826\n","Training at Epoch 289 iteration 28 with loss 0.06830167\n","Training at Epoch 289 iteration 29 with loss 0.06638439\n","Training at Epoch 289 iteration 30 with loss 0.06876843\n","Training at Epoch 290 iteration 0 with loss 0.06946484\n","Training at Epoch 290 iteration 1 with loss 0.06873445\n","Training at Epoch 290 iteration 2 with loss 0.071073465\n","Training at Epoch 290 iteration 3 with loss 0.071080446\n","Training at Epoch 290 iteration 4 with loss 0.06912476\n","Training at Epoch 290 iteration 5 with loss 0.069926485\n","Training at Epoch 290 iteration 6 with loss 0.070318654\n","Training at Epoch 290 iteration 7 with loss 0.06946411\n","Training at Epoch 290 iteration 8 with loss 0.06755887\n","Training at Epoch 290 iteration 9 with loss 0.06830974\n","Training at Epoch 290 iteration 10 with loss 0.07024522\n","Training at Epoch 290 iteration 11 with loss 0.067604326\n","Training at Epoch 290 iteration 12 with loss 0.068364106\n","Training at Epoch 290 iteration 13 with loss 0.06875678\n","Training at Epoch 290 iteration 14 with loss 0.06796311\n","Training at Epoch 290 iteration 15 with loss 0.069941916\n","Training at Epoch 290 iteration 16 with loss 0.06839831\n","Training at Epoch 290 iteration 17 with loss 0.06870959\n","Training at Epoch 290 iteration 18 with loss 0.06869102\n","Training at Epoch 290 iteration 19 with loss 0.06878488\n","Training at Epoch 290 iteration 20 with loss 0.06875144\n","Training at Epoch 290 iteration 21 with loss 0.069469534\n","Training at Epoch 290 iteration 22 with loss 0.06752984\n","Training at Epoch 290 iteration 23 with loss 0.06832228\n","Training at Epoch 290 iteration 24 with loss 0.07029645\n","Training at Epoch 290 iteration 25 with loss 0.07187165\n","Training at Epoch 290 iteration 26 with loss 0.06876917\n","Training at Epoch 290 iteration 27 with loss 0.066761404\n","Training at Epoch 290 iteration 28 with loss 0.06875795\n","Training at Epoch 290 iteration 29 with loss 0.07025072\n","Training at Epoch 290 iteration 30 with loss 0.06989731\n","Training at Epoch 291 iteration 0 with loss 0.068287544\n","Training at Epoch 291 iteration 1 with loss 0.07029933\n","Training at Epoch 291 iteration 2 with loss 0.067159\n","Training at Epoch 291 iteration 3 with loss 0.06910177\n","Training at Epoch 291 iteration 4 with loss 0.0695049\n","Training at Epoch 291 iteration 5 with loss 0.0687113\n","Training at Epoch 291 iteration 6 with loss 0.06797074\n","Training at Epoch 291 iteration 7 with loss 0.06832807\n","Training at Epoch 291 iteration 8 with loss 0.0690761\n","Training at Epoch 291 iteration 9 with loss 0.06987913\n","Training at Epoch 291 iteration 10 with loss 0.069882825\n","Training at Epoch 291 iteration 11 with loss 0.07068464\n","Training at Epoch 291 iteration 12 with loss 0.06870689\n","Training at Epoch 291 iteration 13 with loss 0.069929145\n","Training at Epoch 291 iteration 14 with loss 0.06990656\n","Training at Epoch 291 iteration 15 with loss 0.06989042\n","Training at Epoch 291 iteration 16 with loss 0.06831605\n","Training at Epoch 291 iteration 17 with loss 0.06915795\n","Training at Epoch 291 iteration 18 with loss 0.069503285\n","Training at Epoch 291 iteration 19 with loss 0.06949719\n","Training at Epoch 291 iteration 20 with loss 0.06867521\n","Training at Epoch 291 iteration 21 with loss 0.06747654\n","Training at Epoch 291 iteration 22 with loss 0.06949199\n","Training at Epoch 291 iteration 23 with loss 0.06911908\n","Training at Epoch 291 iteration 24 with loss 0.06909496\n","Training at Epoch 291 iteration 25 with loss 0.069872335\n","Training at Epoch 291 iteration 26 with loss 0.069965094\n","Training at Epoch 291 iteration 27 with loss 0.0690601\n","Training at Epoch 291 iteration 28 with loss 0.068685114\n","Training at Epoch 291 iteration 29 with loss 0.06872824\n","Training at Epoch 291 iteration 30 with loss 0.06830023\n","Training at Epoch 292 iteration 0 with loss 0.06795503\n","Training at Epoch 292 iteration 1 with loss 0.06953783\n","Training at Epoch 292 iteration 2 with loss 0.06992002\n","Training at Epoch 292 iteration 3 with loss 0.068698816\n","Training at Epoch 292 iteration 4 with loss 0.067513585\n","Training at Epoch 292 iteration 5 with loss 0.06828611\n","Training at Epoch 292 iteration 6 with loss 0.0690978\n","Training at Epoch 292 iteration 7 with loss 0.07032461\n","Training at Epoch 292 iteration 8 with loss 0.06832856\n","Training at Epoch 292 iteration 9 with loss 0.069935426\n","Training at Epoch 292 iteration 10 with loss 0.069999196\n","Training at Epoch 292 iteration 11 with loss 0.07202299\n","Training at Epoch 292 iteration 12 with loss 0.06798591\n","Training at Epoch 292 iteration 13 with loss 0.06793972\n","Training at Epoch 292 iteration 14 with loss 0.069573015\n","Training at Epoch 292 iteration 15 with loss 0.067069426\n","Training at Epoch 292 iteration 16 with loss 0.06910607\n","Training at Epoch 292 iteration 17 with loss 0.0670853\n","Training at Epoch 292 iteration 18 with loss 0.06835551\n","Training at Epoch 292 iteration 19 with loss 0.06947608\n","Training at Epoch 292 iteration 20 with loss 0.07033442\n","Training at Epoch 292 iteration 21 with loss 0.06829631\n","Training at Epoch 292 iteration 22 with loss 0.068709835\n","Training at Epoch 292 iteration 23 with loss 0.069457226\n","Training at Epoch 292 iteration 24 with loss 0.06993995\n","Training at Epoch 292 iteration 25 with loss 0.06955756\n","Training at Epoch 292 iteration 26 with loss 0.06914697\n","Training at Epoch 292 iteration 27 with loss 0.06825954\n","Training at Epoch 292 iteration 28 with loss 0.06872153\n","Training at Epoch 292 iteration 29 with loss 0.071975246\n","Training at Epoch 292 iteration 30 with loss 0.069966346\n","Training at Epoch 293 iteration 0 with loss 0.06746204\n","Training at Epoch 293 iteration 1 with loss 0.0707414\n","Training at Epoch 293 iteration 2 with loss 0.06867115\n","Training at Epoch 293 iteration 3 with loss 0.06955087\n","Training at Epoch 293 iteration 4 with loss 0.067490324\n","Training at Epoch 293 iteration 5 with loss 0.06744324\n","Training at Epoch 293 iteration 6 with loss 0.06790561\n","Training at Epoch 293 iteration 7 with loss 0.069078304\n","Training at Epoch 293 iteration 8 with loss 0.07028963\n","Training at Epoch 293 iteration 9 with loss 0.06712649\n","Training at Epoch 293 iteration 10 with loss 0.069179825\n","Training at Epoch 293 iteration 11 with loss 0.070293956\n","Training at Epoch 293 iteration 12 with loss 0.0707026\n","Training at Epoch 293 iteration 13 with loss 0.06906982\n","Training at Epoch 293 iteration 14 with loss 0.069502056\n","Training at Epoch 293 iteration 15 with loss 0.06868575\n","Training at Epoch 293 iteration 16 with loss 0.06824239\n","Training at Epoch 293 iteration 17 with loss 0.06791524\n","Training at Epoch 293 iteration 18 with loss 0.06865741\n","Training at Epoch 293 iteration 19 with loss 0.069228426\n","Training at Epoch 293 iteration 20 with loss 0.068982385\n","Training at Epoch 293 iteration 21 with loss 0.068709396\n","Training at Epoch 293 iteration 22 with loss 0.070787236\n","Training at Epoch 293 iteration 23 with loss 0.06946217\n","Training at Epoch 293 iteration 24 with loss 0.06872227\n","Training at Epoch 293 iteration 25 with loss 0.07172023\n","Training at Epoch 293 iteration 26 with loss 0.06956588\n","Training at Epoch 293 iteration 27 with loss 0.068636365\n","Training at Epoch 293 iteration 28 with loss 0.068634875\n","Training at Epoch 293 iteration 29 with loss 0.07042173\n","Training at Epoch 293 iteration 30 with loss 0.06919089\n","Training at Epoch 294 iteration 0 with loss 0.06914629\n","Training at Epoch 294 iteration 1 with loss 0.07152937\n","Training at Epoch 294 iteration 2 with loss 0.06947664\n","Training at Epoch 294 iteration 3 with loss 0.06913603\n","Training at Epoch 294 iteration 4 with loss 0.068750575\n","Training at Epoch 294 iteration 5 with loss 0.06918173\n","Training at Epoch 294 iteration 6 with loss 0.070001\n","Training at Epoch 294 iteration 7 with loss 0.06778463\n","Training at Epoch 294 iteration 8 with loss 0.06874545\n","Training at Epoch 294 iteration 9 with loss 0.06869568\n","Training at Epoch 294 iteration 10 with loss 0.06990542\n","Training at Epoch 294 iteration 11 with loss 0.06898049\n","Training at Epoch 294 iteration 12 with loss 0.068332806\n","Training at Epoch 294 iteration 13 with loss 0.06699394\n","Training at Epoch 294 iteration 14 with loss 0.06824509\n","Training at Epoch 294 iteration 15 with loss 0.06699048\n","Training at Epoch 294 iteration 16 with loss 0.06829239\n","Training at Epoch 294 iteration 17 with loss 0.07037078\n","Training at Epoch 294 iteration 18 with loss 0.06699979\n","Training at Epoch 294 iteration 19 with loss 0.0686048\n","Training at Epoch 294 iteration 20 with loss 0.06987564\n","Training at Epoch 294 iteration 21 with loss 0.06826289\n","Training at Epoch 294 iteration 22 with loss 0.070821725\n","Training at Epoch 294 iteration 23 with loss 0.06998684\n","Training at Epoch 294 iteration 24 with loss 0.06869816\n","Training at Epoch 294 iteration 25 with loss 0.07057188\n","Training at Epoch 294 iteration 26 with loss 0.07029568\n","Training at Epoch 294 iteration 27 with loss 0.07091813\n","Training at Epoch 294 iteration 28 with loss 0.06901945\n","Training at Epoch 294 iteration 29 with loss 0.06868795\n","Training at Epoch 294 iteration 30 with loss 0.06784373\n","Training at Epoch 295 iteration 0 with loss 0.06694488\n","Training at Epoch 295 iteration 1 with loss 0.06910671\n","Training at Epoch 295 iteration 2 with loss 0.06883967\n","Training at Epoch 295 iteration 3 with loss 0.071718946\n","Training at Epoch 295 iteration 4 with loss 0.07073023\n","Training at Epoch 295 iteration 5 with loss 0.06873527\n","Training at Epoch 295 iteration 6 with loss 0.06820558\n","Training at Epoch 295 iteration 7 with loss 0.06907739\n","Training at Epoch 295 iteration 8 with loss 0.06778869\n","Training at Epoch 295 iteration 9 with loss 0.0677642\n","Training at Epoch 295 iteration 10 with loss 0.07064353\n","Training at Epoch 295 iteration 11 with loss 0.06869437\n","Training at Epoch 295 iteration 12 with loss 0.06686222\n","Training at Epoch 295 iteration 13 with loss 0.06949268\n","Training at Epoch 295 iteration 14 with loss 0.06942092\n","Training at Epoch 295 iteration 15 with loss 0.070817046\n","Training at Epoch 295 iteration 16 with loss 0.06864434\n","Training at Epoch 295 iteration 17 with loss 0.06864534\n","Training at Epoch 295 iteration 18 with loss 0.070758685\n","Training at Epoch 295 iteration 19 with loss 0.06861864\n","Training at Epoch 295 iteration 20 with loss 0.06973769\n","Training at Epoch 295 iteration 21 with loss 0.06992242\n","Training at Epoch 295 iteration 22 with loss 0.06843422\n","Training at Epoch 295 iteration 23 with loss 0.06919076\n","Training at Epoch 295 iteration 24 with loss 0.06953468\n","Training at Epoch 295 iteration 25 with loss 0.068903536\n","Training at Epoch 295 iteration 26 with loss 0.06888752\n","Training at Epoch 295 iteration 27 with loss 0.06957177\n","Training at Epoch 295 iteration 28 with loss 0.06884674\n","Training at Epoch 295 iteration 29 with loss 0.067776434\n","Training at Epoch 295 iteration 30 with loss 0.06832898\n","Training at Epoch 296 iteration 0 with loss 0.06807736\n","Training at Epoch 296 iteration 1 with loss 0.06786449\n","Training at Epoch 296 iteration 2 with loss 0.06837006\n","Training at Epoch 296 iteration 3 with loss 0.06805432\n","Training at Epoch 296 iteration 4 with loss 0.07037387\n","Training at Epoch 296 iteration 5 with loss 0.06718753\n","Training at Epoch 296 iteration 6 with loss 0.06856179\n","Training at Epoch 296 iteration 7 with loss 0.06869002\n","Training at Epoch 296 iteration 8 with loss 0.0682715\n","Training at Epoch 296 iteration 9 with loss 0.06854266\n","Training at Epoch 296 iteration 10 with loss 0.07177125\n","Training at Epoch 296 iteration 11 with loss 0.06910227\n","Training at Epoch 296 iteration 12 with loss 0.07093771\n","Training at Epoch 296 iteration 13 with loss 0.06898867\n","Training at Epoch 296 iteration 14 with loss 0.069832265\n","Training at Epoch 296 iteration 15 with loss 0.067300595\n","Training at Epoch 296 iteration 16 with loss 0.071136795\n","Training at Epoch 296 iteration 17 with loss 0.06917516\n","Training at Epoch 296 iteration 18 with loss 0.06805032\n","Training at Epoch 296 iteration 19 with loss 0.07044853\n","Training at Epoch 296 iteration 20 with loss 0.070627585\n","Training at Epoch 296 iteration 21 with loss 0.067705974\n","Training at Epoch 296 iteration 22 with loss 0.06776719\n","Training at Epoch 296 iteration 23 with loss 0.06970298\n","Training at Epoch 296 iteration 24 with loss 0.06750093\n","Training at Epoch 296 iteration 25 with loss 0.0723989\n","Training at Epoch 296 iteration 26 with loss 0.06965837\n","Training at Epoch 296 iteration 27 with loss 0.06954659\n","Training at Epoch 296 iteration 28 with loss 0.07110339\n","Training at Epoch 296 iteration 29 with loss 0.06745319\n","Training at Epoch 296 iteration 30 with loss 0.067990735\n","Training at Epoch 297 iteration 0 with loss 0.070324466\n","Training at Epoch 297 iteration 1 with loss 0.06989742\n","Training at Epoch 297 iteration 2 with loss 0.0700404\n","Training at Epoch 297 iteration 3 with loss 0.06899992\n","Training at Epoch 297 iteration 4 with loss 0.06721363\n","Training at Epoch 297 iteration 5 with loss 0.06901049\n","Training at Epoch 297 iteration 6 with loss 0.068000056\n","Training at Epoch 297 iteration 7 with loss 0.06937354\n","Training at Epoch 297 iteration 8 with loss 0.072405994\n","Training at Epoch 297 iteration 9 with loss 0.07164305\n","Training at Epoch 297 iteration 10 with loss 0.068605185\n","Training at Epoch 297 iteration 11 with loss 0.06854208\n","Training at Epoch 297 iteration 12 with loss 0.06750065\n","Training at Epoch 297 iteration 13 with loss 0.067779236\n","Training at Epoch 297 iteration 14 with loss 0.070233285\n","Training at Epoch 297 iteration 15 with loss 0.06873855\n","Training at Epoch 297 iteration 16 with loss 0.07075985\n","Training at Epoch 297 iteration 17 with loss 0.07048302\n","Training at Epoch 297 iteration 18 with loss 0.06814909\n","Training at Epoch 297 iteration 19 with loss 0.07100503\n","Training at Epoch 297 iteration 20 with loss 0.068646885\n","Training at Epoch 297 iteration 21 with loss 0.06863974\n","Training at Epoch 297 iteration 22 with loss 0.069732614\n","Training at Epoch 297 iteration 23 with loss 0.06934456\n","Training at Epoch 297 iteration 24 with loss 0.07076938\n","Training at Epoch 297 iteration 25 with loss 0.06755592\n","Training at Epoch 297 iteration 26 with loss 0.06790557\n","Training at Epoch 297 iteration 27 with loss 0.068890706\n","Training at Epoch 297 iteration 28 with loss 0.06877239\n","Training at Epoch 297 iteration 29 with loss 0.06655957\n","Training at Epoch 297 iteration 30 with loss 0.06804219\n","Training at Epoch 298 iteration 0 with loss 0.07009022\n","Training at Epoch 298 iteration 1 with loss 0.06925097\n","Training at Epoch 298 iteration 2 with loss 0.06823525\n","Training at Epoch 298 iteration 3 with loss 0.069531485\n","Training at Epoch 298 iteration 4 with loss 0.07005736\n","Training at Epoch 298 iteration 5 with loss 0.06776021\n","Training at Epoch 298 iteration 6 with loss 0.06911406\n","Training at Epoch 298 iteration 7 with loss 0.06821938\n","Training at Epoch 298 iteration 8 with loss 0.07078933\n","Training at Epoch 298 iteration 9 with loss 0.07087039\n","Training at Epoch 298 iteration 10 with loss 0.068476364\n","Training at Epoch 298 iteration 11 with loss 0.06695076\n","Training at Epoch 298 iteration 12 with loss 0.06792995\n","Training at Epoch 298 iteration 13 with loss 0.07036118\n","Training at Epoch 298 iteration 14 with loss 0.069430634\n","Training at Epoch 298 iteration 15 with loss 0.06829627\n","Training at Epoch 298 iteration 16 with loss 0.069005206\n","Training at Epoch 298 iteration 17 with loss 0.06881744\n","Training at Epoch 298 iteration 18 with loss 0.068644054\n","Training at Epoch 298 iteration 19 with loss 0.0698071\n","Training at Epoch 298 iteration 20 with loss 0.069498435\n","Training at Epoch 298 iteration 21 with loss 0.069449976\n","Training at Epoch 298 iteration 22 with loss 0.06963276\n","Training at Epoch 298 iteration 23 with loss 0.06953901\n","Training at Epoch 298 iteration 24 with loss 0.06704395\n","Training at Epoch 298 iteration 25 with loss 0.06909855\n","Training at Epoch 298 iteration 26 with loss 0.06986731\n","Training at Epoch 298 iteration 27 with loss 0.06870411\n","Training at Epoch 298 iteration 28 with loss 0.06997319\n","Training at Epoch 298 iteration 29 with loss 0.068698086\n","Training at Epoch 298 iteration 30 with loss 0.068355784\n","Training at Epoch 299 iteration 0 with loss 0.0707175\n","Training at Epoch 299 iteration 1 with loss 0.066642664\n","Training at Epoch 299 iteration 2 with loss 0.06951171\n","Training at Epoch 299 iteration 3 with loss 0.06882789\n","Training at Epoch 299 iteration 4 with loss 0.06944736\n","Training at Epoch 299 iteration 5 with loss 0.07063459\n","Training at Epoch 299 iteration 6 with loss 0.06926598\n","Training at Epoch 299 iteration 7 with loss 0.06926672\n","Training at Epoch 299 iteration 8 with loss 0.067491435\n","Training at Epoch 299 iteration 9 with loss 0.06882369\n","Training at Epoch 299 iteration 10 with loss 0.06634434\n","Training at Epoch 299 iteration 11 with loss 0.06968705\n","Training at Epoch 299 iteration 12 with loss 0.06794257\n","Training at Epoch 299 iteration 13 with loss 0.068317324\n","Training at Epoch 299 iteration 14 with loss 0.07073403\n","Training at Epoch 299 iteration 15 with loss 0.068010524\n","Training at Epoch 299 iteration 16 with loss 0.06907602\n","Training at Epoch 299 iteration 17 with loss 0.06988484\n","Training at Epoch 299 iteration 18 with loss 0.06903876\n","Training at Epoch 299 iteration 19 with loss 0.06908886\n","Training at Epoch 299 iteration 20 with loss 0.07243722\n","Training at Epoch 299 iteration 21 with loss 0.07036418\n","Training at Epoch 299 iteration 22 with loss 0.06910918\n","Training at Epoch 299 iteration 23 with loss 0.06916368\n","Training at Epoch 299 iteration 24 with loss 0.06909604\n","Training at Epoch 299 iteration 25 with loss 0.068248406\n","Training at Epoch 299 iteration 26 with loss 0.06923605\n","Training at Epoch 299 iteration 27 with loss 0.069072746\n","Training at Epoch 299 iteration 28 with loss 0.06799824\n","Training at Epoch 299 iteration 29 with loss 0.068425804\n","Training at Epoch 299 iteration 30 with loss 0.069936104\n","Training at Epoch 300 iteration 0 with loss 0.0686884\n","Training at Epoch 300 iteration 1 with loss 0.07072734\n","Training at Epoch 300 iteration 2 with loss 0.069558226\n","Training at Epoch 300 iteration 3 with loss 0.0686684\n","Training at Epoch 300 iteration 4 with loss 0.06821382\n","Training at Epoch 300 iteration 5 with loss 0.06866451\n","Training at Epoch 300 iteration 6 with loss 0.0699934\n","Training at Epoch 300 iteration 7 with loss 0.06961274\n","Training at Epoch 300 iteration 8 with loss 0.07150097\n","Training at Epoch 300 iteration 9 with loss 0.06874322\n","Training at Epoch 300 iteration 10 with loss 0.070320606\n","Training at Epoch 300 iteration 11 with loss 0.068319164\n","Training at Epoch 300 iteration 12 with loss 0.06867845\n","Training at Epoch 300 iteration 13 with loss 0.0687261\n","Training at Epoch 300 iteration 14 with loss 0.06711131\n","Training at Epoch 300 iteration 15 with loss 0.06910327\n","Training at Epoch 300 iteration 16 with loss 0.06754239\n","Training at Epoch 300 iteration 17 with loss 0.06992329\n","Training at Epoch 300 iteration 18 with loss 0.069529586\n","Training at Epoch 300 iteration 19 with loss 0.069489054\n","Training at Epoch 300 iteration 20 with loss 0.068756476\n","Training at Epoch 300 iteration 21 with loss 0.07071315\n","Training at Epoch 300 iteration 22 with loss 0.06753473\n","Training at Epoch 300 iteration 23 with loss 0.06793298\n","Training at Epoch 300 iteration 24 with loss 0.06868674\n","Training at Epoch 300 iteration 25 with loss 0.0687408\n","Training at Epoch 300 iteration 26 with loss 0.06873335\n","Training at Epoch 300 iteration 27 with loss 0.06874989\n","Training at Epoch 300 iteration 28 with loss 0.069138385\n","Training at Epoch 300 iteration 29 with loss 0.07029647\n","Training at Epoch 300 iteration 30 with loss 0.06958653\n","Training at Epoch 301 iteration 0 with loss 0.06791242\n","Training at Epoch 301 iteration 1 with loss 0.068322174\n","Training at Epoch 301 iteration 2 with loss 0.06953303\n","Training at Epoch 301 iteration 3 with loss 0.07072251\n","Training at Epoch 301 iteration 4 with loss 0.0678959\n","Training at Epoch 301 iteration 5 with loss 0.07114907\n","Training at Epoch 301 iteration 6 with loss 0.068669505\n","Training at Epoch 301 iteration 7 with loss 0.06953999\n","Training at Epoch 301 iteration 8 with loss 0.067904234\n","Training at Epoch 301 iteration 9 with loss 0.06749533\n","Training at Epoch 301 iteration 10 with loss 0.06869643\n","Training at Epoch 301 iteration 11 with loss 0.06993373\n","Training at Epoch 301 iteration 12 with loss 0.067509264\n","Training at Epoch 301 iteration 13 with loss 0.06953191\n","Training at Epoch 301 iteration 14 with loss 0.0691264\n","Training at Epoch 301 iteration 15 with loss 0.070344366\n","Training at Epoch 301 iteration 16 with loss 0.068684116\n","Training at Epoch 301 iteration 17 with loss 0.06867982\n","Training at Epoch 301 iteration 18 with loss 0.070375726\n","Training at Epoch 301 iteration 19 with loss 0.06795613\n","Training at Epoch 301 iteration 20 with loss 0.06872006\n","Training at Epoch 301 iteration 21 with loss 0.06830611\n","Training at Epoch 301 iteration 22 with loss 0.067907825\n","Training at Epoch 301 iteration 23 with loss 0.06912455\n","Training at Epoch 301 iteration 24 with loss 0.069082946\n","Training at Epoch 301 iteration 25 with loss 0.07200434\n","Training at Epoch 301 iteration 26 with loss 0.06993907\n","Training at Epoch 301 iteration 27 with loss 0.068312064\n","Training at Epoch 301 iteration 28 with loss 0.07154276\n","Training at Epoch 301 iteration 29 with loss 0.067892075\n","Training at Epoch 301 iteration 30 with loss 0.06993996\n","Training at Epoch 302 iteration 0 with loss 0.0691002\n","Training at Epoch 302 iteration 1 with loss 0.06868521\n","Training at Epoch 302 iteration 2 with loss 0.069102295\n","Training at Epoch 302 iteration 3 with loss 0.06789793\n","Training at Epoch 302 iteration 4 with loss 0.070323616\n","Training at Epoch 302 iteration 5 with loss 0.06750201\n","Training at Epoch 302 iteration 6 with loss 0.070740044\n","Training at Epoch 302 iteration 7 with loss 0.06872125\n","Training at Epoch 302 iteration 8 with loss 0.070737764\n","Training at Epoch 302 iteration 9 with loss 0.06996372\n","Training at Epoch 302 iteration 10 with loss 0.06951594\n","Training at Epoch 302 iteration 11 with loss 0.06751047\n","Training at Epoch 302 iteration 12 with loss 0.07034092\n","Training at Epoch 302 iteration 13 with loss 0.06951856\n","Training at Epoch 302 iteration 14 with loss 0.06953888\n","Training at Epoch 302 iteration 15 with loss 0.06953718\n","Training at Epoch 302 iteration 16 with loss 0.06909843\n","Training at Epoch 302 iteration 17 with loss 0.067109056\n","Training at Epoch 302 iteration 18 with loss 0.07033439\n","Training at Epoch 302 iteration 19 with loss 0.069514886\n","Training at Epoch 302 iteration 20 with loss 0.06669555\n","Training at Epoch 302 iteration 21 with loss 0.06872477\n","Training at Epoch 302 iteration 22 with loss 0.070329234\n","Training at Epoch 302 iteration 23 with loss 0.067918144\n","Training at Epoch 302 iteration 24 with loss 0.07032502\n","Training at Epoch 302 iteration 25 with loss 0.06951971\n","Training at Epoch 302 iteration 26 with loss 0.06912052\n","Training at Epoch 302 iteration 27 with loss 0.069110766\n","Training at Epoch 302 iteration 28 with loss 0.06751036\n","Training at Epoch 302 iteration 29 with loss 0.06911682\n","Training at Epoch 302 iteration 30 with loss 0.06829299\n","Training at Epoch 303 iteration 0 with loss 0.069927104\n","Training at Epoch 303 iteration 1 with loss 0.0691186\n","Training at Epoch 303 iteration 2 with loss 0.068709925\n","Training at Epoch 303 iteration 3 with loss 0.07193645\n","Training at Epoch 303 iteration 4 with loss 0.06994218\n","Training at Epoch 303 iteration 5 with loss 0.06791718\n","Training at Epoch 303 iteration 6 with loss 0.06705944\n","Training at Epoch 303 iteration 7 with loss 0.06709895\n","Training at Epoch 303 iteration 8 with loss 0.069153324\n","Training at Epoch 303 iteration 9 with loss 0.0695171\n","Training at Epoch 303 iteration 10 with loss 0.06952421\n","Training at Epoch 303 iteration 11 with loss 0.06831709\n","Training at Epoch 303 iteration 12 with loss 0.06831128\n","Training at Epoch 303 iteration 13 with loss 0.06993196\n","Training at Epoch 303 iteration 14 with loss 0.06749757\n","Training at Epoch 303 iteration 15 with loss 0.06951442\n","Training at Epoch 303 iteration 16 with loss 0.069519326\n","Training at Epoch 303 iteration 17 with loss 0.068315245\n","Training at Epoch 303 iteration 18 with loss 0.07032912\n","Training at Epoch 303 iteration 19 with loss 0.06912534\n","Training at Epoch 303 iteration 20 with loss 0.06953452\n","Training at Epoch 303 iteration 21 with loss 0.06952441\n","Training at Epoch 303 iteration 22 with loss 0.068687335\n","Training at Epoch 303 iteration 23 with loss 0.068315305\n","Training at Epoch 303 iteration 24 with loss 0.06830449\n","Training at Epoch 303 iteration 25 with loss 0.06790806\n","Training at Epoch 303 iteration 26 with loss 0.071168415\n","Training at Epoch 303 iteration 27 with loss 0.067871474\n","Training at Epoch 303 iteration 28 with loss 0.0674917\n","Training at Epoch 303 iteration 29 with loss 0.071931355\n","Training at Epoch 303 iteration 30 with loss 0.06993864\n","Training at Epoch 304 iteration 0 with loss 0.06749967\n","Training at Epoch 304 iteration 1 with loss 0.069118954\n","Training at Epoch 304 iteration 2 with loss 0.06993963\n","Training at Epoch 304 iteration 3 with loss 0.06828971\n","Training at Epoch 304 iteration 4 with loss 0.06991197\n","Training at Epoch 304 iteration 5 with loss 0.06828827\n","Training at Epoch 304 iteration 6 with loss 0.06829312\n","Training at Epoch 304 iteration 7 with loss 0.06952552\n","Training at Epoch 304 iteration 8 with loss 0.06705584\n","Training at Epoch 304 iteration 9 with loss 0.068717815\n","Training at Epoch 304 iteration 10 with loss 0.06705094\n","Training at Epoch 304 iteration 11 with loss 0.07076256\n","Training at Epoch 304 iteration 12 with loss 0.06911823\n","Training at Epoch 304 iteration 13 with loss 0.06831169\n","Training at Epoch 304 iteration 14 with loss 0.06995319\n","Training at Epoch 304 iteration 15 with loss 0.067048594\n","Training at Epoch 304 iteration 16 with loss 0.072839476\n","Training at Epoch 304 iteration 17 with loss 0.06829197\n","Training at Epoch 304 iteration 18 with loss 0.07286124\n","Training at Epoch 304 iteration 19 with loss 0.07080058\n","Training at Epoch 304 iteration 20 with loss 0.06870306\n","Training at Epoch 304 iteration 21 with loss 0.06623439\n","Training at Epoch 304 iteration 22 with loss 0.06912913\n","Training at Epoch 304 iteration 23 with loss 0.06907357\n","Training at Epoch 304 iteration 24 with loss 0.069534816\n","Training at Epoch 304 iteration 25 with loss 0.06869973\n","Training at Epoch 304 iteration 26 with loss 0.07202369\n","Training at Epoch 304 iteration 27 with loss 0.06912516\n","Training at Epoch 304 iteration 28 with loss 0.06829208\n","Training at Epoch 304 iteration 29 with loss 0.069107905\n","Training at Epoch 304 iteration 30 with loss 0.06828857\n","Training at Epoch 305 iteration 0 with loss 0.06787695\n","Training at Epoch 305 iteration 1 with loss 0.06704457\n","Training at Epoch 305 iteration 2 with loss 0.06867291\n","Training at Epoch 305 iteration 3 with loss 0.0695473\n","Training at Epoch 305 iteration 4 with loss 0.06912044\n","Training at Epoch 305 iteration 5 with loss 0.06995546\n","Training at Epoch 305 iteration 6 with loss 0.06829642\n","Training at Epoch 305 iteration 7 with loss 0.06914841\n","Training at Epoch 305 iteration 8 with loss 0.069131315\n","Training at Epoch 305 iteration 9 with loss 0.06994672\n","Training at Epoch 305 iteration 10 with loss 0.06911808\n","Training at Epoch 305 iteration 11 with loss 0.06912201\n","Training at Epoch 305 iteration 12 with loss 0.068700284\n","Training at Epoch 305 iteration 13 with loss 0.07079148\n","Training at Epoch 305 iteration 14 with loss 0.06830478\n","Training at Epoch 305 iteration 15 with loss 0.07037244\n","Training at Epoch 305 iteration 16 with loss 0.0703475\n","Training at Epoch 305 iteration 17 with loss 0.06829279\n","Training at Epoch 305 iteration 18 with loss 0.07078573\n","Training at Epoch 305 iteration 19 with loss 0.070762835\n","Training at Epoch 305 iteration 20 with loss 0.06912671\n","Training at Epoch 305 iteration 21 with loss 0.06746244\n","Training at Epoch 305 iteration 22 with loss 0.06994841\n","Training at Epoch 305 iteration 23 with loss 0.0699466\n","Training at Epoch 305 iteration 24 with loss 0.06788209\n","Training at Epoch 305 iteration 25 with loss 0.066671535\n","Training at Epoch 305 iteration 26 with loss 0.06953033\n","Training at Epoch 305 iteration 27 with loss 0.068292916\n","Training at Epoch 305 iteration 28 with loss 0.06869625\n","Training at Epoch 305 iteration 29 with loss 0.06911608\n","Training at Epoch 305 iteration 30 with loss 0.07039457\n","Training at Epoch 306 iteration 0 with loss 0.06869878\n","Training at Epoch 306 iteration 1 with loss 0.06703417\n","Training at Epoch 306 iteration 2 with loss 0.06744995\n","Training at Epoch 306 iteration 3 with loss 0.06873797\n","Training at Epoch 306 iteration 4 with loss 0.06870551\n","Training at Epoch 306 iteration 5 with loss 0.06910089\n","Training at Epoch 306 iteration 6 with loss 0.071625054\n","Training at Epoch 306 iteration 7 with loss 0.0691331\n","Training at Epoch 306 iteration 8 with loss 0.06911777\n","Training at Epoch 306 iteration 9 with loss 0.06870194\n","Training at Epoch 306 iteration 10 with loss 0.068706624\n","Training at Epoch 306 iteration 11 with loss 0.069121316\n","Training at Epoch 306 iteration 12 with loss 0.07079038\n","Training at Epoch 306 iteration 13 with loss 0.06912312\n","Training at Epoch 306 iteration 14 with loss 0.067830674\n","Training at Epoch 306 iteration 15 with loss 0.069115326\n","Training at Epoch 306 iteration 16 with loss 0.069959715\n","Training at Epoch 306 iteration 17 with loss 0.06868565\n","Training at Epoch 306 iteration 18 with loss 0.07037498\n","Training at Epoch 306 iteration 19 with loss 0.067864254\n","Training at Epoch 306 iteration 20 with loss 0.06913063\n","Training at Epoch 306 iteration 21 with loss 0.06869855\n","Training at Epoch 306 iteration 22 with loss 0.06954254\n","Training at Epoch 306 iteration 23 with loss 0.06785412\n","Training at Epoch 306 iteration 24 with loss 0.06995558\n","Training at Epoch 306 iteration 25 with loss 0.07037318\n","Training at Epoch 306 iteration 26 with loss 0.06999041\n","Training at Epoch 306 iteration 27 with loss 0.06827565\n","Training at Epoch 306 iteration 28 with loss 0.0704027\n","Training at Epoch 306 iteration 29 with loss 0.06786535\n","Training at Epoch 306 iteration 30 with loss 0.06994537\n","Training at Epoch 307 iteration 0 with loss 0.07039009\n","Training at Epoch 307 iteration 1 with loss 0.068699464\n","Training at Epoch 307 iteration 2 with loss 0.07038883\n","Training at Epoch 307 iteration 3 with loss 0.066590406\n","Training at Epoch 307 iteration 4 with loss 0.069108374\n","Training at Epoch 307 iteration 5 with loss 0.06827954\n","Training at Epoch 307 iteration 6 with loss 0.06870322\n","Training at Epoch 307 iteration 7 with loss 0.06788485\n","Training at Epoch 307 iteration 8 with loss 0.06911754\n","Training at Epoch 307 iteration 9 with loss 0.07206601\n","Training at Epoch 307 iteration 10 with loss 0.068273224\n","Training at Epoch 307 iteration 11 with loss 0.06956691\n","Training at Epoch 307 iteration 12 with loss 0.07041846\n","Training at Epoch 307 iteration 13 with loss 0.06785589\n","Training at Epoch 307 iteration 14 with loss 0.06742181\n","Training at Epoch 307 iteration 15 with loss 0.07038267\n","Training at Epoch 307 iteration 16 with loss 0.06828441\n","Training at Epoch 307 iteration 17 with loss 0.06912268\n","Training at Epoch 307 iteration 18 with loss 0.06912743\n","Training at Epoch 307 iteration 19 with loss 0.07037727\n","Training at Epoch 307 iteration 20 with loss 0.066625096\n","Training at Epoch 307 iteration 21 with loss 0.07078384\n","Training at Epoch 307 iteration 22 with loss 0.0678665\n","Training at Epoch 307 iteration 23 with loss 0.06912623\n","Training at Epoch 307 iteration 24 with loss 0.06869537\n","Training at Epoch 307 iteration 25 with loss 0.072470486\n","Training at Epoch 307 iteration 26 with loss 0.06953263\n","Training at Epoch 307 iteration 27 with loss 0.06912607\n","Training at Epoch 307 iteration 28 with loss 0.067858055\n","Training at Epoch 307 iteration 29 with loss 0.06786447\n","Training at Epoch 307 iteration 30 with loss 0.069534734\n","Training at Epoch 308 iteration 0 with loss 0.06786914\n","Training at Epoch 308 iteration 1 with loss 0.07037933\n","Training at Epoch 308 iteration 2 with loss 0.06954599\n","Training at Epoch 308 iteration 3 with loss 0.06872089\n","Training at Epoch 308 iteration 4 with loss 0.067864195\n","Training at Epoch 308 iteration 5 with loss 0.06915729\n","Training at Epoch 308 iteration 6 with loss 0.06870391\n","Training at Epoch 308 iteration 7 with loss 0.069957\n","Training at Epoch 308 iteration 8 with loss 0.06827968\n","Training at Epoch 308 iteration 9 with loss 0.069977105\n","Training at Epoch 308 iteration 10 with loss 0.06869563\n","Training at Epoch 308 iteration 11 with loss 0.06995792\n","Training at Epoch 308 iteration 12 with loss 0.06912217\n","Training at Epoch 308 iteration 13 with loss 0.06912152\n","Training at Epoch 308 iteration 14 with loss 0.068708815\n","Training at Epoch 308 iteration 15 with loss 0.06869681\n","Training at Epoch 308 iteration 16 with loss 0.06953883\n","Training at Epoch 308 iteration 17 with loss 0.06786241\n","Training at Epoch 308 iteration 18 with loss 0.06785362\n","Training at Epoch 308 iteration 19 with loss 0.06995646\n","Training at Epoch 308 iteration 20 with loss 0.069569275\n","Training at Epoch 308 iteration 21 with loss 0.070380524\n","Training at Epoch 308 iteration 22 with loss 0.069129154\n","Training at Epoch 308 iteration 23 with loss 0.07124125\n","Training at Epoch 308 iteration 24 with loss 0.068694256\n","Training at Epoch 308 iteration 25 with loss 0.069131054\n","Training at Epoch 308 iteration 26 with loss 0.06658194\n","Training at Epoch 308 iteration 27 with loss 0.06827064\n","Training at Epoch 308 iteration 28 with loss 0.07081498\n","Training at Epoch 308 iteration 29 with loss 0.06911986\n","Training at Epoch 308 iteration 30 with loss 0.0691197\n","Training at Epoch 309 iteration 0 with loss 0.06870622\n","Training at Epoch 309 iteration 1 with loss 0.06869947\n","Training at Epoch 309 iteration 2 with loss 0.06952057\n","Training at Epoch 309 iteration 3 with loss 0.067436256\n","Training at Epoch 309 iteration 4 with loss 0.06869781\n","Training at Epoch 309 iteration 5 with loss 0.06658085\n","Training at Epoch 309 iteration 6 with loss 0.06913604\n","Training at Epoch 309 iteration 7 with loss 0.06700793\n","Training at Epoch 309 iteration 8 with loss 0.06785427\n","Training at Epoch 309 iteration 9 with loss 0.06911375\n","Training at Epoch 309 iteration 10 with loss 0.06997724\n","Training at Epoch 309 iteration 11 with loss 0.06997578\n","Training at Epoch 309 iteration 12 with loss 0.06998203\n","Training at Epoch 309 iteration 13 with loss 0.07127209\n","Training at Epoch 309 iteration 14 with loss 0.071271345\n","Training at Epoch 309 iteration 15 with loss 0.068254545\n","Training at Epoch 309 iteration 16 with loss 0.067407995\n","Training at Epoch 309 iteration 17 with loss 0.06568726\n","Training at Epoch 309 iteration 18 with loss 0.06998353\n","Training at Epoch 309 iteration 19 with loss 0.06869153\n","Training at Epoch 309 iteration 20 with loss 0.06999054\n","Training at Epoch 309 iteration 21 with loss 0.06824686\n","Training at Epoch 309 iteration 22 with loss 0.07045793\n","Training at Epoch 309 iteration 23 with loss 0.06694649\n","Training at Epoch 309 iteration 24 with loss 0.071305275\n","Training at Epoch 309 iteration 25 with loss 0.06955127\n","Training at Epoch 309 iteration 26 with loss 0.0687013\n","Training at Epoch 309 iteration 27 with loss 0.07086022\n","Training at Epoch 309 iteration 28 with loss 0.07260321\n","Training at Epoch 309 iteration 29 with loss 0.06824806\n","Training at Epoch 309 iteration 30 with loss 0.06954392\n","Training at Epoch 310 iteration 0 with loss 0.06912375\n","Training at Epoch 310 iteration 1 with loss 0.06955646\n","Training at Epoch 310 iteration 2 with loss 0.06825749\n","Training at Epoch 310 iteration 3 with loss 0.06739483\n","Training at Epoch 310 iteration 4 with loss 0.06827021\n","Training at Epoch 310 iteration 5 with loss 0.06955908\n","Training at Epoch 310 iteration 6 with loss 0.06867178\n","Training at Epoch 310 iteration 7 with loss 0.06825733\n","Training at Epoch 310 iteration 8 with loss 0.06825834\n","Training at Epoch 310 iteration 9 with loss 0.07085173\n","Training at Epoch 310 iteration 10 with loss 0.07129612\n","Training at Epoch 310 iteration 11 with loss 0.068252325\n","Training at Epoch 310 iteration 12 with loss 0.066513516\n","Training at Epoch 310 iteration 13 with loss 0.06955336\n","Training at Epoch 310 iteration 14 with loss 0.06607187\n","Training at Epoch 310 iteration 15 with loss 0.06912317\n","Training at Epoch 310 iteration 16 with loss 0.07041285\n","Training at Epoch 310 iteration 17 with loss 0.07129444\n","Training at Epoch 310 iteration 18 with loss 0.06956072\n","Training at Epoch 310 iteration 19 with loss 0.06953807\n","Training at Epoch 310 iteration 20 with loss 0.06998572\n","Training at Epoch 310 iteration 21 with loss 0.0708593\n","Training at Epoch 310 iteration 22 with loss 0.06910522\n","Training at Epoch 310 iteration 23 with loss 0.06738113\n","Training at Epoch 310 iteration 24 with loss 0.06868559\n","Training at Epoch 310 iteration 25 with loss 0.06999672\n","Training at Epoch 310 iteration 26 with loss 0.066952705\n","Training at Epoch 310 iteration 27 with loss 0.068255685\n","Training at Epoch 310 iteration 28 with loss 0.06868887\n","Training at Epoch 310 iteration 29 with loss 0.07174816\n","Training at Epoch 310 iteration 30 with loss 0.07042217\n","Training at Epoch 311 iteration 0 with loss 0.06781848\n","Training at Epoch 311 iteration 1 with loss 0.067820214\n","Training at Epoch 311 iteration 2 with loss 0.06998953\n","Training at Epoch 311 iteration 3 with loss 0.06912576\n","Training at Epoch 311 iteration 4 with loss 0.06781956\n","Training at Epoch 311 iteration 5 with loss 0.06916326\n","Training at Epoch 311 iteration 6 with loss 0.0695554\n","Training at Epoch 311 iteration 7 with loss 0.07086151\n","Training at Epoch 311 iteration 8 with loss 0.069552094\n","Training at Epoch 311 iteration 9 with loss 0.07128991\n","Training at Epoch 311 iteration 10 with loss 0.07041113\n","Training at Epoch 311 iteration 11 with loss 0.0686808\n","Training at Epoch 311 iteration 12 with loss 0.06869175\n","Training at Epoch 311 iteration 13 with loss 0.069123834\n","Training at Epoch 311 iteration 14 with loss 0.06955871\n","Training at Epoch 311 iteration 15 with loss 0.06740259\n","Training at Epoch 311 iteration 16 with loss 0.06826265\n","Training at Epoch 311 iteration 17 with loss 0.06955235\n","Training at Epoch 311 iteration 18 with loss 0.07213168\n","Training at Epoch 311 iteration 19 with loss 0.06955366\n","Training at Epoch 311 iteration 20 with loss 0.067424454\n","Training at Epoch 311 iteration 21 with loss 0.07208203\n","Training at Epoch 311 iteration 22 with loss 0.06785964\n","Training at Epoch 311 iteration 23 with loss 0.067851916\n","Training at Epoch 311 iteration 24 with loss 0.066584006\n","Training at Epoch 311 iteration 25 with loss 0.068697505\n","Training at Epoch 311 iteration 26 with loss 0.06995979\n","Training at Epoch 311 iteration 27 with loss 0.068273924\n","Training at Epoch 311 iteration 28 with loss 0.06996766\n","Training at Epoch 311 iteration 29 with loss 0.06700446\n","Training at Epoch 311 iteration 30 with loss 0.06996095\n","Training at Epoch 312 iteration 0 with loss 0.06995602\n","Training at Epoch 312 iteration 1 with loss 0.06742188\n","Training at Epoch 312 iteration 2 with loss 0.07038511\n","Training at Epoch 312 iteration 3 with loss 0.06869804\n","Training at Epoch 312 iteration 4 with loss 0.068694025\n","Training at Epoch 312 iteration 5 with loss 0.06912471\n","Training at Epoch 312 iteration 6 with loss 0.06954883\n","Training at Epoch 312 iteration 7 with loss 0.07038821\n","Training at Epoch 312 iteration 8 with loss 0.068684645\n","Training at Epoch 312 iteration 9 with loss 0.06996445\n","Training at Epoch 312 iteration 10 with loss 0.06912023\n","Training at Epoch 312 iteration 11 with loss 0.06995847\n","Training at Epoch 312 iteration 12 with loss 0.068274476\n","Training at Epoch 312 iteration 13 with loss 0.06828295\n","Training at Epoch 312 iteration 14 with loss 0.06785986\n","Training at Epoch 312 iteration 15 with loss 0.06744187\n","Training at Epoch 312 iteration 16 with loss 0.06871088\n","Training at Epoch 312 iteration 17 with loss 0.06954836\n","Training at Epoch 312 iteration 18 with loss 0.070386976\n","Training at Epoch 312 iteration 19 with loss 0.07078812\n","Training at Epoch 312 iteration 20 with loss 0.06703392\n","Training at Epoch 312 iteration 21 with loss 0.068701744\n","Training at Epoch 312 iteration 22 with loss 0.06870572\n","Training at Epoch 312 iteration 23 with loss 0.07162498\n","Training at Epoch 312 iteration 24 with loss 0.069535166\n","Training at Epoch 312 iteration 25 with loss 0.06745274\n","Training at Epoch 312 iteration 26 with loss 0.06912078\n","Training at Epoch 312 iteration 27 with loss 0.07077877\n","Training at Epoch 312 iteration 28 with loss 0.0686973\n","Training at Epoch 312 iteration 29 with loss 0.069529675\n","Training at Epoch 312 iteration 30 with loss 0.06787585\n","Training at Epoch 313 iteration 0 with loss 0.06912062\n","Training at Epoch 313 iteration 1 with loss 0.069535136\n","Training at Epoch 313 iteration 2 with loss 0.06829299\n","Training at Epoch 313 iteration 3 with loss 0.0699368\n","Training at Epoch 313 iteration 4 with loss 0.06953491\n","Training at Epoch 313 iteration 5 with loss 0.06870913\n","Training at Epoch 313 iteration 6 with loss 0.06869365\n","Training at Epoch 313 iteration 7 with loss 0.069546685\n","Training at Epoch 313 iteration 8 with loss 0.06953184\n","Training at Epoch 313 iteration 9 with loss 0.06829281\n","Training at Epoch 313 iteration 10 with loss 0.070770785\n","Training at Epoch 313 iteration 11 with loss 0.069947086\n","Training at Epoch 313 iteration 12 with loss 0.069519214\n","Training at Epoch 313 iteration 13 with loss 0.067883894\n","Training at Epoch 313 iteration 14 with loss 0.06870904\n","Training at Epoch 313 iteration 15 with loss 0.06623074\n","Training at Epoch 313 iteration 16 with loss 0.06951236\n","Training at Epoch 313 iteration 17 with loss 0.06994406\n","Training at Epoch 313 iteration 18 with loss 0.069533594\n","Training at Epoch 313 iteration 19 with loss 0.06746756\n","Training at Epoch 313 iteration 20 with loss 0.06870786\n","Training at Epoch 313 iteration 21 with loss 0.06912281\n","Training at Epoch 313 iteration 22 with loss 0.069945194\n","Training at Epoch 313 iteration 23 with loss 0.067873135\n","Training at Epoch 313 iteration 24 with loss 0.069939986\n","Training at Epoch 313 iteration 25 with loss 0.07159914\n","Training at Epoch 313 iteration 26 with loss 0.0687086\n","Training at Epoch 313 iteration 27 with loss 0.06953044\n","Training at Epoch 313 iteration 28 with loss 0.06787345\n","Training at Epoch 313 iteration 29 with loss 0.06953835\n","Training at Epoch 313 iteration 30 with loss 0.06788606\n","Training at Epoch 314 iteration 0 with loss 0.07036164\n","Training at Epoch 314 iteration 1 with loss 0.06748454\n","Training at Epoch 314 iteration 2 with loss 0.071573175\n","Training at Epoch 314 iteration 3 with loss 0.06870343\n","Training at Epoch 314 iteration 4 with loss 0.06830767\n","Training at Epoch 314 iteration 5 with loss 0.07076956\n","Training at Epoch 314 iteration 6 with loss 0.06830235\n","Training at Epoch 314 iteration 7 with loss 0.06870504\n","Training at Epoch 314 iteration 8 with loss 0.068295114\n","Training at Epoch 314 iteration 9 with loss 0.06788445\n","Training at Epoch 314 iteration 10 with loss 0.06912211\n","Training at Epoch 314 iteration 11 with loss 0.06747081\n","Training at Epoch 314 iteration 12 with loss 0.06829711\n","Training at Epoch 314 iteration 13 with loss 0.068706945\n","Training at Epoch 314 iteration 14 with loss 0.06911198\n","Training at Epoch 314 iteration 15 with loss 0.07078243\n","Training at Epoch 314 iteration 16 with loss 0.071177505\n","Training at Epoch 314 iteration 17 with loss 0.0695311\n","Training at Epoch 314 iteration 18 with loss 0.06910728\n","Training at Epoch 314 iteration 19 with loss 0.06994392\n","Training at Epoch 314 iteration 20 with loss 0.070348546\n","Training at Epoch 314 iteration 21 with loss 0.06828102\n","Training at Epoch 314 iteration 22 with loss 0.069116846\n","Training at Epoch 314 iteration 23 with loss 0.0687127\n","Training at Epoch 314 iteration 24 with loss 0.06870403\n","Training at Epoch 314 iteration 25 with loss 0.06666286\n","Training at Epoch 314 iteration 26 with loss 0.067484766\n","Training at Epoch 314 iteration 27 with loss 0.071189955\n","Training at Epoch 314 iteration 28 with loss 0.06830614\n","Training at Epoch 314 iteration 29 with loss 0.06953278\n","Training at Epoch 314 iteration 30 with loss 0.07035151\n","Training at Epoch 315 iteration 0 with loss 0.07117237\n","Training at Epoch 315 iteration 1 with loss 0.06911748\n","Training at Epoch 315 iteration 2 with loss 0.07035326\n","Training at Epoch 315 iteration 3 with loss 0.06994078\n","Training at Epoch 315 iteration 4 with loss 0.06912058\n","Training at Epoch 315 iteration 5 with loss 0.0691161\n","Training at Epoch 315 iteration 6 with loss 0.06749073\n","Training at Epoch 315 iteration 7 with loss 0.0699358\n","Training at Epoch 315 iteration 8 with loss 0.068707995\n","Training at Epoch 315 iteration 9 with loss 0.066656485\n","Training at Epoch 315 iteration 10 with loss 0.06914682\n","Training at Epoch 315 iteration 11 with loss 0.07034494\n","Training at Epoch 315 iteration 12 with loss 0.06995189\n","Training at Epoch 315 iteration 13 with loss 0.06828885\n","Training at Epoch 315 iteration 14 with loss 0.06953535\n","Training at Epoch 315 iteration 15 with loss 0.06830017\n","Training at Epoch 315 iteration 16 with loss 0.06707428\n","Training at Epoch 315 iteration 17 with loss 0.06791641\n","Training at Epoch 315 iteration 18 with loss 0.06748018\n","Training at Epoch 315 iteration 19 with loss 0.06871001\n","Training at Epoch 315 iteration 20 with loss 0.06913691\n","Training at Epoch 315 iteration 21 with loss 0.06828371\n","Training at Epoch 315 iteration 22 with loss 0.06952725\n","Training at Epoch 315 iteration 23 with loss 0.06829445\n","Training at Epoch 315 iteration 24 with loss 0.06993728\n","Training at Epoch 315 iteration 25 with loss 0.07077027\n","Training at Epoch 315 iteration 26 with loss 0.07117817\n","Training at Epoch 315 iteration 27 with loss 0.06870764\n","Training at Epoch 315 iteration 28 with loss 0.07034376\n","Training at Epoch 315 iteration 29 with loss 0.069107965\n","Training at Epoch 315 iteration 30 with loss 0.068290465\n","Training at Epoch 316 iteration 0 with loss 0.0687076\n","Training at Epoch 316 iteration 1 with loss 0.07159486\n","Training at Epoch 316 iteration 2 with loss 0.069120094\n","Training at Epoch 316 iteration 3 with loss 0.067882165\n","Training at Epoch 316 iteration 4 with loss 0.06911425\n","Training at Epoch 316 iteration 5 with loss 0.069940984\n","Training at Epoch 316 iteration 6 with loss 0.069104835\n","Training at Epoch 316 iteration 7 with loss 0.0715928\n","Training at Epoch 316 iteration 8 with loss 0.07158628\n","Training at Epoch 316 iteration 9 with loss 0.06953939\n","Training at Epoch 316 iteration 10 with loss 0.067918986\n","Training at Epoch 316 iteration 11 with loss 0.068719015\n","Training at Epoch 316 iteration 12 with loss 0.06831367\n","Training at Epoch 316 iteration 13 with loss 0.06750373\n","Training at Epoch 316 iteration 14 with loss 0.068723634\n","Training at Epoch 316 iteration 15 with loss 0.06748275\n","Training at Epoch 316 iteration 16 with loss 0.070732005\n","Training at Epoch 316 iteration 17 with loss 0.071144015\n","Training at Epoch 316 iteration 18 with loss 0.06709663\n","Training at Epoch 316 iteration 19 with loss 0.06912415\n","Training at Epoch 316 iteration 20 with loss 0.06912579\n","Training at Epoch 316 iteration 21 with loss 0.06871536\n","Training at Epoch 316 iteration 22 with loss 0.0675096\n","Training at Epoch 316 iteration 23 with loss 0.07032449\n","Training at Epoch 316 iteration 24 with loss 0.06872033\n","Training at Epoch 316 iteration 25 with loss 0.06992287\n","Training at Epoch 316 iteration 26 with loss 0.069934085\n","Training at Epoch 316 iteration 27 with loss 0.06831813\n","Training at Epoch 316 iteration 28 with loss 0.06871943\n","Training at Epoch 316 iteration 29 with loss 0.066309035\n","Training at Epoch 316 iteration 30 with loss 0.06952136\n","Training at Epoch 317 iteration 0 with loss 0.06871778\n","Training at Epoch 317 iteration 1 with loss 0.06790836\n","Training at Epoch 317 iteration 2 with loss 0.06791882\n","Training at Epoch 317 iteration 3 with loss 0.06871733\n","Training at Epoch 317 iteration 4 with loss 0.06991647\n","Training at Epoch 317 iteration 5 with loss 0.06831242\n","Training at Epoch 317 iteration 6 with loss 0.07072733\n","Training at Epoch 317 iteration 7 with loss 0.06911937\n","Training at Epoch 317 iteration 8 with loss 0.06953223\n","Training at Epoch 317 iteration 9 with loss 0.07029853\n","Training at Epoch 317 iteration 10 with loss 0.070742376\n","Training at Epoch 317 iteration 11 with loss 0.06831002\n","Training at Epoch 317 iteration 12 with loss 0.06871923\n","Training at Epoch 317 iteration 13 with loss 0.067914054\n","Training at Epoch 317 iteration 14 with loss 0.071541555\n","Training at Epoch 317 iteration 15 with loss 0.06831716\n","Training at Epoch 317 iteration 16 with loss 0.068693355\n","Training at Epoch 317 iteration 17 with loss 0.06788435\n","Training at Epoch 317 iteration 18 with loss 0.06993411\n","Training at Epoch 317 iteration 19 with loss 0.06952059\n","Training at Epoch 317 iteration 20 with loss 0.06669466\n","Training at Epoch 317 iteration 21 with loss 0.070327066\n","Training at Epoch 317 iteration 22 with loss 0.06751234\n","Training at Epoch 317 iteration 23 with loss 0.067902185\n","Training at Epoch 317 iteration 24 with loss 0.069135115\n","Training at Epoch 317 iteration 25 with loss 0.0679121\n","Training at Epoch 317 iteration 26 with loss 0.06914404\n","Training at Epoch 317 iteration 27 with loss 0.069928326\n","Training at Epoch 317 iteration 28 with loss 0.06950949\n","Training at Epoch 317 iteration 29 with loss 0.07033205\n","Training at Epoch 317 iteration 30 with loss 0.07076464\n","Training at Epoch 318 iteration 0 with loss 0.0691219\n","Training at Epoch 318 iteration 1 with loss 0.069120824\n","Training at Epoch 318 iteration 2 with loss 0.06829176\n","Training at Epoch 318 iteration 3 with loss 0.06952877\n","Training at Epoch 318 iteration 4 with loss 0.06789559\n","Training at Epoch 318 iteration 5 with loss 0.06668316\n","Training at Epoch 318 iteration 6 with loss 0.067530505\n","Training at Epoch 318 iteration 7 with loss 0.068703294\n","Training at Epoch 318 iteration 8 with loss 0.06830059\n","Training at Epoch 318 iteration 9 with loss 0.07033684\n","Training at Epoch 318 iteration 10 with loss 0.068703614\n","Training at Epoch 318 iteration 11 with loss 0.0711867\n","Training at Epoch 318 iteration 12 with loss 0.06911294\n","Training at Epoch 318 iteration 13 with loss 0.06870581\n","Training at Epoch 318 iteration 14 with loss 0.06747411\n","Training at Epoch 318 iteration 15 with loss 0.07076756\n","Training at Epoch 318 iteration 16 with loss 0.070313476\n","Training at Epoch 318 iteration 17 with loss 0.06911621\n","Training at Epoch 318 iteration 18 with loss 0.06914805\n","Training at Epoch 318 iteration 19 with loss 0.0707475\n","Training at Epoch 318 iteration 20 with loss 0.06953685\n","Training at Epoch 318 iteration 21 with loss 0.069114044\n","Training at Epoch 318 iteration 22 with loss 0.06787985\n","Training at Epoch 318 iteration 23 with loss 0.06868235\n","Training at Epoch 318 iteration 24 with loss 0.068289526\n","Training at Epoch 318 iteration 25 with loss 0.06912147\n","Training at Epoch 318 iteration 26 with loss 0.06995224\n","Training at Epoch 318 iteration 27 with loss 0.07077286\n","Training at Epoch 318 iteration 28 with loss 0.06910382\n","Training at Epoch 318 iteration 29 with loss 0.07035385\n","Training at Epoch 318 iteration 30 with loss 0.06829499\n","Training at Epoch 319 iteration 0 with loss 0.06786508\n","Training at Epoch 319 iteration 1 with loss 0.06827514\n","Training at Epoch 319 iteration 2 with loss 0.071170196\n","Training at Epoch 319 iteration 3 with loss 0.06790358\n","Training at Epoch 319 iteration 4 with loss 0.0682924\n","Training at Epoch 319 iteration 5 with loss 0.069118746\n","Training at Epoch 319 iteration 6 with loss 0.068328835\n","Training at Epoch 319 iteration 7 with loss 0.06913282\n","Training at Epoch 319 iteration 8 with loss 0.0682988\n","Training at Epoch 319 iteration 9 with loss 0.06994916\n","Training at Epoch 319 iteration 10 with loss 0.06909456\n","Training at Epoch 319 iteration 11 with loss 0.06913032\n","Training at Epoch 319 iteration 12 with loss 0.068281464\n","Training at Epoch 319 iteration 13 with loss 0.070345536\n","Training at Epoch 319 iteration 14 with loss 0.06496714\n","Training at Epoch 319 iteration 15 with loss 0.07116632\n","Training at Epoch 319 iteration 16 with loss 0.068287104\n","Training at Epoch 319 iteration 17 with loss 0.07032116\n","Training at Epoch 319 iteration 18 with loss 0.06953256\n","Training at Epoch 319 iteration 19 with loss 0.07077542\n","Training at Epoch 319 iteration 20 with loss 0.06908546\n","Training at Epoch 319 iteration 21 with loss 0.06912025\n","Training at Epoch 319 iteration 22 with loss 0.069536015\n","Training at Epoch 319 iteration 23 with loss 0.06828252\n","Training at Epoch 319 iteration 24 with loss 0.071640715\n","Training at Epoch 319 iteration 25 with loss 0.069918744\n","Training at Epoch 319 iteration 26 with loss 0.070373796\n","Training at Epoch 319 iteration 27 with loss 0.06872606\n","Training at Epoch 319 iteration 28 with loss 0.069957934\n","Training at Epoch 319 iteration 29 with loss 0.069128454\n","Training at Epoch 319 iteration 30 with loss 0.06579974\n","Training at Epoch 320 iteration 0 with loss 0.06953714\n","Training at Epoch 320 iteration 1 with loss 0.06870534\n","Training at Epoch 320 iteration 2 with loss 0.06867686\n","Training at Epoch 320 iteration 3 with loss 0.06913014\n","Training at Epoch 320 iteration 4 with loss 0.069515325\n","Training at Epoch 320 iteration 5 with loss 0.06785584\n","Training at Epoch 320 iteration 6 with loss 0.06954628\n","Training at Epoch 320 iteration 7 with loss 0.07117982\n","Training at Epoch 320 iteration 8 with loss 0.0682942\n","Training at Epoch 320 iteration 9 with loss 0.06875424\n","Training at Epoch 320 iteration 10 with loss 0.06995146\n","Training at Epoch 320 iteration 11 with loss 0.067891896\n","Training at Epoch 320 iteration 12 with loss 0.07073426\n","Training at Epoch 320 iteration 13 with loss 0.06994952\n","Training at Epoch 320 iteration 14 with loss 0.0678729\n","Training at Epoch 320 iteration 15 with loss 0.067492165\n","Training at Epoch 320 iteration 16 with loss 0.069118045\n","Training at Epoch 320 iteration 17 with loss 0.07041447\n","Training at Epoch 320 iteration 18 with loss 0.06789197\n","Training at Epoch 320 iteration 19 with loss 0.07078938\n","Training at Epoch 320 iteration 20 with loss 0.069576226\n","Training at Epoch 320 iteration 21 with loss 0.067467555\n","Training at Epoch 320 iteration 22 with loss 0.06706246\n","Training at Epoch 320 iteration 23 with loss 0.07076008\n","Training at Epoch 320 iteration 24 with loss 0.069516845\n","Training at Epoch 320 iteration 25 with loss 0.06986723\n","Training at Epoch 320 iteration 26 with loss 0.068261966\n","Training at Epoch 320 iteration 27 with loss 0.06995194\n","Training at Epoch 320 iteration 28 with loss 0.06996503\n","Training at Epoch 320 iteration 29 with loss 0.06748499\n","Training at Epoch 320 iteration 30 with loss 0.06907822\n","Training at Epoch 321 iteration 0 with loss 0.07079587\n","Training at Epoch 321 iteration 1 with loss 0.069120884\n","Training at Epoch 321 iteration 2 with loss 0.069991246\n","Training at Epoch 321 iteration 3 with loss 0.0703675\n","Training at Epoch 321 iteration 4 with loss 0.06832911\n","Training at Epoch 321 iteration 5 with loss 0.06751323\n","Training at Epoch 321 iteration 6 with loss 0.070360124\n","Training at Epoch 321 iteration 7 with loss 0.067845695\n","Training at Epoch 321 iteration 8 with loss 0.06906987\n","Training at Epoch 321 iteration 9 with loss 0.0674709\n","Training at Epoch 321 iteration 10 with loss 0.06831905\n","Training at Epoch 321 iteration 11 with loss 0.06787373\n","Training at Epoch 321 iteration 12 with loss 0.06830776\n","Training at Epoch 321 iteration 13 with loss 0.07076707\n","Training at Epoch 321 iteration 14 with loss 0.06995356\n","Training at Epoch 321 iteration 15 with loss 0.06994067\n","Training at Epoch 321 iteration 16 with loss 0.06955088\n","Training at Epoch 321 iteration 17 with loss 0.067104444\n","Training at Epoch 321 iteration 18 with loss 0.06868155\n","Training at Epoch 321 iteration 19 with loss 0.06914685\n","Training at Epoch 321 iteration 20 with loss 0.069124356\n","Training at Epoch 321 iteration 21 with loss 0.06994735\n","Training at Epoch 321 iteration 22 with loss 0.06992215\n","Training at Epoch 321 iteration 23 with loss 0.06830215\n","Training at Epoch 321 iteration 24 with loss 0.06780317\n","Training at Epoch 321 iteration 25 with loss 0.07161562\n","Training at Epoch 321 iteration 26 with loss 0.06786746\n","Training at Epoch 321 iteration 27 with loss 0.07034656\n","Training at Epoch 321 iteration 28 with loss 0.069969706\n","Training at Epoch 321 iteration 29 with loss 0.06950941\n","Training at Epoch 321 iteration 30 with loss 0.06699004\n","Training at Epoch 322 iteration 0 with loss 0.07080066\n","Training at Epoch 322 iteration 1 with loss 0.06665073\n","Training at Epoch 322 iteration 2 with loss 0.06828527\n","Training at Epoch 322 iteration 3 with loss 0.07076877\n","Training at Epoch 322 iteration 4 with loss 0.069085486\n","Training at Epoch 322 iteration 5 with loss 0.07076416\n","Training at Epoch 322 iteration 6 with loss 0.068711445\n","Training at Epoch 322 iteration 7 with loss 0.0694644\n","Training at Epoch 322 iteration 8 with loss 0.06751428\n","Training at Epoch 322 iteration 9 with loss 0.06830238\n","Training at Epoch 322 iteration 10 with loss 0.067029245\n","Training at Epoch 322 iteration 11 with loss 0.06752207\n","Training at Epoch 322 iteration 12 with loss 0.069937445\n","Training at Epoch 322 iteration 13 with loss 0.06948558\n","Training at Epoch 322 iteration 14 with loss 0.07035143\n","Training at Epoch 322 iteration 15 with loss 0.06834031\n","Training at Epoch 322 iteration 16 with loss 0.06987501\n","Training at Epoch 322 iteration 17 with loss 0.06995394\n","Training at Epoch 322 iteration 18 with loss 0.069058515\n","Training at Epoch 322 iteration 19 with loss 0.069932655\n","Training at Epoch 322 iteration 20 with loss 0.0678848\n","Training at Epoch 322 iteration 21 with loss 0.06872869\n","Training at Epoch 322 iteration 22 with loss 0.06831133\n","Training at Epoch 322 iteration 23 with loss 0.0686879\n","Training at Epoch 322 iteration 24 with loss 0.06995084\n","Training at Epoch 322 iteration 25 with loss 0.0695372\n","Training at Epoch 322 iteration 26 with loss 0.07116341\n","Training at Epoch 322 iteration 27 with loss 0.069460966\n","Training at Epoch 322 iteration 28 with loss 0.06662503\n","Training at Epoch 322 iteration 29 with loss 0.06960033\n","Training at Epoch 322 iteration 30 with loss 0.069525145\n","Training at Epoch 323 iteration 0 with loss 0.06829275\n","Training at Epoch 323 iteration 1 with loss 0.067934245\n","Training at Epoch 323 iteration 2 with loss 0.06790387\n","Training at Epoch 323 iteration 3 with loss 0.07030286\n","Training at Epoch 323 iteration 4 with loss 0.06907893\n","Training at Epoch 323 iteration 5 with loss 0.06582047\n","Training at Epoch 323 iteration 6 with loss 0.06823286\n","Training at Epoch 323 iteration 7 with loss 0.070409045\n","Training at Epoch 323 iteration 8 with loss 0.0705115\n","Training at Epoch 323 iteration 9 with loss 0.06868336\n","Training at Epoch 323 iteration 10 with loss 0.06829484\n","Training at Epoch 323 iteration 11 with loss 0.070384145\n","Training at Epoch 323 iteration 12 with loss 0.06865391\n","Training at Epoch 323 iteration 13 with loss 0.069133446\n","Training at Epoch 323 iteration 14 with loss 0.06876201\n","Training at Epoch 323 iteration 15 with loss 0.067015946\n","Training at Epoch 323 iteration 16 with loss 0.07078257\n","Training at Epoch 323 iteration 17 with loss 0.06951472\n","Training at Epoch 323 iteration 18 with loss 0.06949814\n","Training at Epoch 323 iteration 19 with loss 0.072103634\n","Training at Epoch 323 iteration 20 with loss 0.06991106\n","Training at Epoch 323 iteration 21 with loss 0.06776917\n","Training at Epoch 323 iteration 22 with loss 0.06869519\n","Training at Epoch 323 iteration 23 with loss 0.068306446\n","Training at Epoch 323 iteration 24 with loss 0.06919724\n","Training at Epoch 323 iteration 25 with loss 0.06872447\n","Training at Epoch 323 iteration 26 with loss 0.069884144\n","Training at Epoch 323 iteration 27 with loss 0.070878245\n","Training at Epoch 323 iteration 28 with loss 0.06830855\n","Training at Epoch 323 iteration 29 with loss 0.06953083\n","Training at Epoch 323 iteration 30 with loss 0.0691184\n","Training at Epoch 324 iteration 0 with loss 0.06953316\n","Training at Epoch 324 iteration 1 with loss 0.06913469\n","Training at Epoch 324 iteration 2 with loss 0.06994022\n","Training at Epoch 324 iteration 3 with loss 0.067872226\n","Training at Epoch 324 iteration 4 with loss 0.070784405\n","Training at Epoch 324 iteration 5 with loss 0.070390046\n","Training at Epoch 324 iteration 6 with loss 0.06948432\n","Training at Epoch 324 iteration 7 with loss 0.06833076\n","Training at Epoch 324 iteration 8 with loss 0.06740576\n","Training at Epoch 324 iteration 9 with loss 0.07037376\n","Training at Epoch 324 iteration 10 with loss 0.06831695\n","Training at Epoch 324 iteration 11 with loss 0.06746277\n","Training at Epoch 324 iteration 12 with loss 0.067863844\n","Training at Epoch 324 iteration 13 with loss 0.06995521\n","Training at Epoch 324 iteration 14 with loss 0.07075389\n","Training at Epoch 324 iteration 15 with loss 0.06748907\n","Training at Epoch 324 iteration 16 with loss 0.06878544\n","Training at Epoch 324 iteration 17 with loss 0.068239786\n","Training at Epoch 324 iteration 18 with loss 0.067454524\n","Training at Epoch 324 iteration 19 with loss 0.069496825\n","Training at Epoch 324 iteration 20 with loss 0.070334606\n","Training at Epoch 324 iteration 21 with loss 0.068742804\n","Training at Epoch 324 iteration 22 with loss 0.06872691\n","Training at Epoch 324 iteration 23 with loss 0.068192296\n","Training at Epoch 324 iteration 24 with loss 0.06999656\n","Training at Epoch 324 iteration 25 with loss 0.071135774\n","Training at Epoch 324 iteration 26 with loss 0.06988218\n","Training at Epoch 324 iteration 27 with loss 0.07041441\n","Training at Epoch 324 iteration 28 with loss 0.06826209\n","Training at Epoch 324 iteration 29 with loss 0.06786891\n","Training at Epoch 324 iteration 30 with loss 0.06915784\n","Training at Epoch 325 iteration 0 with loss 0.06992682\n","Training at Epoch 325 iteration 1 with loss 0.069509685\n","Training at Epoch 325 iteration 2 with loss 0.06913547\n","Training at Epoch 325 iteration 3 with loss 0.066558644\n","Training at Epoch 325 iteration 4 with loss 0.069559045\n","Training at Epoch 325 iteration 5 with loss 0.06834825\n","Training at Epoch 325 iteration 6 with loss 0.06904222\n","Training at Epoch 325 iteration 7 with loss 0.06987558\n","Training at Epoch 325 iteration 8 with loss 0.06919305\n","Training at Epoch 325 iteration 9 with loss 0.067411356\n","Training at Epoch 325 iteration 10 with loss 0.067123495\n","Training at Epoch 325 iteration 11 with loss 0.069958605\n","Training at Epoch 325 iteration 12 with loss 0.06916573\n","Training at Epoch 325 iteration 13 with loss 0.067878954\n","Training at Epoch 325 iteration 14 with loss 0.06828697\n","Training at Epoch 325 iteration 15 with loss 0.06866956\n","Training at Epoch 325 iteration 16 with loss 0.070027925\n","Training at Epoch 325 iteration 17 with loss 0.07043018\n","Training at Epoch 325 iteration 18 with loss 0.06726581\n","Training at Epoch 325 iteration 19 with loss 0.07079136\n","Training at Epoch 325 iteration 20 with loss 0.06935156\n","Training at Epoch 325 iteration 21 with loss 0.06997236\n","Training at Epoch 325 iteration 22 with loss 0.06910269\n","Training at Epoch 325 iteration 23 with loss 0.06910441\n","Training at Epoch 325 iteration 24 with loss 0.070975095\n","Training at Epoch 325 iteration 25 with loss 0.07031889\n","Training at Epoch 325 iteration 26 with loss 0.06961045\n","Training at Epoch 325 iteration 27 with loss 0.06781997\n","Training at Epoch 325 iteration 28 with loss 0.070344046\n","Training at Epoch 325 iteration 29 with loss 0.06946172\n","Training at Epoch 325 iteration 30 with loss 0.0682541\n","Training at Epoch 326 iteration 0 with loss 0.0682445\n","Training at Epoch 326 iteration 1 with loss 0.06698424\n","Training at Epoch 326 iteration 2 with loss 0.071320996\n","Training at Epoch 326 iteration 3 with loss 0.07041475\n","Training at Epoch 326 iteration 4 with loss 0.067807354\n","Training at Epoch 326 iteration 5 with loss 0.06946184\n","Training at Epoch 326 iteration 6 with loss 0.068693414\n","Training at Epoch 326 iteration 7 with loss 0.069469586\n","Training at Epoch 326 iteration 8 with loss 0.07243311\n","Training at Epoch 326 iteration 9 with loss 0.071222246\n","Training at Epoch 326 iteration 10 with loss 0.06799488\n","Training at Epoch 326 iteration 11 with loss 0.06869657\n","Training at Epoch 326 iteration 12 with loss 0.0672849\n","Training at Epoch 326 iteration 13 with loss 0.069166556\n","Training at Epoch 326 iteration 14 with loss 0.0690042\n","Training at Epoch 326 iteration 15 with loss 0.06910047\n","Training at Epoch 326 iteration 16 with loss 0.06986056\n","Training at Epoch 326 iteration 17 with loss 0.06868007\n","Training at Epoch 326 iteration 18 with loss 0.067570895\n","Training at Epoch 326 iteration 19 with loss 0.07048115\n","Training at Epoch 326 iteration 20 with loss 0.06960543\n","Training at Epoch 326 iteration 21 with loss 0.068886824\n","Training at Epoch 326 iteration 22 with loss 0.06778812\n","Training at Epoch 326 iteration 23 with loss 0.06993667\n","Training at Epoch 326 iteration 24 with loss 0.07047393\n","Training at Epoch 326 iteration 25 with loss 0.06837617\n","Training at Epoch 326 iteration 26 with loss 0.06912864\n","Training at Epoch 326 iteration 27 with loss 0.06842135\n","Training at Epoch 326 iteration 28 with loss 0.06915079\n","Training at Epoch 326 iteration 29 with loss 0.06740188\n","Training at Epoch 326 iteration 30 with loss 0.06910811\n","Training at Epoch 327 iteration 0 with loss 0.070023745\n","Training at Epoch 327 iteration 1 with loss 0.06816047\n","Training at Epoch 327 iteration 2 with loss 0.071162246\n","Training at Epoch 327 iteration 3 with loss 0.06820907\n","Training at Epoch 327 iteration 4 with loss 0.069850266\n","Training at Epoch 327 iteration 5 with loss 0.06713102\n","Training at Epoch 327 iteration 6 with loss 0.07109876\n","Training at Epoch 327 iteration 7 with loss 0.06853144\n","Training at Epoch 327 iteration 8 with loss 0.0675038\n","Training at Epoch 327 iteration 9 with loss 0.06839067\n","Training at Epoch 327 iteration 10 with loss 0.069533296\n","Training at Epoch 327 iteration 11 with loss 0.06758411\n","Training at Epoch 327 iteration 12 with loss 0.06989942\n","Training at Epoch 327 iteration 13 with loss 0.06979255\n","Training at Epoch 327 iteration 14 with loss 0.07073803\n","Training at Epoch 327 iteration 15 with loss 0.07051938\n","Training at Epoch 327 iteration 16 with loss 0.066215925\n","Training at Epoch 327 iteration 17 with loss 0.06754204\n","Training at Epoch 327 iteration 18 with loss 0.06885382\n","Training at Epoch 327 iteration 19 with loss 0.068809055\n","Training at Epoch 327 iteration 20 with loss 0.070363425\n","Training at Epoch 327 iteration 21 with loss 0.069111384\n","Training at Epoch 327 iteration 22 with loss 0.069967724\n","Training at Epoch 327 iteration 23 with loss 0.0716416\n","Training at Epoch 327 iteration 24 with loss 0.06935785\n","Training at Epoch 327 iteration 25 with loss 0.07025989\n","Training at Epoch 327 iteration 26 with loss 0.068622224\n","Training at Epoch 327 iteration 27 with loss 0.06734208\n","Training at Epoch 327 iteration 28 with loss 0.06785445\n","Training at Epoch 327 iteration 29 with loss 0.069041945\n","Training at Epoch 327 iteration 30 with loss 0.068828784\n","Training at Epoch 328 iteration 0 with loss 0.070918426\n","Training at Epoch 328 iteration 1 with loss 0.06855504\n","Training at Epoch 328 iteration 2 with loss 0.06755322\n","Training at Epoch 328 iteration 3 with loss 0.0673015\n","Training at Epoch 328 iteration 4 with loss 0.07052018\n","Training at Epoch 328 iteration 5 with loss 0.070721686\n","Training at Epoch 328 iteration 6 with loss 0.06919627\n","Training at Epoch 328 iteration 7 with loss 0.07079418\n","Training at Epoch 328 iteration 8 with loss 0.07068236\n","Training at Epoch 328 iteration 9 with loss 0.067933924\n","Training at Epoch 328 iteration 10 with loss 0.06764124\n","Training at Epoch 328 iteration 11 with loss 0.06732461\n","Training at Epoch 328 iteration 12 with loss 0.070387915\n","Training at Epoch 328 iteration 13 with loss 0.07028849\n","Training at Epoch 328 iteration 14 with loss 0.0690586\n","Training at Epoch 328 iteration 15 with loss 0.067414925\n","Training at Epoch 328 iteration 16 with loss 0.06965469\n","Training at Epoch 328 iteration 17 with loss 0.067259885\n","Training at Epoch 328 iteration 18 with loss 0.073030345\n","Training at Epoch 328 iteration 19 with loss 0.06905081\n","Training at Epoch 328 iteration 20 with loss 0.07044919\n","Training at Epoch 328 iteration 21 with loss 0.06801963\n","Training at Epoch 328 iteration 22 with loss 0.067830466\n","Training at Epoch 328 iteration 23 with loss 0.0696944\n","Training at Epoch 328 iteration 24 with loss 0.06968565\n","Training at Epoch 328 iteration 25 with loss 0.067135155\n","Training at Epoch 328 iteration 26 with loss 0.069733374\n","Training at Epoch 328 iteration 27 with loss 0.068336084\n","Training at Epoch 328 iteration 28 with loss 0.06849106\n","Training at Epoch 328 iteration 29 with loss 0.06808169\n","Training at Epoch 328 iteration 30 with loss 0.06841741\n","Training at Epoch 329 iteration 0 with loss 0.068514064\n","Training at Epoch 329 iteration 1 with loss 0.07004006\n","Training at Epoch 329 iteration 2 with loss 0.067726396\n","Training at Epoch 329 iteration 3 with loss 0.06832473\n","Training at Epoch 329 iteration 4 with loss 0.07014321\n","Training at Epoch 329 iteration 5 with loss 0.067949906\n","Training at Epoch 329 iteration 6 with loss 0.06858041\n","Training at Epoch 329 iteration 7 with loss 0.0692835\n","Training at Epoch 329 iteration 8 with loss 0.06694749\n","Training at Epoch 329 iteration 9 with loss 0.06966664\n","Training at Epoch 329 iteration 10 with loss 0.06754714\n","Training at Epoch 329 iteration 11 with loss 0.069820814\n","Training at Epoch 329 iteration 12 with loss 0.06945334\n","Training at Epoch 329 iteration 13 with loss 0.06874003\n","Training at Epoch 329 iteration 14 with loss 0.069180235\n","Training at Epoch 329 iteration 15 with loss 0.06906217\n","Training at Epoch 329 iteration 16 with loss 0.0681396\n","Training at Epoch 329 iteration 17 with loss 0.068589054\n","Training at Epoch 329 iteration 18 with loss 0.06913924\n","Training at Epoch 329 iteration 19 with loss 0.07004915\n","Training at Epoch 329 iteration 20 with loss 0.06891047\n","Training at Epoch 329 iteration 21 with loss 0.06923151\n","Training at Epoch 329 iteration 22 with loss 0.069938466\n","Training at Epoch 329 iteration 23 with loss 0.0683821\n","Training at Epoch 329 iteration 24 with loss 0.06847356\n","Training at Epoch 329 iteration 25 with loss 0.07026907\n","Training at Epoch 329 iteration 26 with loss 0.07073365\n","Training at Epoch 329 iteration 27 with loss 0.06883673\n","Training at Epoch 329 iteration 28 with loss 0.06976752\n","Training at Epoch 329 iteration 29 with loss 0.06959319\n","Training at Epoch 329 iteration 30 with loss 0.07044408\n","Training at Epoch 330 iteration 0 with loss 0.0682174\n","Training at Epoch 330 iteration 1 with loss 0.069831684\n","Training at Epoch 330 iteration 2 with loss 0.06925082\n","Training at Epoch 330 iteration 3 with loss 0.06906955\n","Training at Epoch 330 iteration 4 with loss 0.07110557\n","Training at Epoch 330 iteration 5 with loss 0.06780426\n","Training at Epoch 330 iteration 6 with loss 0.069219746\n","Training at Epoch 330 iteration 7 with loss 0.07150003\n","Training at Epoch 330 iteration 8 with loss 0.06921587\n","Training at Epoch 330 iteration 9 with loss 0.06912918\n","Training at Epoch 330 iteration 10 with loss 0.06891085\n","Training at Epoch 330 iteration 11 with loss 0.06837716\n","Training at Epoch 330 iteration 12 with loss 0.06678152\n","Training at Epoch 330 iteration 13 with loss 0.06933606\n","Training at Epoch 330 iteration 14 with loss 0.06801104\n","Training at Epoch 330 iteration 15 with loss 0.069416896\n","Training at Epoch 330 iteration 16 with loss 0.0696139\n","Training at Epoch 330 iteration 17 with loss 0.06944459\n","Training at Epoch 330 iteration 18 with loss 0.06755378\n","Training at Epoch 330 iteration 19 with loss 0.0697868\n","Training at Epoch 330 iteration 20 with loss 0.06980155\n","Training at Epoch 330 iteration 21 with loss 0.067649625\n","Training at Epoch 330 iteration 22 with loss 0.06982009\n","Training at Epoch 330 iteration 23 with loss 0.068113275\n","Training at Epoch 330 iteration 24 with loss 0.068741865\n","Training at Epoch 330 iteration 25 with loss 0.06877579\n","Training at Epoch 330 iteration 26 with loss 0.070159584\n","Training at Epoch 330 iteration 27 with loss 0.06884058\n","Training at Epoch 330 iteration 28 with loss 0.06970197\n","Training at Epoch 330 iteration 29 with loss 0.06908279\n","Training at Epoch 330 iteration 30 with loss 0.06879654\n","Training at Epoch 331 iteration 0 with loss 0.069561206\n","Training at Epoch 331 iteration 1 with loss 0.07001073\n","Training at Epoch 331 iteration 2 with loss 0.07050085\n","Training at Epoch 331 iteration 3 with loss 0.0697359\n","Training at Epoch 331 iteration 4 with loss 0.06906001\n","Training at Epoch 331 iteration 5 with loss 0.06850775\n","Training at Epoch 331 iteration 6 with loss 0.06907792\n","Training at Epoch 331 iteration 7 with loss 0.070648625\n","Training at Epoch 331 iteration 8 with loss 0.068422265\n","Training at Epoch 331 iteration 9 with loss 0.069253705\n","Training at Epoch 331 iteration 10 with loss 0.0691375\n","Training at Epoch 331 iteration 11 with loss 0.06678374\n","Training at Epoch 331 iteration 12 with loss 0.068011776\n","Training at Epoch 331 iteration 13 with loss 0.06746979\n","Training at Epoch 331 iteration 14 with loss 0.069650725\n","Training at Epoch 331 iteration 15 with loss 0.06884168\n","Training at Epoch 331 iteration 16 with loss 0.07080326\n","Training at Epoch 331 iteration 17 with loss 0.06881224\n","Training at Epoch 331 iteration 18 with loss 0.070018604\n","Training at Epoch 331 iteration 19 with loss 0.068810165\n","Training at Epoch 331 iteration 20 with loss 0.06852189\n","Training at Epoch 331 iteration 21 with loss 0.06932129\n","Training at Epoch 331 iteration 22 with loss 0.06941294\n","Training at Epoch 331 iteration 23 with loss 0.068085715\n","Training at Epoch 331 iteration 24 with loss 0.06929907\n","Training at Epoch 331 iteration 25 with loss 0.06877631\n","Training at Epoch 331 iteration 26 with loss 0.0702314\n","Training at Epoch 331 iteration 27 with loss 0.069671676\n","Training at Epoch 331 iteration 28 with loss 0.069328226\n","Training at Epoch 331 iteration 29 with loss 0.06907477\n","Training at Epoch 331 iteration 30 with loss 0.07051663\n","Training at Epoch 332 iteration 0 with loss 0.06935897\n","Training at Epoch 332 iteration 1 with loss 0.068516575\n","Training at Epoch 332 iteration 2 with loss 0.06940192\n","Training at Epoch 332 iteration 3 with loss 0.0688348\n","Training at Epoch 332 iteration 4 with loss 0.06930293\n","Training at Epoch 332 iteration 5 with loss 0.06902798\n","Training at Epoch 332 iteration 6 with loss 0.070403\n","Training at Epoch 332 iteration 7 with loss 0.06762138\n","Training at Epoch 332 iteration 8 with loss 0.06856761\n","Training at Epoch 332 iteration 9 with loss 0.06859249\n","Training at Epoch 332 iteration 10 with loss 0.06810729\n","Training at Epoch 332 iteration 11 with loss 0.07147016\n","Training at Epoch 332 iteration 12 with loss 0.07020273\n","Training at Epoch 332 iteration 13 with loss 0.06918057\n","Training at Epoch 332 iteration 14 with loss 0.06755499\n","Training at Epoch 332 iteration 15 with loss 0.06786742\n","Training at Epoch 332 iteration 16 with loss 0.068988346\n","Training at Epoch 332 iteration 17 with loss 0.070838645\n","Training at Epoch 332 iteration 18 with loss 0.0694811\n","Training at Epoch 332 iteration 19 with loss 0.06961374\n","Training at Epoch 332 iteration 20 with loss 0.06822171\n","Training at Epoch 332 iteration 21 with loss 0.070366405\n","Training at Epoch 332 iteration 22 with loss 0.07037939\n","Training at Epoch 332 iteration 23 with loss 0.06923971\n","Training at Epoch 332 iteration 24 with loss 0.06835302\n","Training at Epoch 332 iteration 25 with loss 0.066602245\n","Training at Epoch 332 iteration 26 with loss 0.067939855\n","Training at Epoch 332 iteration 27 with loss 0.07005894\n","Training at Epoch 332 iteration 28 with loss 0.07039356\n","Training at Epoch 332 iteration 29 with loss 0.06923471\n","Training at Epoch 332 iteration 30 with loss 0.06807202\n","Training at Epoch 333 iteration 0 with loss 0.067650065\n","Training at Epoch 333 iteration 1 with loss 0.06779492\n","Training at Epoch 333 iteration 2 with loss 0.06721705\n","Training at Epoch 333 iteration 3 with loss 0.06874156\n","Training at Epoch 333 iteration 4 with loss 0.06925203\n","Training at Epoch 333 iteration 5 with loss 0.07170007\n","Training at Epoch 333 iteration 6 with loss 0.068267606\n","Training at Epoch 333 iteration 7 with loss 0.071266636\n","Training at Epoch 333 iteration 8 with loss 0.06892719\n","Training at Epoch 333 iteration 9 with loss 0.06870484\n","Training at Epoch 333 iteration 10 with loss 0.06987461\n","Training at Epoch 333 iteration 11 with loss 0.07060155\n","Training at Epoch 333 iteration 12 with loss 0.06935295\n","Training at Epoch 333 iteration 13 with loss 0.06921469\n","Training at Epoch 333 iteration 14 with loss 0.069074474\n","Training at Epoch 333 iteration 15 with loss 0.06876471\n","Training at Epoch 333 iteration 16 with loss 0.069952324\n","Training at Epoch 333 iteration 17 with loss 0.06920251\n","Training at Epoch 333 iteration 18 with loss 0.06931309\n","Training at Epoch 333 iteration 19 with loss 0.06887395\n","Training at Epoch 333 iteration 20 with loss 0.070866376\n","Training at Epoch 333 iteration 21 with loss 0.06972842\n","Training at Epoch 333 iteration 22 with loss 0.066226155\n","Training at Epoch 333 iteration 23 with loss 0.067706086\n","Training at Epoch 333 iteration 24 with loss 0.06942499\n","Training at Epoch 333 iteration 25 with loss 0.07032411\n","Training at Epoch 333 iteration 26 with loss 0.068397805\n","Training at Epoch 333 iteration 27 with loss 0.06698406\n","Training at Epoch 333 iteration 28 with loss 0.0671419\n","Training at Epoch 333 iteration 29 with loss 0.07012338\n","Training at Epoch 333 iteration 30 with loss 0.07179768\n","Training at Epoch 334 iteration 0 with loss 0.069507346\n","Training at Epoch 334 iteration 1 with loss 0.06652634\n","Training at Epoch 334 iteration 2 with loss 0.0701444\n","Training at Epoch 334 iteration 3 with loss 0.068889\n","Training at Epoch 334 iteration 4 with loss 0.06946321\n","Training at Epoch 334 iteration 5 with loss 0.06992003\n","Training at Epoch 334 iteration 6 with loss 0.06882593\n","Training at Epoch 334 iteration 7 with loss 0.07063146\n","Training at Epoch 334 iteration 8 with loss 0.06869726\n","Training at Epoch 334 iteration 9 with loss 0.068183325\n","Training at Epoch 334 iteration 10 with loss 0.06958364\n","Training at Epoch 334 iteration 11 with loss 0.06848083\n","Training at Epoch 334 iteration 12 with loss 0.07061953\n","Training at Epoch 334 iteration 13 with loss 0.07034071\n","Training at Epoch 334 iteration 14 with loss 0.067936406\n","Training at Epoch 334 iteration 15 with loss 0.07008252\n","Training at Epoch 334 iteration 16 with loss 0.067085125\n","Training at Epoch 334 iteration 17 with loss 0.06779228\n","Training at Epoch 334 iteration 18 with loss 0.06969813\n","Training at Epoch 334 iteration 19 with loss 0.06810717\n","Training at Epoch 334 iteration 20 with loss 0.06790803\n","Training at Epoch 334 iteration 21 with loss 0.06938333\n","Training at Epoch 334 iteration 22 with loss 0.06982941\n","Training at Epoch 334 iteration 23 with loss 0.06982153\n","Training at Epoch 334 iteration 24 with loss 0.068546005\n","Training at Epoch 334 iteration 25 with loss 0.071470365\n","Training at Epoch 334 iteration 26 with loss 0.06946189\n","Training at Epoch 334 iteration 27 with loss 0.06843244\n","Training at Epoch 334 iteration 28 with loss 0.067733765\n","Training at Epoch 334 iteration 29 with loss 0.07146162\n","Training at Epoch 334 iteration 30 with loss 0.06804219\n","Training at Epoch 335 iteration 0 with loss 0.071092986\n","Training at Epoch 335 iteration 1 with loss 0.068339944\n","Training at Epoch 335 iteration 2 with loss 0.068596624\n","Training at Epoch 335 iteration 3 with loss 0.07051419\n","Training at Epoch 335 iteration 4 with loss 0.07018215\n","Training at Epoch 335 iteration 5 with loss 0.068377964\n","Training at Epoch 335 iteration 6 with loss 0.06978932\n","Training at Epoch 335 iteration 7 with loss 0.06925615\n","Training at Epoch 335 iteration 8 with loss 0.068499364\n","Training at Epoch 335 iteration 9 with loss 0.06946037\n","Training at Epoch 335 iteration 10 with loss 0.07247706\n","Training at Epoch 335 iteration 11 with loss 0.06898496\n","Training at Epoch 335 iteration 12 with loss 0.070110455\n","Training at Epoch 335 iteration 13 with loss 0.06773728\n","Training at Epoch 335 iteration 14 with loss 0.070338406\n","Training at Epoch 335 iteration 15 with loss 0.06895363\n","Training at Epoch 335 iteration 16 with loss 0.0688363\n","Training at Epoch 335 iteration 17 with loss 0.06878868\n","Training at Epoch 335 iteration 18 with loss 0.069917955\n","Training at Epoch 335 iteration 19 with loss 0.06660601\n","Training at Epoch 335 iteration 20 with loss 0.069978796\n","Training at Epoch 335 iteration 21 with loss 0.06809714\n","Training at Epoch 335 iteration 22 with loss 0.069492705\n","Training at Epoch 335 iteration 23 with loss 0.068410866\n","Training at Epoch 335 iteration 24 with loss 0.06702961\n","Training at Epoch 335 iteration 25 with loss 0.069165885\n","Training at Epoch 335 iteration 26 with loss 0.07074345\n","Training at Epoch 335 iteration 27 with loss 0.06831826\n","Training at Epoch 335 iteration 28 with loss 0.06964343\n","Training at Epoch 335 iteration 29 with loss 0.067917734\n","Training at Epoch 335 iteration 30 with loss 0.0664801\n","Training at Epoch 336 iteration 0 with loss 0.069569506\n","Training at Epoch 336 iteration 1 with loss 0.07033451\n","Training at Epoch 336 iteration 2 with loss 0.06849034\n","Training at Epoch 336 iteration 3 with loss 0.06885174\n","Training at Epoch 336 iteration 4 with loss 0.066758394\n","Training at Epoch 336 iteration 5 with loss 0.070509315\n","Training at Epoch 336 iteration 6 with loss 0.06951709\n","Training at Epoch 336 iteration 7 with loss 0.07218302\n","Training at Epoch 336 iteration 8 with loss 0.0691886\n","Training at Epoch 336 iteration 9 with loss 0.06711268\n","Training at Epoch 336 iteration 10 with loss 0.0682501\n","Training at Epoch 336 iteration 11 with loss 0.068291\n","Training at Epoch 336 iteration 12 with loss 0.06863999\n","Training at Epoch 336 iteration 13 with loss 0.068795174\n","Training at Epoch 336 iteration 14 with loss 0.0700186\n","Training at Epoch 336 iteration 15 with loss 0.0670823\n","Training at Epoch 336 iteration 16 with loss 0.07037713\n","Training at Epoch 336 iteration 17 with loss 0.07086364\n","Training at Epoch 336 iteration 18 with loss 0.068444505\n","Training at Epoch 336 iteration 19 with loss 0.070155576\n","Training at Epoch 336 iteration 20 with loss 0.06873697\n","Training at Epoch 336 iteration 21 with loss 0.06789001\n","Training at Epoch 336 iteration 22 with loss 0.06834295\n","Training at Epoch 336 iteration 23 with loss 0.070866264\n","Training at Epoch 336 iteration 24 with loss 0.06830649\n","Training at Epoch 336 iteration 25 with loss 0.06896792\n","Training at Epoch 336 iteration 26 with loss 0.06662209\n","Training at Epoch 336 iteration 27 with loss 0.0698362\n","Training at Epoch 336 iteration 28 with loss 0.06961417\n","Training at Epoch 336 iteration 29 with loss 0.06911679\n","Training at Epoch 336 iteration 30 with loss 0.071204826\n","Training at Epoch 337 iteration 0 with loss 0.0670197\n","Training at Epoch 337 iteration 1 with loss 0.07088792\n","Training at Epoch 337 iteration 2 with loss 0.06839983\n","Training at Epoch 337 iteration 3 with loss 0.070486926\n","Training at Epoch 337 iteration 4 with loss 0.07026297\n","Training at Epoch 337 iteration 5 with loss 0.070420265\n","Training at Epoch 337 iteration 6 with loss 0.06924094\n","Training at Epoch 337 iteration 7 with loss 0.071398\n","Training at Epoch 337 iteration 8 with loss 0.06909894\n","Training at Epoch 337 iteration 9 with loss 0.067362174\n","Training at Epoch 337 iteration 10 with loss 0.07096422\n","Training at Epoch 337 iteration 11 with loss 0.07049225\n","Training at Epoch 337 iteration 12 with loss 0.06751816\n","Training at Epoch 337 iteration 13 with loss 0.06900627\n","Training at Epoch 337 iteration 14 with loss 0.068951465\n","Training at Epoch 337 iteration 15 with loss 0.07092124\n","Training at Epoch 337 iteration 16 with loss 0.06656118\n","Training at Epoch 337 iteration 17 with loss 0.07008897\n","Training at Epoch 337 iteration 18 with loss 0.06871439\n","Training at Epoch 337 iteration 19 with loss 0.06814321\n","Training at Epoch 337 iteration 20 with loss 0.067075714\n","Training at Epoch 337 iteration 21 with loss 0.06745241\n","Training at Epoch 337 iteration 22 with loss 0.06867362\n","Training at Epoch 337 iteration 23 with loss 0.069832616\n","Training at Epoch 337 iteration 24 with loss 0.06951422\n","Training at Epoch 337 iteration 25 with loss 0.06903469\n","Training at Epoch 337 iteration 26 with loss 0.06850755\n","Training at Epoch 337 iteration 27 with loss 0.066930585\n","Training at Epoch 337 iteration 28 with loss 0.068290785\n","Training at Epoch 337 iteration 29 with loss 0.07067768\n","Training at Epoch 337 iteration 30 with loss 0.068485856\n","Training at Epoch 338 iteration 0 with loss 0.06829966\n","Training at Epoch 338 iteration 1 with loss 0.068211496\n","Training at Epoch 338 iteration 2 with loss 0.06959331\n","Training at Epoch 338 iteration 3 with loss 0.068687804\n","Training at Epoch 338 iteration 4 with loss 0.07040144\n","Training at Epoch 338 iteration 5 with loss 0.066707574\n","Training at Epoch 338 iteration 6 with loss 0.06831011\n","Training at Epoch 338 iteration 7 with loss 0.06906094\n","Training at Epoch 338 iteration 8 with loss 0.0693775\n","Training at Epoch 338 iteration 9 with loss 0.06828516\n","Training at Epoch 338 iteration 10 with loss 0.06951916\n","Training at Epoch 338 iteration 11 with loss 0.06956778\n","Training at Epoch 338 iteration 12 with loss 0.07163085\n","Training at Epoch 338 iteration 13 with loss 0.0692337\n","Training at Epoch 338 iteration 14 with loss 0.066564254\n","Training at Epoch 338 iteration 15 with loss 0.067844346\n","Training at Epoch 338 iteration 16 with loss 0.07090042\n","Training at Epoch 338 iteration 17 with loss 0.06924354\n","Training at Epoch 338 iteration 18 with loss 0.06839015\n","Training at Epoch 338 iteration 19 with loss 0.071269795\n","Training at Epoch 338 iteration 20 with loss 0.06902357\n","Training at Epoch 338 iteration 21 with loss 0.06772476\n","Training at Epoch 338 iteration 22 with loss 0.07090269\n","Training at Epoch 338 iteration 23 with loss 0.06775512\n","Training at Epoch 338 iteration 24 with loss 0.07076617\n","Training at Epoch 338 iteration 25 with loss 0.06957139\n","Training at Epoch 338 iteration 26 with loss 0.06788315\n","Training at Epoch 338 iteration 27 with loss 0.070346214\n","Training at Epoch 338 iteration 28 with loss 0.070345156\n","Training at Epoch 338 iteration 29 with loss 0.068269625\n","Training at Epoch 338 iteration 30 with loss 0.068254225\n","Training at Epoch 339 iteration 0 with loss 0.06911804\n","Training at Epoch 339 iteration 1 with loss 0.072159536\n","Training at Epoch 339 iteration 2 with loss 0.06911693\n","Training at Epoch 339 iteration 3 with loss 0.06743081\n","Training at Epoch 339 iteration 4 with loss 0.07050672\n","Training at Epoch 339 iteration 5 with loss 0.069186024\n","Training at Epoch 339 iteration 6 with loss 0.06785564\n","Training at Epoch 339 iteration 7 with loss 0.06740003\n","Training at Epoch 339 iteration 8 with loss 0.066663906\n","Training at Epoch 339 iteration 9 with loss 0.069912724\n","Training at Epoch 339 iteration 10 with loss 0.06916079\n","Training at Epoch 339 iteration 11 with loss 0.07087259\n","Training at Epoch 339 iteration 12 with loss 0.06864781\n","Training at Epoch 339 iteration 13 with loss 0.06855522\n","Training at Epoch 339 iteration 14 with loss 0.06822644\n","Training at Epoch 339 iteration 15 with loss 0.069971524\n","Training at Epoch 339 iteration 16 with loss 0.06863983\n","Training at Epoch 339 iteration 17 with loss 0.06910766\n","Training at Epoch 339 iteration 18 with loss 0.069973335\n","Training at Epoch 339 iteration 19 with loss 0.06879929\n","Training at Epoch 339 iteration 20 with loss 0.06826185\n","Training at Epoch 339 iteration 21 with loss 0.06989775\n","Training at Epoch 339 iteration 22 with loss 0.06912766\n","Training at Epoch 339 iteration 23 with loss 0.06959484\n","Training at Epoch 339 iteration 24 with loss 0.06876583\n","Training at Epoch 339 iteration 25 with loss 0.06906928\n","Training at Epoch 339 iteration 26 with loss 0.07086651\n","Training at Epoch 339 iteration 27 with loss 0.06900127\n","Training at Epoch 339 iteration 28 with loss 0.06902394\n","Training at Epoch 339 iteration 29 with loss 0.068687215\n","Training at Epoch 339 iteration 30 with loss 0.06870132\n","Training at Epoch 340 iteration 0 with loss 0.06874426\n","Training at Epoch 340 iteration 1 with loss 0.070859954\n","Training at Epoch 340 iteration 2 with loss 0.068238825\n","Training at Epoch 340 iteration 3 with loss 0.06647842\n","Training at Epoch 340 iteration 4 with loss 0.06919096\n","Training at Epoch 340 iteration 5 with loss 0.06867619\n","Training at Epoch 340 iteration 6 with loss 0.07044095\n","Training at Epoch 340 iteration 7 with loss 0.070855066\n","Training at Epoch 340 iteration 8 with loss 0.06918939\n","Training at Epoch 340 iteration 9 with loss 0.06998453\n","Training at Epoch 340 iteration 10 with loss 0.06866011\n","Training at Epoch 340 iteration 11 with loss 0.06743764\n","Training at Epoch 340 iteration 12 with loss 0.06912438\n","Training at Epoch 340 iteration 13 with loss 0.06999559\n","Training at Epoch 340 iteration 14 with loss 0.069553286\n","Training at Epoch 340 iteration 15 with loss 0.06785208\n","Training at Epoch 340 iteration 16 with loss 0.067346886\n","Training at Epoch 340 iteration 17 with loss 0.067804344\n","Training at Epoch 340 iteration 18 with loss 0.069108546\n","Training at Epoch 340 iteration 19 with loss 0.06865877\n","Training at Epoch 340 iteration 20 with loss 0.0712617\n","Training at Epoch 340 iteration 21 with loss 0.06822178\n","Training at Epoch 340 iteration 22 with loss 0.06826406\n","Training at Epoch 340 iteration 23 with loss 0.069630116\n","Training at Epoch 340 iteration 24 with loss 0.068710454\n","Training at Epoch 340 iteration 25 with loss 0.06867461\n","Training at Epoch 340 iteration 26 with loss 0.0695681\n","Training at Epoch 340 iteration 27 with loss 0.0694931\n","Training at Epoch 340 iteration 28 with loss 0.07175185\n","Training at Epoch 340 iteration 29 with loss 0.067895785\n","Training at Epoch 340 iteration 30 with loss 0.07084808\n","Training at Epoch 341 iteration 0 with loss 0.068213016\n","Training at Epoch 341 iteration 1 with loss 0.070417516\n","Training at Epoch 341 iteration 2 with loss 0.06913962\n","Training at Epoch 341 iteration 3 with loss 0.068673186\n","Training at Epoch 341 iteration 4 with loss 0.06910025\n","Training at Epoch 341 iteration 5 with loss 0.07003848\n","Training at Epoch 341 iteration 6 with loss 0.06783163\n","Training at Epoch 341 iteration 7 with loss 0.070819296\n","Training at Epoch 341 iteration 8 with loss 0.06687735\n","Training at Epoch 341 iteration 9 with loss 0.067840055\n","Training at Epoch 341 iteration 10 with loss 0.06903495\n","Training at Epoch 341 iteration 11 with loss 0.06914749\n","Training at Epoch 341 iteration 12 with loss 0.06742886\n","Training at Epoch 341 iteration 13 with loss 0.07260127\n","Training at Epoch 341 iteration 14 with loss 0.06772896\n","Training at Epoch 341 iteration 15 with loss 0.06868084\n","Training at Epoch 341 iteration 16 with loss 0.06688993\n","Training at Epoch 341 iteration 17 with loss 0.06830309\n","Training at Epoch 341 iteration 18 with loss 0.07123612\n","Training at Epoch 341 iteration 19 with loss 0.06700357\n","Training at Epoch 341 iteration 20 with loss 0.06694796\n","Training at Epoch 341 iteration 21 with loss 0.068167195\n","Training at Epoch 341 iteration 22 with loss 0.07001856\n","Training at Epoch 341 iteration 23 with loss 0.06991632\n","Training at Epoch 341 iteration 24 with loss 0.07217954\n","Training at Epoch 341 iteration 25 with loss 0.069993965\n","Training at Epoch 341 iteration 26 with loss 0.06862581\n","Training at Epoch 341 iteration 27 with loss 0.0707816\n","Training at Epoch 341 iteration 28 with loss 0.06819159\n","Training at Epoch 341 iteration 29 with loss 0.07045232\n","Training at Epoch 341 iteration 30 with loss 0.068224356\n","Training at Epoch 342 iteration 0 with loss 0.06860698\n","Training at Epoch 342 iteration 1 with loss 0.06950367\n","Training at Epoch 342 iteration 2 with loss 0.06776554\n","Training at Epoch 342 iteration 3 with loss 0.069474034\n","Training at Epoch 342 iteration 4 with loss 0.06876232\n","Training at Epoch 342 iteration 5 with loss 0.068129495\n","Training at Epoch 342 iteration 6 with loss 0.06903751\n","Training at Epoch 342 iteration 7 with loss 0.06995576\n","Training at Epoch 342 iteration 8 with loss 0.06917529\n","Training at Epoch 342 iteration 9 with loss 0.06912928\n","Training at Epoch 342 iteration 10 with loss 0.07005002\n","Training at Epoch 342 iteration 11 with loss 0.069579825\n","Training at Epoch 342 iteration 12 with loss 0.069052264\n","Training at Epoch 342 iteration 13 with loss 0.070006035\n","Training at Epoch 342 iteration 14 with loss 0.07084759\n","Training at Epoch 342 iteration 15 with loss 0.06915047\n","Training at Epoch 342 iteration 16 with loss 0.06821621\n","Training at Epoch 342 iteration 17 with loss 0.07040613\n","Training at Epoch 342 iteration 18 with loss 0.0681472\n","Training at Epoch 342 iteration 19 with loss 0.0704925\n","Training at Epoch 342 iteration 20 with loss 0.069888815\n","Training at Epoch 342 iteration 21 with loss 0.069665596\n","Training at Epoch 342 iteration 22 with loss 0.06993706\n","Training at Epoch 342 iteration 23 with loss 0.06825782\n","Training at Epoch 342 iteration 24 with loss 0.069977\n","Training at Epoch 342 iteration 25 with loss 0.067453854\n","Training at Epoch 342 iteration 26 with loss 0.06738706\n","Training at Epoch 342 iteration 27 with loss 0.070082426\n","Training at Epoch 342 iteration 28 with loss 0.068215966\n","Training at Epoch 342 iteration 29 with loss 0.06775445\n","Training at Epoch 342 iteration 30 with loss 0.068171345\n","Training at Epoch 343 iteration 0 with loss 0.0678469\n","Training at Epoch 343 iteration 1 with loss 0.06873279\n","Training at Epoch 343 iteration 2 with loss 0.0690919\n","Training at Epoch 343 iteration 3 with loss 0.069555\n","Training at Epoch 343 iteration 4 with loss 0.067853376\n","Training at Epoch 343 iteration 5 with loss 0.06839469\n","Training at Epoch 343 iteration 6 with loss 0.06786455\n","Training at Epoch 343 iteration 7 with loss 0.07041778\n","Training at Epoch 343 iteration 8 with loss 0.069177486\n","Training at Epoch 343 iteration 9 with loss 0.06827076\n","Training at Epoch 343 iteration 10 with loss 0.07085284\n","Training at Epoch 343 iteration 11 with loss 0.069248915\n","Training at Epoch 343 iteration 12 with loss 0.06868674\n","Training at Epoch 343 iteration 13 with loss 0.069548376\n","Training at Epoch 343 iteration 14 with loss 0.0696056\n","Training at Epoch 343 iteration 15 with loss 0.07042512\n","Training at Epoch 343 iteration 16 with loss 0.06913248\n","Training at Epoch 343 iteration 17 with loss 0.06786754\n","Training at Epoch 343 iteration 18 with loss 0.0698785\n","Training at Epoch 343 iteration 19 with loss 0.06952524\n","Training at Epoch 343 iteration 20 with loss 0.07069905\n","Training at Epoch 343 iteration 21 with loss 0.06872664\n","Training at Epoch 343 iteration 22 with loss 0.0703189\n","Training at Epoch 343 iteration 23 with loss 0.066666216\n","Training at Epoch 343 iteration 24 with loss 0.07020252\n","Training at Epoch 343 iteration 25 with loss 0.06835209\n","Training at Epoch 343 iteration 26 with loss 0.06836436\n","Training at Epoch 343 iteration 27 with loss 0.06791976\n","Training at Epoch 343 iteration 28 with loss 0.06939618\n","Training at Epoch 343 iteration 29 with loss 0.06870784\n","Training at Epoch 343 iteration 30 with loss 0.071391724\n","Training at Epoch 344 iteration 0 with loss 0.06954656\n","Training at Epoch 344 iteration 1 with loss 0.06921811\n","Training at Epoch 344 iteration 2 with loss 0.06792925\n","Training at Epoch 344 iteration 3 with loss 0.06934593\n","Training at Epoch 344 iteration 4 with loss 0.06815638\n","Training at Epoch 344 iteration 5 with loss 0.06901799\n","Training at Epoch 344 iteration 6 with loss 0.06781823\n","Training at Epoch 344 iteration 7 with loss 0.06828653\n","Training at Epoch 344 iteration 8 with loss 0.06999697\n","Training at Epoch 344 iteration 9 with loss 0.069123715\n","Training at Epoch 344 iteration 10 with loss 0.069313385\n","Training at Epoch 344 iteration 11 with loss 0.0697328\n","Training at Epoch 344 iteration 12 with loss 0.06909942\n","Training at Epoch 344 iteration 13 with loss 0.07037121\n","Training at Epoch 344 iteration 14 with loss 0.07006932\n","Training at Epoch 344 iteration 15 with loss 0.06845935\n","Training at Epoch 344 iteration 16 with loss 0.06738534\n","Training at Epoch 344 iteration 17 with loss 0.070790395\n","Training at Epoch 344 iteration 18 with loss 0.06866701\n","Training at Epoch 344 iteration 19 with loss 0.0695522\n","Training at Epoch 344 iteration 20 with loss 0.07036588\n","Training at Epoch 344 iteration 21 with loss 0.06781995\n","Training at Epoch 344 iteration 22 with loss 0.06951978\n","Training at Epoch 344 iteration 23 with loss 0.06847049\n","Training at Epoch 344 iteration 24 with loss 0.06696339\n","Training at Epoch 344 iteration 25 with loss 0.06872476\n","Training at Epoch 344 iteration 26 with loss 0.070690915\n","Training at Epoch 344 iteration 27 with loss 0.068394385\n","Training at Epoch 344 iteration 28 with loss 0.07029618\n","Training at Epoch 344 iteration 29 with loss 0.069593534\n","Training at Epoch 344 iteration 30 with loss 0.06839758\n","Training at Epoch 345 iteration 0 with loss 0.06773524\n","Training at Epoch 345 iteration 1 with loss 0.07039955\n","Training at Epoch 345 iteration 2 with loss 0.07139326\n","Training at Epoch 345 iteration 3 with loss 0.06990437\n","Training at Epoch 345 iteration 4 with loss 0.069121234\n","Training at Epoch 345 iteration 5 with loss 0.06959249\n","Training at Epoch 345 iteration 6 with loss 0.06910609\n","Training at Epoch 345 iteration 7 with loss 0.067345224\n","Training at Epoch 345 iteration 8 with loss 0.06863346\n","Training at Epoch 345 iteration 9 with loss 0.068947524\n","Training at Epoch 345 iteration 10 with loss 0.06911637\n","Training at Epoch 345 iteration 11 with loss 0.06668393\n","Training at Epoch 345 iteration 12 with loss 0.07038586\n","Training at Epoch 345 iteration 13 with loss 0.068852365\n","Training at Epoch 345 iteration 14 with loss 0.06654941\n","Training at Epoch 345 iteration 15 with loss 0.067765035\n","Training at Epoch 345 iteration 16 with loss 0.06820395\n","Training at Epoch 345 iteration 17 with loss 0.06996283\n","Training at Epoch 345 iteration 18 with loss 0.069044665\n","Training at Epoch 345 iteration 19 with loss 0.06995014\n","Training at Epoch 345 iteration 20 with loss 0.06783529\n","Training at Epoch 345 iteration 21 with loss 0.068910316\n","Training at Epoch 345 iteration 22 with loss 0.06942088\n","Training at Epoch 345 iteration 23 with loss 0.07024019\n","Training at Epoch 345 iteration 24 with loss 0.0680092\n","Training at Epoch 345 iteration 25 with loss 0.07197664\n","Training at Epoch 345 iteration 26 with loss 0.069112375\n","Training at Epoch 345 iteration 27 with loss 0.06789185\n","Training at Epoch 345 iteration 28 with loss 0.06870336\n","Training at Epoch 345 iteration 29 with loss 0.070446216\n","Training at Epoch 345 iteration 30 with loss 0.07022172\n","Training at Epoch 346 iteration 0 with loss 0.06963101\n","Training at Epoch 346 iteration 1 with loss 0.06782175\n","Training at Epoch 346 iteration 2 with loss 0.07023626\n","Training at Epoch 346 iteration 3 with loss 0.06804839\n","Training at Epoch 346 iteration 4 with loss 0.07039323\n","Training at Epoch 346 iteration 5 with loss 0.06953807\n","Training at Epoch 346 iteration 6 with loss 0.06664934\n","Training at Epoch 346 iteration 7 with loss 0.0685514\n","Training at Epoch 346 iteration 8 with loss 0.06825133\n","Training at Epoch 346 iteration 9 with loss 0.0694378\n","Training at Epoch 346 iteration 10 with loss 0.06874403\n","Training at Epoch 346 iteration 11 with loss 0.06807481\n","Training at Epoch 346 iteration 12 with loss 0.06800359\n","Training at Epoch 346 iteration 13 with loss 0.06908723\n","Training at Epoch 346 iteration 14 with loss 0.06960521\n","Training at Epoch 346 iteration 15 with loss 0.06780314\n","Training at Epoch 346 iteration 16 with loss 0.0694391\n","Training at Epoch 346 iteration 17 with loss 0.0667299\n","Training at Epoch 346 iteration 18 with loss 0.067481525\n","Training at Epoch 346 iteration 19 with loss 0.06946451\n","Training at Epoch 346 iteration 20 with loss 0.06982799\n","Training at Epoch 346 iteration 21 with loss 0.07055718\n","Training at Epoch 346 iteration 22 with loss 0.069360346\n","Training at Epoch 346 iteration 23 with loss 0.06838413\n","Training at Epoch 346 iteration 24 with loss 0.070218526\n","Training at Epoch 346 iteration 25 with loss 0.07073599\n","Training at Epoch 346 iteration 26 with loss 0.069012545\n","Training at Epoch 346 iteration 27 with loss 0.069619134\n","Training at Epoch 346 iteration 28 with loss 0.06940083\n","Training at Epoch 346 iteration 29 with loss 0.06938539\n","Training at Epoch 346 iteration 30 with loss 0.07185077\n","Training at Epoch 347 iteration 0 with loss 0.06945492\n","Training at Epoch 347 iteration 1 with loss 0.06822378\n","Training at Epoch 347 iteration 2 with loss 0.07053585\n","Training at Epoch 347 iteration 3 with loss 0.06958918\n","Training at Epoch 347 iteration 4 with loss 0.069521464\n","Training at Epoch 347 iteration 5 with loss 0.06949057\n","Training at Epoch 347 iteration 6 with loss 0.069959566\n","Training at Epoch 347 iteration 7 with loss 0.068999864\n","Training at Epoch 347 iteration 8 with loss 0.069185875\n","Training at Epoch 347 iteration 9 with loss 0.069743484\n","Training at Epoch 347 iteration 10 with loss 0.06900157\n","Training at Epoch 347 iteration 11 with loss 0.06790763\n","Training at Epoch 347 iteration 12 with loss 0.06889664\n","Training at Epoch 347 iteration 13 with loss 0.06975716\n","Training at Epoch 347 iteration 14 with loss 0.06805801\n","Training at Epoch 347 iteration 15 with loss 0.06921677\n","Training at Epoch 347 iteration 16 with loss 0.068431064\n","Training at Epoch 347 iteration 17 with loss 0.06849249\n","Training at Epoch 347 iteration 18 with loss 0.06773995\n","Training at Epoch 347 iteration 19 with loss 0.069595166\n","Training at Epoch 347 iteration 20 with loss 0.067202754\n","Training at Epoch 347 iteration 21 with loss 0.068623744\n","Training at Epoch 347 iteration 22 with loss 0.069036655\n","Training at Epoch 347 iteration 23 with loss 0.06995345\n","Training at Epoch 347 iteration 24 with loss 0.06776871\n","Training at Epoch 347 iteration 25 with loss 0.07059047\n","Training at Epoch 347 iteration 26 with loss 0.06950573\n","Training at Epoch 347 iteration 27 with loss 0.069490865\n","Training at Epoch 347 iteration 28 with loss 0.07035538\n","Training at Epoch 347 iteration 29 with loss 0.06942352\n","Training at Epoch 347 iteration 30 with loss 0.06886787\n","Training at Epoch 348 iteration 0 with loss 0.06934762\n","Training at Epoch 348 iteration 1 with loss 0.06890952\n","Training at Epoch 348 iteration 2 with loss 0.06923362\n","Training at Epoch 348 iteration 3 with loss 0.069708206\n","Training at Epoch 348 iteration 4 with loss 0.069415025\n","Training at Epoch 348 iteration 5 with loss 0.06895989\n","Training at Epoch 348 iteration 6 with loss 0.06962272\n","Training at Epoch 348 iteration 7 with loss 0.068868056\n","Training at Epoch 348 iteration 8 with loss 0.06856035\n","Training at Epoch 348 iteration 9 with loss 0.071502365\n","Training at Epoch 348 iteration 10 with loss 0.068263605\n","Training at Epoch 348 iteration 11 with loss 0.069184795\n","Training at Epoch 348 iteration 12 with loss 0.06849855\n","Training at Epoch 348 iteration 13 with loss 0.06705056\n","Training at Epoch 348 iteration 14 with loss 0.06819319\n","Training at Epoch 348 iteration 15 with loss 0.06704952\n","Training at Epoch 348 iteration 16 with loss 0.06843917\n","Training at Epoch 348 iteration 17 with loss 0.066769436\n","Training at Epoch 348 iteration 18 with loss 0.06844981\n","Training at Epoch 348 iteration 19 with loss 0.06885848\n","Training at Epoch 348 iteration 20 with loss 0.06945642\n","Training at Epoch 348 iteration 21 with loss 0.070975095\n","Training at Epoch 348 iteration 22 with loss 0.07086612\n","Training at Epoch 348 iteration 23 with loss 0.06936078\n","Training at Epoch 348 iteration 24 with loss 0.06807612\n","Training at Epoch 348 iteration 25 with loss 0.07019828\n","Training at Epoch 348 iteration 26 with loss 0.06811784\n","Training at Epoch 348 iteration 27 with loss 0.070080474\n","Training at Epoch 348 iteration 28 with loss 0.06972022\n","Training at Epoch 348 iteration 29 with loss 0.07215436\n","Training at Epoch 348 iteration 30 with loss 0.069282606\n","Training at Epoch 349 iteration 0 with loss 0.0695147\n","Training at Epoch 349 iteration 1 with loss 0.07059871\n","Training at Epoch 349 iteration 2 with loss 0.07003407\n","Training at Epoch 349 iteration 3 with loss 0.06811561\n","Training at Epoch 349 iteration 4 with loss 0.06824651\n","Training at Epoch 349 iteration 5 with loss 0.06966063\n","Training at Epoch 349 iteration 6 with loss 0.06927244\n","Training at Epoch 349 iteration 7 with loss 0.0687026\n","Training at Epoch 349 iteration 8 with loss 0.06932558\n","Training at Epoch 349 iteration 9 with loss 0.06828036\n","Training at Epoch 349 iteration 10 with loss 0.069566\n","Training at Epoch 349 iteration 11 with loss 0.07081152\n","Training at Epoch 349 iteration 12 with loss 0.07067405\n","Training at Epoch 349 iteration 13 with loss 0.067162946\n","Training at Epoch 349 iteration 14 with loss 0.06752239\n","Training at Epoch 349 iteration 15 with loss 0.07003282\n","Training at Epoch 349 iteration 16 with loss 0.07001546\n","Training at Epoch 349 iteration 17 with loss 0.071537614\n","Training at Epoch 349 iteration 18 with loss 0.06889234\n","Training at Epoch 349 iteration 19 with loss 0.06982915\n","Training at Epoch 349 iteration 20 with loss 0.06774254\n","Training at Epoch 349 iteration 21 with loss 0.069458134\n","Training at Epoch 349 iteration 22 with loss 0.06807224\n","Training at Epoch 349 iteration 23 with loss 0.06734021\n","Training at Epoch 349 iteration 24 with loss 0.068773374\n","Training at Epoch 349 iteration 25 with loss 0.07013051\n","Training at Epoch 349 iteration 26 with loss 0.068457775\n","Training at Epoch 349 iteration 27 with loss 0.069549605\n","Training at Epoch 349 iteration 28 with loss 0.06915378\n","Training at Epoch 349 iteration 29 with loss 0.068978816\n","Training at Epoch 349 iteration 30 with loss 0.069048956\n","Training at Epoch 350 iteration 0 with loss 0.06895163\n","Training at Epoch 350 iteration 1 with loss 0.0683027\n","Training at Epoch 350 iteration 2 with loss 0.06917278\n","Training at Epoch 350 iteration 3 with loss 0.06843826\n","Training at Epoch 350 iteration 4 with loss 0.06972677\n","Training at Epoch 350 iteration 5 with loss 0.070847005\n","Training at Epoch 350 iteration 6 with loss 0.0690104\n","Training at Epoch 350 iteration 7 with loss 0.06981041\n","Training at Epoch 350 iteration 8 with loss 0.06795082\n","Training at Epoch 350 iteration 9 with loss 0.06869302\n","Training at Epoch 350 iteration 10 with loss 0.06688441\n","Training at Epoch 350 iteration 11 with loss 0.06865727\n","Training at Epoch 350 iteration 12 with loss 0.07107051\n","Training at Epoch 350 iteration 13 with loss 0.06821442\n","Training at Epoch 350 iteration 14 with loss 0.07012297\n","Training at Epoch 350 iteration 15 with loss 0.070956245\n","Training at Epoch 350 iteration 16 with loss 0.06985868\n","Training at Epoch 350 iteration 17 with loss 0.06866019\n","Training at Epoch 350 iteration 18 with loss 0.06946229\n","Training at Epoch 350 iteration 19 with loss 0.06881884\n","Training at Epoch 350 iteration 20 with loss 0.06861077\n","Training at Epoch 350 iteration 21 with loss 0.06859288\n","Training at Epoch 350 iteration 22 with loss 0.06909037\n","Training at Epoch 350 iteration 23 with loss 0.06860075\n","Training at Epoch 350 iteration 24 with loss 0.06830984\n","Training at Epoch 350 iteration 25 with loss 0.068371214\n","Training at Epoch 350 iteration 26 with loss 0.0684403\n","Training at Epoch 350 iteration 27 with loss 0.06909588\n","Training at Epoch 350 iteration 28 with loss 0.07039344\n","Training at Epoch 350 iteration 29 with loss 0.069176696\n","Training at Epoch 350 iteration 30 with loss 0.0690963\n","Training at Epoch 351 iteration 0 with loss 0.068603404\n","Training at Epoch 351 iteration 1 with loss 0.06956905\n","Training at Epoch 351 iteration 2 with loss 0.06957404\n","Training at Epoch 351 iteration 3 with loss 0.07063196\n","Training at Epoch 351 iteration 4 with loss 0.072003424\n","Training at Epoch 351 iteration 5 with loss 0.07030518\n","Training at Epoch 351 iteration 6 with loss 0.069565505\n","Training at Epoch 351 iteration 7 with loss 0.06845009\n","Training at Epoch 351 iteration 8 with loss 0.06888162\n","Training at Epoch 351 iteration 9 with loss 0.06913675\n","Training at Epoch 351 iteration 10 with loss 0.06894654\n","Training at Epoch 351 iteration 11 with loss 0.067045555\n","Training at Epoch 351 iteration 12 with loss 0.06814653\n","Training at Epoch 351 iteration 13 with loss 0.06717045\n","Training at Epoch 351 iteration 14 with loss 0.06884026\n","Training at Epoch 351 iteration 15 with loss 0.0679529\n","Training at Epoch 351 iteration 16 with loss 0.06910149\n","Training at Epoch 351 iteration 17 with loss 0.069194056\n","Training at Epoch 351 iteration 18 with loss 0.0700856\n","Training at Epoch 351 iteration 19 with loss 0.06714421\n","Training at Epoch 351 iteration 20 with loss 0.071575895\n","Training at Epoch 351 iteration 21 with loss 0.06749563\n","Training at Epoch 351 iteration 22 with loss 0.070830174\n","Training at Epoch 351 iteration 23 with loss 0.070422485\n","Training at Epoch 351 iteration 24 with loss 0.067464866\n","Training at Epoch 351 iteration 25 with loss 0.06861883\n","Training at Epoch 351 iteration 26 with loss 0.06943202\n","Training at Epoch 351 iteration 27 with loss 0.06911324\n","Training at Epoch 351 iteration 28 with loss 0.068652436\n","Training at Epoch 351 iteration 29 with loss 0.069708124\n","Training at Epoch 351 iteration 30 with loss 0.069177054\n","Training at Epoch 352 iteration 0 with loss 0.067887306\n","Training at Epoch 352 iteration 1 with loss 0.06948443\n","Training at Epoch 352 iteration 2 with loss 0.06747152\n","Training at Epoch 352 iteration 3 with loss 0.06864311\n","Training at Epoch 352 iteration 4 with loss 0.06776859\n","Training at Epoch 352 iteration 5 with loss 0.06997112\n","Training at Epoch 352 iteration 6 with loss 0.06660792\n","Training at Epoch 352 iteration 7 with loss 0.06943534\n","Training at Epoch 352 iteration 8 with loss 0.067490645\n","Training at Epoch 352 iteration 9 with loss 0.069888934\n","Training at Epoch 352 iteration 10 with loss 0.06751333\n","Training at Epoch 352 iteration 11 with loss 0.070852146\n","Training at Epoch 352 iteration 12 with loss 0.069566\n","Training at Epoch 352 iteration 13 with loss 0.07199627\n","Training at Epoch 352 iteration 14 with loss 0.06957273\n","Training at Epoch 352 iteration 15 with loss 0.069218725\n","Training at Epoch 352 iteration 16 with loss 0.069106\n","Training at Epoch 352 iteration 17 with loss 0.06926165\n","Training at Epoch 352 iteration 18 with loss 0.06828241\n","Training at Epoch 352 iteration 19 with loss 0.067937315\n","Training at Epoch 352 iteration 20 with loss 0.07082491\n","Training at Epoch 352 iteration 21 with loss 0.06903984\n","Training at Epoch 352 iteration 22 with loss 0.067897305\n","Training at Epoch 352 iteration 23 with loss 0.06960656\n","Training at Epoch 352 iteration 24 with loss 0.07027552\n","Training at Epoch 352 iteration 25 with loss 0.071355306\n","Training at Epoch 352 iteration 26 with loss 0.068288356\n","Training at Epoch 352 iteration 27 with loss 0.06920655\n","Training at Epoch 352 iteration 28 with loss 0.069585934\n","Training at Epoch 352 iteration 29 with loss 0.06926242\n","Training at Epoch 352 iteration 30 with loss 0.06912454\n","Training at Epoch 353 iteration 0 with loss 0.07249515\n","Training at Epoch 353 iteration 1 with loss 0.069193736\n","Training at Epoch 353 iteration 2 with loss 0.06615838\n","Training at Epoch 353 iteration 3 with loss 0.07044157\n","Training at Epoch 353 iteration 4 with loss 0.06988442\n","Training at Epoch 353 iteration 5 with loss 0.06783788\n","Training at Epoch 353 iteration 6 with loss 0.06778609\n","Training at Epoch 353 iteration 7 with loss 0.06902955\n","Training at Epoch 353 iteration 8 with loss 0.068224385\n","Training at Epoch 353 iteration 9 with loss 0.07004324\n","Training at Epoch 353 iteration 10 with loss 0.06921101\n","Training at Epoch 353 iteration 11 with loss 0.06941601\n","Training at Epoch 353 iteration 12 with loss 0.0686705\n","Training at Epoch 353 iteration 13 with loss 0.06827777\n","Training at Epoch 353 iteration 14 with loss 0.069671296\n","Training at Epoch 353 iteration 15 with loss 0.06777344\n","Training at Epoch 353 iteration 16 with loss 0.06867738\n","Training at Epoch 353 iteration 17 with loss 0.06908816\n","Training at Epoch 353 iteration 18 with loss 0.06824854\n","Training at Epoch 353 iteration 19 with loss 0.0681998\n","Training at Epoch 353 iteration 20 with loss 0.06776084\n","Training at Epoch 353 iteration 21 with loss 0.07167407\n","Training at Epoch 353 iteration 22 with loss 0.068222895\n","Training at Epoch 353 iteration 23 with loss 0.068711236\n","Training at Epoch 353 iteration 24 with loss 0.07045312\n","Training at Epoch 353 iteration 25 with loss 0.06785694\n","Training at Epoch 353 iteration 26 with loss 0.06780858\n","Training at Epoch 353 iteration 27 with loss 0.06913792\n","Training at Epoch 353 iteration 28 with loss 0.07123536\n","Training at Epoch 353 iteration 29 with loss 0.07086458\n","Training at Epoch 353 iteration 30 with loss 0.06867373\n","Training at Epoch 354 iteration 0 with loss 0.07117445\n","Training at Epoch 354 iteration 1 with loss 0.06867694\n","Training at Epoch 354 iteration 2 with loss 0.06832716\n","Training at Epoch 354 iteration 3 with loss 0.07032846\n","Training at Epoch 354 iteration 4 with loss 0.07132572\n","Training at Epoch 354 iteration 5 with loss 0.06869818\n","Training at Epoch 354 iteration 6 with loss 0.067805\n","Training at Epoch 354 iteration 7 with loss 0.06826459\n","Training at Epoch 354 iteration 8 with loss 0.06920919\n","Training at Epoch 354 iteration 9 with loss 0.069067135\n","Training at Epoch 354 iteration 10 with loss 0.06823681\n","Training at Epoch 354 iteration 11 with loss 0.06745763\n","Training at Epoch 354 iteration 12 with loss 0.06871657\n","Training at Epoch 354 iteration 13 with loss 0.070395246\n","Training at Epoch 354 iteration 14 with loss 0.068663165\n","Training at Epoch 354 iteration 15 with loss 0.07000522\n","Training at Epoch 354 iteration 16 with loss 0.0687074\n","Training at Epoch 354 iteration 17 with loss 0.06781168\n","Training at Epoch 354 iteration 18 with loss 0.068289496\n","Training at Epoch 354 iteration 19 with loss 0.06998392\n","Training at Epoch 354 iteration 20 with loss 0.0682467\n","Training at Epoch 354 iteration 21 with loss 0.06823095\n","Training at Epoch 354 iteration 22 with loss 0.06824982\n","Training at Epoch 354 iteration 23 with loss 0.06871489\n","Training at Epoch 354 iteration 24 with loss 0.0691008\n","Training at Epoch 354 iteration 25 with loss 0.07037379\n","Training at Epoch 354 iteration 26 with loss 0.06988218\n","Training at Epoch 354 iteration 27 with loss 0.069557965\n","Training at Epoch 354 iteration 28 with loss 0.06785226\n","Training at Epoch 354 iteration 29 with loss 0.07036104\n","Training at Epoch 354 iteration 30 with loss 0.06920048\n","Training at Epoch 355 iteration 0 with loss 0.07084523\n","Training at Epoch 355 iteration 1 with loss 0.06902105\n","Training at Epoch 355 iteration 2 with loss 0.071320474\n","Training at Epoch 355 iteration 3 with loss 0.06741486\n","Training at Epoch 355 iteration 4 with loss 0.069171526\n","Training at Epoch 355 iteration 5 with loss 0.07050785\n","Training at Epoch 355 iteration 6 with loss 0.069126114\n","Training at Epoch 355 iteration 7 with loss 0.069929115\n","Training at Epoch 355 iteration 8 with loss 0.06654659\n","Training at Epoch 355 iteration 9 with loss 0.06953044\n","Training at Epoch 355 iteration 10 with loss 0.071216896\n","Training at Epoch 355 iteration 11 with loss 0.06993234\n","Training at Epoch 355 iteration 12 with loss 0.06967563\n","Training at Epoch 355 iteration 13 with loss 0.06658187\n","Training at Epoch 355 iteration 14 with loss 0.07041597\n","Training at Epoch 355 iteration 15 with loss 0.06659934\n","Training at Epoch 355 iteration 16 with loss 0.06740851\n","Training at Epoch 355 iteration 17 with loss 0.06867791\n","Training at Epoch 355 iteration 18 with loss 0.06782134\n","Training at Epoch 355 iteration 19 with loss 0.06956647\n","Training at Epoch 355 iteration 20 with loss 0.06994601\n","Training at Epoch 355 iteration 21 with loss 0.06955522\n","Training at Epoch 355 iteration 22 with loss 0.06918246\n","Training at Epoch 355 iteration 23 with loss 0.06872724\n","Training at Epoch 355 iteration 24 with loss 0.06863678\n","Training at Epoch 355 iteration 25 with loss 0.06869869\n","Training at Epoch 355 iteration 26 with loss 0.0682377\n","Training at Epoch 355 iteration 27 with loss 0.068285555\n","Training at Epoch 355 iteration 28 with loss 0.06960037\n","Training at Epoch 355 iteration 29 with loss 0.07094794\n","Training at Epoch 355 iteration 30 with loss 0.068271145\n","Training at Epoch 356 iteration 0 with loss 0.068264045\n","Training at Epoch 356 iteration 1 with loss 0.069548994\n","Training at Epoch 356 iteration 2 with loss 0.06823518\n","Training at Epoch 356 iteration 3 with loss 0.071335174\n","Training at Epoch 356 iteration 4 with loss 0.06912799\n","Training at Epoch 356 iteration 5 with loss 0.06906214\n","Training at Epoch 356 iteration 6 with loss 0.07037296\n","Training at Epoch 356 iteration 7 with loss 0.07001038\n","Training at Epoch 356 iteration 8 with loss 0.06746034\n","Training at Epoch 356 iteration 9 with loss 0.068266824\n","Training at Epoch 356 iteration 10 with loss 0.0687027\n","Training at Epoch 356 iteration 11 with loss 0.06873059\n","Training at Epoch 356 iteration 12 with loss 0.06736966\n","Training at Epoch 356 iteration 13 with loss 0.068746045\n","Training at Epoch 356 iteration 14 with loss 0.06961503\n","Training at Epoch 356 iteration 15 with loss 0.070405826\n","Training at Epoch 356 iteration 16 with loss 0.069073275\n","Training at Epoch 356 iteration 17 with loss 0.06781406\n","Training at Epoch 356 iteration 18 with loss 0.07082441\n","Training at Epoch 356 iteration 19 with loss 0.06689606\n","Training at Epoch 356 iteration 20 with loss 0.069601595\n","Training at Epoch 356 iteration 21 with loss 0.06819864\n","Training at Epoch 356 iteration 22 with loss 0.06870051\n","Training at Epoch 356 iteration 23 with loss 0.06998873\n","Training at Epoch 356 iteration 24 with loss 0.072574526\n","Training at Epoch 356 iteration 25 with loss 0.06867663\n","Training at Epoch 356 iteration 26 with loss 0.0695323\n","Training at Epoch 356 iteration 27 with loss 0.06912838\n","Training at Epoch 356 iteration 28 with loss 0.06959189\n","Training at Epoch 356 iteration 29 with loss 0.06738205\n","Training at Epoch 356 iteration 30 with loss 0.068619415\n","Training at Epoch 357 iteration 0 with loss 0.07086307\n","Training at Epoch 357 iteration 1 with loss 0.069582894\n","Training at Epoch 357 iteration 2 with loss 0.07135251\n","Training at Epoch 357 iteration 3 with loss 0.07000666\n","Training at Epoch 357 iteration 4 with loss 0.06953788\n","Training at Epoch 357 iteration 5 with loss 0.06829245\n","Training at Epoch 357 iteration 6 with loss 0.070807025\n","Training at Epoch 357 iteration 7 with loss 0.068743385\n","Training at Epoch 357 iteration 8 with loss 0.06956454\n","Training at Epoch 357 iteration 9 with loss 0.0686914\n","Training at Epoch 357 iteration 10 with loss 0.06873348\n","Training at Epoch 357 iteration 11 with loss 0.069174536\n","Training at Epoch 357 iteration 12 with loss 0.06744753\n","Training at Epoch 357 iteration 13 with loss 0.068217956\n","Training at Epoch 357 iteration 14 with loss 0.068709925\n","Training at Epoch 357 iteration 15 with loss 0.06663443\n","Training at Epoch 357 iteration 16 with loss 0.069132864\n","Training at Epoch 357 iteration 17 with loss 0.06739716\n","Training at Epoch 357 iteration 18 with loss 0.06785636\n","Training at Epoch 357 iteration 19 with loss 0.06956716\n","Training at Epoch 357 iteration 20 with loss 0.06911682\n","Training at Epoch 357 iteration 21 with loss 0.06998575\n","Training at Epoch 357 iteration 22 with loss 0.06783259\n","Training at Epoch 357 iteration 23 with loss 0.0695277\n","Training at Epoch 357 iteration 24 with loss 0.069577426\n","Training at Epoch 357 iteration 25 with loss 0.06781886\n","Training at Epoch 357 iteration 26 with loss 0.069990754\n","Training at Epoch 357 iteration 27 with loss 0.06906437\n","Training at Epoch 357 iteration 28 with loss 0.06994784\n","Training at Epoch 357 iteration 29 with loss 0.069116935\n","Training at Epoch 357 iteration 30 with loss 0.070043474\n","Training at Epoch 358 iteration 0 with loss 0.06828294\n","Training at Epoch 358 iteration 1 with loss 0.06827058\n","Training at Epoch 358 iteration 2 with loss 0.0691146\n","Training at Epoch 358 iteration 3 with loss 0.07041158\n","Training at Epoch 358 iteration 4 with loss 0.07127157\n","Training at Epoch 358 iteration 5 with loss 0.0709043\n","Training at Epoch 358 iteration 6 with loss 0.0700532\n","Training at Epoch 358 iteration 7 with loss 0.06875591\n","Training at Epoch 358 iteration 8 with loss 0.06953772\n","Training at Epoch 358 iteration 9 with loss 0.068253316\n","Training at Epoch 358 iteration 10 with loss 0.06958766\n","Training at Epoch 358 iteration 11 with loss 0.06916982\n","Training at Epoch 358 iteration 12 with loss 0.06781985\n","Training at Epoch 358 iteration 13 with loss 0.069985256\n","Training at Epoch 358 iteration 14 with loss 0.06866813\n","Training at Epoch 358 iteration 15 with loss 0.06954557\n","Training at Epoch 358 iteration 16 with loss 0.06996435\n","Training at Epoch 358 iteration 17 with loss 0.069939606\n","Training at Epoch 358 iteration 18 with loss 0.06919084\n","Training at Epoch 358 iteration 19 with loss 0.06825201\n","Training at Epoch 358 iteration 20 with loss 0.06698631\n","Training at Epoch 358 iteration 21 with loss 0.06874843\n","Training at Epoch 358 iteration 22 with loss 0.068304546\n","Training at Epoch 358 iteration 23 with loss 0.0712324\n","Training at Epoch 358 iteration 24 with loss 0.06823703\n","Training at Epoch 358 iteration 25 with loss 0.06697149\n","Training at Epoch 358 iteration 26 with loss 0.07002638\n","Training at Epoch 358 iteration 27 with loss 0.06867684\n","Training at Epoch 358 iteration 28 with loss 0.06909469\n","Training at Epoch 358 iteration 29 with loss 0.067869745\n","Training at Epoch 358 iteration 30 with loss 0.068693005\n","Training at Epoch 359 iteration 0 with loss 0.06869502\n","Training at Epoch 359 iteration 1 with loss 0.07121889\n","Training at Epoch 359 iteration 2 with loss 0.070815645\n","Training at Epoch 359 iteration 3 with loss 0.068697736\n","Training at Epoch 359 iteration 4 with loss 0.06618312\n","Training at Epoch 359 iteration 5 with loss 0.06998892\n","Training at Epoch 359 iteration 6 with loss 0.06911884\n","Training at Epoch 359 iteration 7 with loss 0.06998422\n","Training at Epoch 359 iteration 8 with loss 0.0674324\n","Training at Epoch 359 iteration 9 with loss 0.068692744\n","Training at Epoch 359 iteration 10 with loss 0.07040683\n","Training at Epoch 359 iteration 11 with loss 0.06947609\n","Training at Epoch 359 iteration 12 with loss 0.06995222\n","Training at Epoch 359 iteration 13 with loss 0.06867962\n","Training at Epoch 359 iteration 14 with loss 0.06958036\n","Training at Epoch 359 iteration 15 with loss 0.06870653\n","Training at Epoch 359 iteration 16 with loss 0.06656848\n","Training at Epoch 359 iteration 17 with loss 0.06957569\n","Training at Epoch 359 iteration 18 with loss 0.068628386\n","Training at Epoch 359 iteration 19 with loss 0.071220115\n","Training at Epoch 359 iteration 20 with loss 0.06829307\n","Training at Epoch 359 iteration 21 with loss 0.06956878\n","Training at Epoch 359 iteration 22 with loss 0.06827323\n","Training at Epoch 359 iteration 23 with loss 0.06910638\n","Training at Epoch 359 iteration 24 with loss 0.06915946\n","Training at Epoch 359 iteration 25 with loss 0.06784412\n","Training at Epoch 359 iteration 26 with loss 0.069956526\n","Training at Epoch 359 iteration 27 with loss 0.0691151\n","Training at Epoch 359 iteration 28 with loss 0.06705052\n","Training at Epoch 359 iteration 29 with loss 0.069122516\n","Training at Epoch 359 iteration 30 with loss 0.07124983\n","Training at Epoch 360 iteration 0 with loss 0.07125399\n","Training at Epoch 360 iteration 1 with loss 0.07123762\n","Training at Epoch 360 iteration 2 with loss 0.06954342\n","Training at Epoch 360 iteration 3 with loss 0.06994749\n","Training at Epoch 360 iteration 4 with loss 0.06827915\n","Training at Epoch 360 iteration 5 with loss 0.06949438\n","Training at Epoch 360 iteration 6 with loss 0.06955806\n","Training at Epoch 360 iteration 7 with loss 0.06909925\n","Training at Epoch 360 iteration 8 with loss 0.07121923\n","Training at Epoch 360 iteration 9 with loss 0.06697525\n","Training at Epoch 360 iteration 10 with loss 0.06994605\n","Training at Epoch 360 iteration 11 with loss 0.06529246\n","Training at Epoch 360 iteration 12 with loss 0.070372716\n","Training at Epoch 360 iteration 13 with loss 0.07207973\n","Training at Epoch 360 iteration 14 with loss 0.06658137\n","Training at Epoch 360 iteration 15 with loss 0.068704024\n","Training at Epoch 360 iteration 16 with loss 0.0674305\n","Training at Epoch 360 iteration 17 with loss 0.06995188\n","Training at Epoch 360 iteration 18 with loss 0.067858644\n","Training at Epoch 360 iteration 19 with loss 0.06954388\n","Training at Epoch 360 iteration 20 with loss 0.0687141\n","Training at Epoch 360 iteration 21 with loss 0.068650536\n","Training at Epoch 360 iteration 22 with loss 0.06743597\n","Training at Epoch 360 iteration 23 with loss 0.068682134\n","Training at Epoch 360 iteration 24 with loss 0.06830305\n","Training at Epoch 360 iteration 25 with loss 0.06869991\n","Training at Epoch 360 iteration 26 with loss 0.06951026\n","Training at Epoch 360 iteration 27 with loss 0.068689175\n","Training at Epoch 360 iteration 28 with loss 0.068678215\n","Training at Epoch 360 iteration 29 with loss 0.06914367\n","Training at Epoch 360 iteration 30 with loss 0.07037899\n","Training at Epoch 361 iteration 0 with loss 0.06744733\n","Training at Epoch 361 iteration 1 with loss 0.07036034\n","Training at Epoch 361 iteration 2 with loss 0.06786665\n","Training at Epoch 361 iteration 3 with loss 0.06866385\n","Training at Epoch 361 iteration 4 with loss 0.069078475\n","Training at Epoch 361 iteration 5 with loss 0.067057125\n","Training at Epoch 361 iteration 6 with loss 0.06786109\n","Training at Epoch 361 iteration 7 with loss 0.07042787\n","Training at Epoch 361 iteration 8 with loss 0.069978416\n","Training at Epoch 361 iteration 9 with loss 0.06989367\n","Training at Epoch 361 iteration 10 with loss 0.069947675\n","Training at Epoch 361 iteration 11 with loss 0.07124378\n","Training at Epoch 361 iteration 12 with loss 0.06787336\n","Training at Epoch 361 iteration 13 with loss 0.068712324\n","Training at Epoch 361 iteration 14 with loss 0.070374034\n","Training at Epoch 361 iteration 15 with loss 0.06997146\n","Training at Epoch 361 iteration 16 with loss 0.067449495\n","Training at Epoch 361 iteration 17 with loss 0.06828295\n","Training at Epoch 361 iteration 18 with loss 0.07081771\n","Training at Epoch 361 iteration 19 with loss 0.07168062\n","Training at Epoch 361 iteration 20 with loss 0.069139615\n","Training at Epoch 361 iteration 21 with loss 0.066227846\n","Training at Epoch 361 iteration 22 with loss 0.06949977\n","Training at Epoch 361 iteration 23 with loss 0.07042472\n","Training at Epoch 361 iteration 24 with loss 0.06996952\n","Training at Epoch 361 iteration 25 with loss 0.06829446\n","Training at Epoch 361 iteration 26 with loss 0.068710186\n","Training at Epoch 361 iteration 27 with loss 0.06953406\n","Training at Epoch 361 iteration 28 with loss 0.06864167\n","Training at Epoch 361 iteration 29 with loss 0.067479745\n","Training at Epoch 361 iteration 30 with loss 0.06870283\n","Training at Epoch 362 iteration 0 with loss 0.07040069\n","Training at Epoch 362 iteration 1 with loss 0.06784869\n","Training at Epoch 362 iteration 2 with loss 0.070366144\n","Training at Epoch 362 iteration 3 with loss 0.069991544\n","Training at Epoch 362 iteration 4 with loss 0.068268955\n","Training at Epoch 362 iteration 5 with loss 0.069538966\n","Training at Epoch 362 iteration 6 with loss 0.06784762\n","Training at Epoch 362 iteration 7 with loss 0.07038166\n","Training at Epoch 362 iteration 8 with loss 0.070491806\n","Training at Epoch 362 iteration 9 with loss 0.06785603\n","Training at Epoch 362 iteration 10 with loss 0.07079513\n","Training at Epoch 362 iteration 11 with loss 0.068277836\n","Training at Epoch 362 iteration 12 with loss 0.069108285\n","Training at Epoch 362 iteration 13 with loss 0.068297505\n","Training at Epoch 362 iteration 14 with loss 0.06744335\n","Training at Epoch 362 iteration 15 with loss 0.067444906\n","Training at Epoch 362 iteration 16 with loss 0.07168995\n","Training at Epoch 362 iteration 17 with loss 0.0703717\n","Training at Epoch 362 iteration 18 with loss 0.069147244\n","Training at Epoch 362 iteration 19 with loss 0.06746815\n","Training at Epoch 362 iteration 20 with loss 0.069861904\n","Training at Epoch 362 iteration 21 with loss 0.06869856\n","Training at Epoch 362 iteration 22 with loss 0.069097854\n","Training at Epoch 362 iteration 23 with loss 0.067517795\n","Training at Epoch 362 iteration 24 with loss 0.06994348\n","Training at Epoch 362 iteration 25 with loss 0.06953092\n","Training at Epoch 362 iteration 26 with loss 0.06832377\n","Training at Epoch 362 iteration 27 with loss 0.06825314\n","Training at Epoch 362 iteration 28 with loss 0.06907192\n","Training at Epoch 362 iteration 29 with loss 0.071583696\n","Training at Epoch 362 iteration 30 with loss 0.067899324\n","Training at Epoch 363 iteration 0 with loss 0.07079136\n","Training at Epoch 363 iteration 1 with loss 0.06740187\n","Training at Epoch 363 iteration 2 with loss 0.06992878\n","Training at Epoch 363 iteration 3 with loss 0.06960534\n","Training at Epoch 363 iteration 4 with loss 0.068674825\n","Training at Epoch 363 iteration 5 with loss 0.06998765\n","Training at Epoch 363 iteration 6 with loss 0.069150105\n","Training at Epoch 363 iteration 7 with loss 0.06914802\n","Training at Epoch 363 iteration 8 with loss 0.06914701\n","Training at Epoch 363 iteration 9 with loss 0.06783857\n","Training at Epoch 363 iteration 10 with loss 0.069083355\n","Training at Epoch 363 iteration 11 with loss 0.068739615\n","Training at Epoch 363 iteration 12 with loss 0.068225905\n","Training at Epoch 363 iteration 13 with loss 0.069916174\n","Training at Epoch 363 iteration 14 with loss 0.067474544\n","Training at Epoch 363 iteration 15 with loss 0.06961485\n","Training at Epoch 363 iteration 16 with loss 0.06953772\n","Training at Epoch 363 iteration 17 with loss 0.06870976\n","Training at Epoch 363 iteration 18 with loss 0.06958531\n","Training at Epoch 363 iteration 19 with loss 0.069928\n","Training at Epoch 363 iteration 20 with loss 0.06874075\n","Training at Epoch 363 iteration 21 with loss 0.069523476\n","Training at Epoch 363 iteration 22 with loss 0.06995001\n","Training at Epoch 363 iteration 23 with loss 0.068741664\n","Training at Epoch 363 iteration 24 with loss 0.06824152\n","Training at Epoch 363 iteration 25 with loss 0.06913059\n","Training at Epoch 363 iteration 26 with loss 0.06787394\n","Training at Epoch 363 iteration 27 with loss 0.069475956\n","Training at Epoch 363 iteration 28 with loss 0.069550395\n","Training at Epoch 363 iteration 29 with loss 0.068687394\n","Training at Epoch 363 iteration 30 with loss 0.07000108\n","Training at Epoch 364 iteration 0 with loss 0.06950223\n","Training at Epoch 364 iteration 1 with loss 0.06871833\n","Training at Epoch 364 iteration 2 with loss 0.06911413\n","Training at Epoch 364 iteration 3 with loss 0.06792944\n","Training at Epoch 364 iteration 4 with loss 0.06834073\n","Training at Epoch 364 iteration 5 with loss 0.06872267\n","Training at Epoch 364 iteration 6 with loss 0.07117959\n","Training at Epoch 364 iteration 7 with loss 0.067415155\n","Training at Epoch 364 iteration 8 with loss 0.07036045\n","Training at Epoch 364 iteration 9 with loss 0.06787187\n","Training at Epoch 364 iteration 10 with loss 0.06909194\n","Training at Epoch 364 iteration 11 with loss 0.070416845\n","Training at Epoch 364 iteration 12 with loss 0.06775416\n","Training at Epoch 364 iteration 13 with loss 0.069091104\n","Training at Epoch 364 iteration 14 with loss 0.06838444\n","Training at Epoch 364 iteration 15 with loss 0.06913079\n","Training at Epoch 364 iteration 16 with loss 0.0696327\n","Training at Epoch 364 iteration 17 with loss 0.069481745\n","Training at Epoch 364 iteration 18 with loss 0.068307824\n","Training at Epoch 364 iteration 19 with loss 0.06911148\n","Training at Epoch 364 iteration 20 with loss 0.06902677\n","Training at Epoch 364 iteration 21 with loss 0.06789921\n","Training at Epoch 364 iteration 22 with loss 0.06920192\n","Training at Epoch 364 iteration 23 with loss 0.07082949\n","Training at Epoch 364 iteration 24 with loss 0.07001941\n","Training at Epoch 364 iteration 25 with loss 0.069150746\n","Training at Epoch 364 iteration 26 with loss 0.06912835\n","Training at Epoch 364 iteration 27 with loss 0.06783298\n","Training at Epoch 364 iteration 28 with loss 0.069915794\n","Training at Epoch 364 iteration 29 with loss 0.06908952\n","Training at Epoch 364 iteration 30 with loss 0.07038805\n","Training at Epoch 365 iteration 0 with loss 0.0708057\n","Training at Epoch 365 iteration 1 with loss 0.068711266\n","Training at Epoch 365 iteration 2 with loss 0.06877243\n","Training at Epoch 365 iteration 3 with loss 0.07249828\n","Training at Epoch 365 iteration 4 with loss 0.06870738\n","Training at Epoch 365 iteration 5 with loss 0.0711852\n","Training at Epoch 365 iteration 6 with loss 0.067007266\n","Training at Epoch 365 iteration 7 with loss 0.069580115\n","Training at Epoch 365 iteration 8 with loss 0.06740989\n","Training at Epoch 365 iteration 9 with loss 0.06822911\n","Training at Epoch 365 iteration 10 with loss 0.07115023\n","Training at Epoch 365 iteration 11 with loss 0.0699442\n","Training at Epoch 365 iteration 12 with loss 0.06663766\n","Training at Epoch 365 iteration 13 with loss 0.069996744\n","Training at Epoch 365 iteration 14 with loss 0.07086846\n","Training at Epoch 365 iteration 15 with loss 0.06817894\n","Training at Epoch 365 iteration 16 with loss 0.06791106\n","Training at Epoch 365 iteration 17 with loss 0.06956698\n","Training at Epoch 365 iteration 18 with loss 0.06785055\n","Training at Epoch 365 iteration 19 with loss 0.06955556\n","Training at Epoch 365 iteration 20 with loss 0.06869741\n","Training at Epoch 365 iteration 21 with loss 0.069099635\n","Training at Epoch 365 iteration 22 with loss 0.069063604\n","Training at Epoch 365 iteration 23 with loss 0.07034048\n","Training at Epoch 365 iteration 24 with loss 0.06828694\n","Training at Epoch 365 iteration 25 with loss 0.06910217\n","Training at Epoch 365 iteration 26 with loss 0.06919819\n","Training at Epoch 365 iteration 27 with loss 0.0687326\n","Training at Epoch 365 iteration 28 with loss 0.06745756\n","Training at Epoch 365 iteration 29 with loss 0.06866951\n","Training at Epoch 365 iteration 30 with loss 0.06833056\n","Training at Epoch 366 iteration 0 with loss 0.070361815\n","Training at Epoch 366 iteration 1 with loss 0.06991105\n","Training at Epoch 366 iteration 2 with loss 0.06704697\n","Training at Epoch 366 iteration 3 with loss 0.06873878\n","Training at Epoch 366 iteration 4 with loss 0.07082574\n","Training at Epoch 366 iteration 5 with loss 0.069075614\n","Training at Epoch 366 iteration 6 with loss 0.06821181\n","Training at Epoch 366 iteration 7 with loss 0.068735264\n","Training at Epoch 366 iteration 8 with loss 0.069485\n","Training at Epoch 366 iteration 9 with loss 0.06951456\n","Training at Epoch 366 iteration 10 with loss 0.067896634\n","Training at Epoch 366 iteration 11 with loss 0.07028886\n","Training at Epoch 366 iteration 12 with loss 0.068697795\n","Training at Epoch 366 iteration 13 with loss 0.07031165\n","Training at Epoch 366 iteration 14 with loss 0.067849934\n","Training at Epoch 366 iteration 15 with loss 0.070392095\n","Training at Epoch 366 iteration 16 with loss 0.06909161\n","Training at Epoch 366 iteration 17 with loss 0.06955059\n","Training at Epoch 366 iteration 18 with loss 0.06744547\n","Training at Epoch 366 iteration 19 with loss 0.06999151\n","Training at Epoch 366 iteration 20 with loss 0.06914504\n","Training at Epoch 366 iteration 21 with loss 0.06952732\n","Training at Epoch 366 iteration 22 with loss 0.067537665\n","Training at Epoch 366 iteration 23 with loss 0.068623945\n","Training at Epoch 366 iteration 24 with loss 0.069143794\n","Training at Epoch 366 iteration 25 with loss 0.06909959\n","Training at Epoch 366 iteration 26 with loss 0.06876541\n","Training at Epoch 366 iteration 27 with loss 0.068307206\n","Training at Epoch 366 iteration 28 with loss 0.06784936\n","Training at Epoch 366 iteration 29 with loss 0.06911884\n","Training at Epoch 366 iteration 30 with loss 0.07120417\n","Training at Epoch 367 iteration 0 with loss 0.06875371\n","Training at Epoch 367 iteration 1 with loss 0.07034749\n","Training at Epoch 367 iteration 2 with loss 0.06954211\n","Training at Epoch 367 iteration 3 with loss 0.0691521\n","Training at Epoch 367 iteration 4 with loss 0.06947915\n","Training at Epoch 367 iteration 5 with loss 0.0696196\n","Training at Epoch 367 iteration 6 with loss 0.07082702\n","Training at Epoch 367 iteration 7 with loss 0.0691165\n","Training at Epoch 367 iteration 8 with loss 0.06832253\n","Training at Epoch 367 iteration 9 with loss 0.06872866\n","Training at Epoch 367 iteration 10 with loss 0.0708215\n","Training at Epoch 367 iteration 11 with loss 0.069592886\n","Training at Epoch 367 iteration 12 with loss 0.06624294\n","Training at Epoch 367 iteration 13 with loss 0.06871261\n","Training at Epoch 367 iteration 14 with loss 0.06945438\n","Training at Epoch 367 iteration 15 with loss 0.06985559\n","Training at Epoch 367 iteration 16 with loss 0.069982134\n","Training at Epoch 367 iteration 17 with loss 0.069622934\n","Training at Epoch 367 iteration 18 with loss 0.06866774\n","Training at Epoch 367 iteration 19 with loss 0.06995743\n","Training at Epoch 367 iteration 20 with loss 0.06817655\n","Training at Epoch 367 iteration 21 with loss 0.069869004\n","Training at Epoch 367 iteration 22 with loss 0.06831743\n","Training at Epoch 367 iteration 23 with loss 0.06958675\n","Training at Epoch 367 iteration 24 with loss 0.06793725\n","Training at Epoch 367 iteration 25 with loss 0.06628491\n","Training at Epoch 367 iteration 26 with loss 0.069081426\n","Training at Epoch 367 iteration 27 with loss 0.06828423\n","Training at Epoch 367 iteration 28 with loss 0.068277076\n","Training at Epoch 367 iteration 29 with loss 0.07033342\n","Training at Epoch 367 iteration 30 with loss 0.069533385\n","Training at Epoch 368 iteration 0 with loss 0.06903175\n","Training at Epoch 368 iteration 1 with loss 0.0699567\n","Training at Epoch 368 iteration 2 with loss 0.06785978\n","Training at Epoch 368 iteration 3 with loss 0.069074586\n","Training at Epoch 368 iteration 4 with loss 0.07066581\n","Training at Epoch 368 iteration 5 with loss 0.06748736\n","Training at Epoch 368 iteration 6 with loss 0.06709341\n","Training at Epoch 368 iteration 7 with loss 0.069123045\n","Training at Epoch 368 iteration 8 with loss 0.07119291\n","Training at Epoch 368 iteration 9 with loss 0.06913784\n","Training at Epoch 368 iteration 10 with loss 0.07107962\n","Training at Epoch 368 iteration 11 with loss 0.07038825\n","Training at Epoch 368 iteration 12 with loss 0.066612124\n","Training at Epoch 368 iteration 13 with loss 0.067874335\n","Training at Epoch 368 iteration 14 with loss 0.06958377\n","Training at Epoch 368 iteration 15 with loss 0.06786586\n","Training at Epoch 368 iteration 16 with loss 0.06873733\n","Training at Epoch 368 iteration 17 with loss 0.06876691\n","Training at Epoch 368 iteration 18 with loss 0.068772666\n","Training at Epoch 368 iteration 19 with loss 0.07004601\n","Training at Epoch 368 iteration 20 with loss 0.06835388\n","Training at Epoch 368 iteration 21 with loss 0.06993445\n","Training at Epoch 368 iteration 22 with loss 0.06960505\n","Training at Epoch 368 iteration 23 with loss 0.06917014\n","Training at Epoch 368 iteration 24 with loss 0.06749081\n","Training at Epoch 368 iteration 25 with loss 0.0699462\n","Training at Epoch 368 iteration 26 with loss 0.06783928\n","Training at Epoch 368 iteration 27 with loss 0.07031765\n","Training at Epoch 368 iteration 28 with loss 0.06949622\n","Training at Epoch 368 iteration 29 with loss 0.06790462\n","Training at Epoch 368 iteration 30 with loss 0.07080606\n","Training at Epoch 369 iteration 0 with loss 0.070865706\n","Training at Epoch 369 iteration 1 with loss 0.068652645\n","Training at Epoch 369 iteration 2 with loss 0.06748638\n","Training at Epoch 369 iteration 3 with loss 0.06915365\n","Training at Epoch 369 iteration 4 with loss 0.06909566\n","Training at Epoch 369 iteration 5 with loss 0.07042091\n","Training at Epoch 369 iteration 6 with loss 0.069194905\n","Training at Epoch 369 iteration 7 with loss 0.06951043\n","Training at Epoch 369 iteration 8 with loss 0.07125102\n","Training at Epoch 369 iteration 9 with loss 0.068675004\n","Training at Epoch 369 iteration 10 with loss 0.069938965\n","Training at Epoch 369 iteration 11 with loss 0.06699222\n","Training at Epoch 369 iteration 12 with loss 0.07038996\n","Training at Epoch 369 iteration 13 with loss 0.06913642\n","Training at Epoch 369 iteration 14 with loss 0.07119229\n","Training at Epoch 369 iteration 15 with loss 0.0703917\n","Training at Epoch 369 iteration 16 with loss 0.069058865\n","Training at Epoch 369 iteration 17 with loss 0.06873317\n","Training at Epoch 369 iteration 18 with loss 0.0675071\n","Training at Epoch 369 iteration 19 with loss 0.06869294\n","Training at Epoch 369 iteration 20 with loss 0.06914574\n","Training at Epoch 369 iteration 21 with loss 0.06992428\n","Training at Epoch 369 iteration 22 with loss 0.06911093\n","Training at Epoch 369 iteration 23 with loss 0.06832655\n","Training at Epoch 369 iteration 24 with loss 0.06785604\n","Training at Epoch 369 iteration 25 with loss 0.06993827\n","Training at Epoch 369 iteration 26 with loss 0.069114335\n","Training at Epoch 369 iteration 27 with loss 0.069982536\n","Training at Epoch 369 iteration 28 with loss 0.0674271\n","Training at Epoch 369 iteration 29 with loss 0.06707053\n","Training at Epoch 369 iteration 30 with loss 0.06788082\n","Training at Epoch 370 iteration 0 with loss 0.072073385\n","Training at Epoch 370 iteration 1 with loss 0.070334084\n","Training at Epoch 370 iteration 2 with loss 0.069538556\n","Training at Epoch 370 iteration 3 with loss 0.068297215\n","Training at Epoch 370 iteration 4 with loss 0.06909832\n","Training at Epoch 370 iteration 5 with loss 0.06873961\n","Training at Epoch 370 iteration 6 with loss 0.06833249\n","Training at Epoch 370 iteration 7 with loss 0.07035458\n","Training at Epoch 370 iteration 8 with loss 0.0699639\n","Training at Epoch 370 iteration 9 with loss 0.067450166\n","Training at Epoch 370 iteration 10 with loss 0.06826854\n","Training at Epoch 370 iteration 11 with loss 0.069107525\n","Training at Epoch 370 iteration 12 with loss 0.06954186\n","Training at Epoch 370 iteration 13 with loss 0.06825829\n","Training at Epoch 370 iteration 14 with loss 0.06916204\n","Training at Epoch 370 iteration 15 with loss 0.0691012\n","Training at Epoch 370 iteration 16 with loss 0.06996827\n","Training at Epoch 370 iteration 17 with loss 0.07035954\n","Training at Epoch 370 iteration 18 with loss 0.06789468\n","Training at Epoch 370 iteration 19 with loss 0.068289995\n","Training at Epoch 370 iteration 20 with loss 0.06829148\n","Training at Epoch 370 iteration 21 with loss 0.068277255\n","Training at Epoch 370 iteration 22 with loss 0.06867951\n","Training at Epoch 370 iteration 23 with loss 0.07036973\n","Training at Epoch 370 iteration 24 with loss 0.07033188\n","Training at Epoch 370 iteration 25 with loss 0.07037787\n","Training at Epoch 370 iteration 26 with loss 0.07035291\n","Training at Epoch 370 iteration 27 with loss 0.06662416\n","Training at Epoch 370 iteration 28 with loss 0.068297386\n","Training at Epoch 370 iteration 29 with loss 0.06828246\n","Training at Epoch 370 iteration 30 with loss 0.06829283\n","Training at Epoch 371 iteration 0 with loss 0.06833081\n","Training at Epoch 371 iteration 1 with loss 0.068715334\n","Training at Epoch 371 iteration 2 with loss 0.06833682\n","Training at Epoch 371 iteration 3 with loss 0.06996061\n","Training at Epoch 371 iteration 4 with loss 0.06785959\n","Training at Epoch 371 iteration 5 with loss 0.07201423\n","Training at Epoch 371 iteration 6 with loss 0.06956997\n","Training at Epoch 371 iteration 7 with loss 0.06871195\n","Training at Epoch 371 iteration 8 with loss 0.06914919\n","Training at Epoch 371 iteration 9 with loss 0.06957251\n","Training at Epoch 371 iteration 10 with loss 0.07036723\n","Training at Epoch 371 iteration 11 with loss 0.06870517\n","Training at Epoch 371 iteration 12 with loss 0.06705446\n","Training at Epoch 371 iteration 13 with loss 0.06955758\n","Training at Epoch 371 iteration 14 with loss 0.06788539\n","Training at Epoch 371 iteration 15 with loss 0.06743771\n","Training at Epoch 371 iteration 16 with loss 0.068275474\n","Training at Epoch 371 iteration 17 with loss 0.070377305\n","Training at Epoch 371 iteration 18 with loss 0.06994129\n","Training at Epoch 371 iteration 19 with loss 0.070765756\n","Training at Epoch 371 iteration 20 with loss 0.07076442\n","Training at Epoch 371 iteration 21 with loss 0.068716705\n","Training at Epoch 371 iteration 22 with loss 0.06909758\n","Training at Epoch 371 iteration 23 with loss 0.069948144\n","Training at Epoch 371 iteration 24 with loss 0.06705812\n","Training at Epoch 371 iteration 25 with loss 0.069939844\n","Training at Epoch 371 iteration 26 with loss 0.069118395\n","Training at Epoch 371 iteration 27 with loss 0.069521114\n","Training at Epoch 371 iteration 28 with loss 0.068307295\n","Training at Epoch 371 iteration 29 with loss 0.06788968\n","Training at Epoch 371 iteration 30 with loss 0.06871565\n","Training at Epoch 372 iteration 0 with loss 0.06787598\n","Training at Epoch 372 iteration 1 with loss 0.06994794\n","Training at Epoch 372 iteration 2 with loss 0.070345685\n","Training at Epoch 372 iteration 3 with loss 0.06951122\n","Training at Epoch 372 iteration 4 with loss 0.068716004\n","Training at Epoch 372 iteration 5 with loss 0.06913279\n","Training at Epoch 372 iteration 6 with loss 0.06746881\n","Training at Epoch 372 iteration 7 with loss 0.06998017\n","Training at Epoch 372 iteration 8 with loss 0.0695311\n","Training at Epoch 372 iteration 9 with loss 0.068703964\n","Training at Epoch 372 iteration 10 with loss 0.067886144\n","Training at Epoch 372 iteration 11 with loss 0.06785742\n","Training at Epoch 372 iteration 12 with loss 0.06870454\n","Training at Epoch 372 iteration 13 with loss 0.06870097\n","Training at Epoch 372 iteration 14 with loss 0.07037578\n","Training at Epoch 372 iteration 15 with loss 0.067031965\n","Training at Epoch 372 iteration 16 with loss 0.06786794\n","Training at Epoch 372 iteration 17 with loss 0.06953284\n","Training at Epoch 372 iteration 18 with loss 0.06912042\n","Training at Epoch 372 iteration 19 with loss 0.06996904\n","Training at Epoch 372 iteration 20 with loss 0.068272896\n","Training at Epoch 372 iteration 21 with loss 0.06827054\n","Training at Epoch 372 iteration 22 with loss 0.069117785\n","Training at Epoch 372 iteration 23 with loss 0.06785538\n","Training at Epoch 372 iteration 24 with loss 0.06996678\n","Training at Epoch 372 iteration 25 with loss 0.069124535\n","Training at Epoch 372 iteration 26 with loss 0.07081266\n","Training at Epoch 372 iteration 27 with loss 0.071684085\n","Training at Epoch 372 iteration 28 with loss 0.06998923\n","Training at Epoch 372 iteration 29 with loss 0.06912227\n","Training at Epoch 372 iteration 30 with loss 0.06954718\n","Training at Epoch 373 iteration 0 with loss 0.06912193\n","Training at Epoch 373 iteration 1 with loss 0.06997752\n","Training at Epoch 373 iteration 2 with loss 0.06869127\n","Training at Epoch 373 iteration 3 with loss 0.068279475\n","Training at Epoch 373 iteration 4 with loss 0.06699918\n","Training at Epoch 373 iteration 5 with loss 0.068268694\n","Training at Epoch 373 iteration 6 with loss 0.068271056\n","Training at Epoch 373 iteration 7 with loss 0.068675965\n","Training at Epoch 373 iteration 8 with loss 0.06870018\n","Training at Epoch 373 iteration 9 with loss 0.06996725\n","Training at Epoch 373 iteration 10 with loss 0.06826704\n","Training at Epoch 373 iteration 11 with loss 0.070823655\n","Training at Epoch 373 iteration 12 with loss 0.068253875\n","Training at Epoch 373 iteration 13 with loss 0.06912224\n","Training at Epoch 373 iteration 14 with loss 0.06869255\n","Training at Epoch 373 iteration 15 with loss 0.06868959\n","Training at Epoch 373 iteration 16 with loss 0.06869359\n","Training at Epoch 373 iteration 17 with loss 0.07041071\n","Training at Epoch 373 iteration 18 with loss 0.0699731\n","Training at Epoch 373 iteration 19 with loss 0.069978654\n","Training at Epoch 373 iteration 20 with loss 0.06955348\n","Training at Epoch 373 iteration 21 with loss 0.069981076\n","Training at Epoch 373 iteration 22 with loss 0.07040234\n","Training at Epoch 373 iteration 23 with loss 0.0682589\n","Training at Epoch 373 iteration 24 with loss 0.06955397\n","Training at Epoch 373 iteration 25 with loss 0.069981724\n","Training at Epoch 373 iteration 26 with loss 0.066965416\n","Training at Epoch 373 iteration 27 with loss 0.06911925\n","Training at Epoch 373 iteration 28 with loss 0.06825143\n","Training at Epoch 373 iteration 29 with loss 0.068684004\n","Training at Epoch 373 iteration 30 with loss 0.071717225\n","Training at Epoch 374 iteration 0 with loss 0.07129095\n","Training at Epoch 374 iteration 1 with loss 0.06782967\n","Training at Epoch 374 iteration 2 with loss 0.07085167\n","Training at Epoch 374 iteration 3 with loss 0.06913144\n","Training at Epoch 374 iteration 4 with loss 0.068686046\n","Training at Epoch 374 iteration 5 with loss 0.06912146\n","Training at Epoch 374 iteration 6 with loss 0.06739099\n","Training at Epoch 374 iteration 7 with loss 0.0695505\n","Training at Epoch 374 iteration 8 with loss 0.06825384\n","Training at Epoch 374 iteration 9 with loss 0.069551945\n","Training at Epoch 374 iteration 10 with loss 0.0686961\n","Training at Epoch 374 iteration 11 with loss 0.06868972\n","Training at Epoch 374 iteration 12 with loss 0.068259306\n","Training at Epoch 374 iteration 13 with loss 0.06653331\n","Training at Epoch 374 iteration 14 with loss 0.06782328\n","Training at Epoch 374 iteration 15 with loss 0.069552176\n","Training at Epoch 374 iteration 16 with loss 0.06955995\n","Training at Epoch 374 iteration 17 with loss 0.06912024\n","Training at Epoch 374 iteration 18 with loss 0.06912134\n","Training at Epoch 374 iteration 19 with loss 0.06739587\n","Training at Epoch 374 iteration 20 with loss 0.070425935\n","Training at Epoch 374 iteration 21 with loss 0.069992736\n","Training at Epoch 374 iteration 22 with loss 0.06780041\n","Training at Epoch 374 iteration 23 with loss 0.069558725\n","Training at Epoch 374 iteration 24 with loss 0.070861414\n","Training at Epoch 374 iteration 25 with loss 0.06956029\n","Training at Epoch 374 iteration 26 with loss 0.06956904\n","Training at Epoch 374 iteration 27 with loss 0.06737974\n","Training at Epoch 374 iteration 28 with loss 0.0686845\n","Training at Epoch 374 iteration 29 with loss 0.07043253\n","Training at Epoch 374 iteration 30 with loss 0.07087598\n","Training at Epoch 375 iteration 0 with loss 0.06781511\n","Training at Epoch 375 iteration 1 with loss 0.068680376\n","Training at Epoch 375 iteration 2 with loss 0.07086544\n","Training at Epoch 375 iteration 3 with loss 0.068243705\n","Training at Epoch 375 iteration 4 with loss 0.068687394\n","Training at Epoch 375 iteration 5 with loss 0.06912139\n","Training at Epoch 375 iteration 6 with loss 0.06868495\n","Training at Epoch 375 iteration 7 with loss 0.06650555\n","Training at Epoch 375 iteration 8 with loss 0.068690225\n","Training at Epoch 375 iteration 9 with loss 0.07042955\n","Training at Epoch 375 iteration 10 with loss 0.06999687\n","Training at Epoch 375 iteration 11 with loss 0.069561794\n","Training at Epoch 375 iteration 12 with loss 0.070869766\n","Training at Epoch 375 iteration 13 with loss 0.068248585\n","Training at Epoch 375 iteration 14 with loss 0.0678103\n","Training at Epoch 375 iteration 15 with loss 0.06825012\n","Training at Epoch 375 iteration 16 with loss 0.07042776\n","Training at Epoch 375 iteration 17 with loss 0.06737511\n","Training at Epoch 375 iteration 18 with loss 0.069561645\n","Training at Epoch 375 iteration 19 with loss 0.068684995\n","Training at Epoch 375 iteration 20 with loss 0.06999583\n","Training at Epoch 375 iteration 21 with loss 0.070878334\n","Training at Epoch 375 iteration 22 with loss 0.068243176\n","Training at Epoch 375 iteration 23 with loss 0.06824452\n","Training at Epoch 375 iteration 24 with loss 0.06956086\n","Training at Epoch 375 iteration 25 with loss 0.07043816\n","Training at Epoch 375 iteration 26 with loss 0.06868408\n","Training at Epoch 375 iteration 27 with loss 0.069121934\n","Training at Epoch 375 iteration 28 with loss 0.06824454\n","Training at Epoch 375 iteration 29 with loss 0.06956439\n","Training at Epoch 375 iteration 30 with loss 0.07087611\n","Training at Epoch 376 iteration 0 with loss 0.06956169\n","Training at Epoch 376 iteration 1 with loss 0.06956248\n","Training at Epoch 376 iteration 2 with loss 0.06955723\n","Training at Epoch 376 iteration 3 with loss 0.069559835\n","Training at Epoch 376 iteration 4 with loss 0.06999466\n","Training at Epoch 376 iteration 5 with loss 0.068684265\n","Training at Epoch 376 iteration 6 with loss 0.06911899\n","Training at Epoch 376 iteration 7 with loss 0.071310356\n","Training at Epoch 376 iteration 8 with loss 0.06780927\n","Training at Epoch 376 iteration 9 with loss 0.06868844\n","Training at Epoch 376 iteration 10 with loss 0.070418686\n","Training at Epoch 376 iteration 11 with loss 0.06782475\n","Training at Epoch 376 iteration 12 with loss 0.06998091\n","Training at Epoch 376 iteration 13 with loss 0.069553\n","Training at Epoch 376 iteration 14 with loss 0.06955323\n","Training at Epoch 376 iteration 15 with loss 0.06825696\n","Training at Epoch 376 iteration 16 with loss 0.0673866\n","Training at Epoch 376 iteration 17 with loss 0.0669629\n","Training at Epoch 376 iteration 18 with loss 0.06739147\n","Training at Epoch 376 iteration 19 with loss 0.069543615\n","Training at Epoch 376 iteration 20 with loss 0.06998444\n","Training at Epoch 376 iteration 21 with loss 0.06912117\n","Training at Epoch 376 iteration 22 with loss 0.06998198\n","Training at Epoch 376 iteration 23 with loss 0.069121115\n","Training at Epoch 376 iteration 24 with loss 0.06826085\n","Training at Epoch 376 iteration 25 with loss 0.07042127\n","Training at Epoch 376 iteration 26 with loss 0.069552764\n","Training at Epoch 376 iteration 27 with loss 0.06696443\n","Training at Epoch 376 iteration 28 with loss 0.06998213\n","Training at Epoch 376 iteration 29 with loss 0.06998254\n","Training at Epoch 376 iteration 30 with loss 0.06741061\n","Training at Epoch 377 iteration 0 with loss 0.0674078\n","Training at Epoch 377 iteration 1 with loss 0.06826642\n","Training at Epoch 377 iteration 2 with loss 0.07083061\n","Training at Epoch 377 iteration 3 with loss 0.068693936\n","Training at Epoch 377 iteration 4 with loss 0.07082946\n","Training at Epoch 377 iteration 5 with loss 0.06826448\n","Training at Epoch 377 iteration 6 with loss 0.07040308\n","Training at Epoch 377 iteration 7 with loss 0.068691716\n","Training at Epoch 377 iteration 8 with loss 0.068700954\n","Training at Epoch 377 iteration 9 with loss 0.06954875\n","Training at Epoch 377 iteration 10 with loss 0.06954554\n","Training at Epoch 377 iteration 11 with loss 0.069970265\n","Training at Epoch 377 iteration 12 with loss 0.06742236\n","Training at Epoch 377 iteration 13 with loss 0.066571794\n","Training at Epoch 377 iteration 14 with loss 0.06869392\n","Training at Epoch 377 iteration 15 with loss 0.06954552\n","Training at Epoch 377 iteration 16 with loss 0.067846045\n","Training at Epoch 377 iteration 17 with loss 0.068271235\n","Training at Epoch 377 iteration 18 with loss 0.07166998\n","Training at Epoch 377 iteration 19 with loss 0.068697855\n","Training at Epoch 377 iteration 20 with loss 0.067416884\n","Training at Epoch 377 iteration 21 with loss 0.06997442\n","Training at Epoch 377 iteration 22 with loss 0.07040334\n","Training at Epoch 377 iteration 23 with loss 0.071246766\n","Training at Epoch 377 iteration 24 with loss 0.06869557\n","Training at Epoch 377 iteration 25 with loss 0.06741826\n","Training at Epoch 377 iteration 26 with loss 0.06954438\n","Training at Epoch 377 iteration 27 with loss 0.070388906\n","Training at Epoch 377 iteration 28 with loss 0.066993795\n","Training at Epoch 377 iteration 29 with loss 0.07082105\n","Training at Epoch 377 iteration 30 with loss 0.06911866\n","Training at Epoch 378 iteration 0 with loss 0.066581026\n","Training at Epoch 378 iteration 1 with loss 0.07039037\n","Training at Epoch 378 iteration 2 with loss 0.06827118\n","Training at Epoch 378 iteration 3 with loss 0.07039162\n","Training at Epoch 378 iteration 4 with loss 0.07081307\n","Training at Epoch 378 iteration 5 with loss 0.06869836\n","Training at Epoch 378 iteration 6 with loss 0.06954942\n","Training at Epoch 378 iteration 7 with loss 0.069546774\n","Training at Epoch 378 iteration 8 with loss 0.06954345\n","Training at Epoch 378 iteration 9 with loss 0.06742337\n","Training at Epoch 378 iteration 10 with loss 0.06870845\n","Training at Epoch 378 iteration 11 with loss 0.07123114\n","Training at Epoch 378 iteration 12 with loss 0.06870388\n","Training at Epoch 378 iteration 13 with loss 0.06827684\n","Training at Epoch 378 iteration 14 with loss 0.068699345\n","Training at Epoch 378 iteration 15 with loss 0.06827717\n","Training at Epoch 378 iteration 16 with loss 0.07038359\n","Training at Epoch 378 iteration 17 with loss 0.06743312\n","Training at Epoch 378 iteration 18 with loss 0.068286665\n","Training at Epoch 378 iteration 19 with loss 0.06997015\n","Training at Epoch 378 iteration 20 with loss 0.06996095\n","Training at Epoch 378 iteration 21 with loss 0.0695401\n","Training at Epoch 378 iteration 22 with loss 0.070383504\n","Training at Epoch 378 iteration 23 with loss 0.069540426\n","Training at Epoch 378 iteration 24 with loss 0.069537565\n","Training at Epoch 378 iteration 25 with loss 0.069118336\n","Training at Epoch 378 iteration 26 with loss 0.06827909\n","Training at Epoch 378 iteration 27 with loss 0.06870004\n","Training at Epoch 378 iteration 28 with loss 0.0678622\n","Training at Epoch 378 iteration 29 with loss 0.06827961\n","Training at Epoch 378 iteration 30 with loss 0.06954031\n","Training at Epoch 379 iteration 0 with loss 0.067862764\n","Training at Epoch 379 iteration 1 with loss 0.06870179\n","Training at Epoch 379 iteration 2 with loss 0.06870109\n","Training at Epoch 379 iteration 3 with loss 0.067861766\n","Training at Epoch 379 iteration 4 with loss 0.07037561\n","Training at Epoch 379 iteration 5 with loss 0.069959976\n","Training at Epoch 379 iteration 6 with loss 0.069540694\n","Training at Epoch 379 iteration 7 with loss 0.069121614\n","Training at Epoch 379 iteration 8 with loss 0.06744552\n","Training at Epoch 379 iteration 9 with loss 0.068283156\n","Training at Epoch 379 iteration 10 with loss 0.07164811\n","Training at Epoch 379 iteration 11 with loss 0.068278775\n","Training at Epoch 379 iteration 12 with loss 0.06869505\n","Training at Epoch 379 iteration 13 with loss 0.06827797\n","Training at Epoch 379 iteration 14 with loss 0.06871154\n","Training at Epoch 379 iteration 15 with loss 0.069961764\n","Training at Epoch 379 iteration 16 with loss 0.06871266\n","Training at Epoch 379 iteration 17 with loss 0.070381135\n","Training at Epoch 379 iteration 18 with loss 0.06701389\n","Training at Epoch 379 iteration 19 with loss 0.0708028\n","Training at Epoch 379 iteration 20 with loss 0.069961354\n","Training at Epoch 379 iteration 21 with loss 0.071226455\n","Training at Epoch 379 iteration 22 with loss 0.069542386\n","Training at Epoch 379 iteration 23 with loss 0.07038485\n","Training at Epoch 379 iteration 24 with loss 0.066589355\n","Training at Epoch 379 iteration 25 with loss 0.067856714\n","Training at Epoch 379 iteration 26 with loss 0.06953671\n","Training at Epoch 379 iteration 27 with loss 0.070804864\n","Training at Epoch 379 iteration 28 with loss 0.06827884\n","Training at Epoch 379 iteration 29 with loss 0.068699315\n","Training at Epoch 379 iteration 30 with loss 0.06828276\n","Training at Epoch 380 iteration 0 with loss 0.067443535\n","Training at Epoch 380 iteration 1 with loss 0.06995736\n","Training at Epoch 380 iteration 2 with loss 0.06912001\n","Training at Epoch 380 iteration 3 with loss 0.069538705\n","Training at Epoch 380 iteration 4 with loss 0.069958344\n","Training at Epoch 380 iteration 5 with loss 0.0682818\n","Training at Epoch 380 iteration 6 with loss 0.070377804\n","Training at Epoch 380 iteration 7 with loss 0.06618692\n","Training at Epoch 380 iteration 8 with loss 0.070803985\n","Training at Epoch 380 iteration 9 with loss 0.066605106\n","Training at Epoch 380 iteration 10 with loss 0.06785868\n","Training at Epoch 380 iteration 11 with loss 0.069538705\n","Training at Epoch 380 iteration 12 with loss 0.06953836\n","Training at Epoch 380 iteration 13 with loss 0.06827978\n","Training at Epoch 380 iteration 14 with loss 0.067438066\n","Training at Epoch 380 iteration 15 with loss 0.067859486\n","Training at Epoch 380 iteration 16 with loss 0.069125645\n","Training at Epoch 380 iteration 17 with loss 0.06997061\n","Training at Epoch 380 iteration 18 with loss 0.06744565\n","Training at Epoch 380 iteration 19 with loss 0.069124155\n","Training at Epoch 380 iteration 20 with loss 0.069967605\n","Training at Epoch 380 iteration 21 with loss 0.069973916\n","Training at Epoch 380 iteration 22 with loss 0.0695465\n","Training at Epoch 380 iteration 23 with loss 0.06741442\n","Training at Epoch 380 iteration 24 with loss 0.06996973\n","Training at Epoch 380 iteration 25 with loss 0.068268165\n","Training at Epoch 380 iteration 26 with loss 0.06997149\n","Training at Epoch 380 iteration 27 with loss 0.07125691\n","Training at Epoch 380 iteration 28 with loss 0.06912578\n","Training at Epoch 380 iteration 29 with loss 0.07125769\n","Training at Epoch 380 iteration 30 with loss 0.07125458\n","Training at Epoch 381 iteration 0 with loss 0.06954785\n","Training at Epoch 381 iteration 1 with loss 0.07040039\n","Training at Epoch 381 iteration 2 with loss 0.06869424\n","Training at Epoch 381 iteration 3 with loss 0.06613704\n","Training at Epoch 381 iteration 4 with loss 0.068268865\n","Training at Epoch 381 iteration 5 with loss 0.06912108\n","Training at Epoch 381 iteration 6 with loss 0.068694316\n","Training at Epoch 381 iteration 7 with loss 0.06912136\n","Training at Epoch 381 iteration 8 with loss 0.06954761\n","Training at Epoch 381 iteration 9 with loss 0.06869427\n","Training at Epoch 381 iteration 10 with loss 0.06954682\n","Training at Epoch 381 iteration 11 with loss 0.06912081\n","Training at Epoch 381 iteration 12 with loss 0.06954707\n","Training at Epoch 381 iteration 13 with loss 0.06869036\n","Training at Epoch 381 iteration 14 with loss 0.07125515\n","Training at Epoch 381 iteration 15 with loss 0.06655155\n","Training at Epoch 381 iteration 16 with loss 0.068697\n","Training at Epoch 381 iteration 17 with loss 0.0712553\n","Training at Epoch 381 iteration 18 with loss 0.0686952\n","Training at Epoch 381 iteration 19 with loss 0.06826512\n","Training at Epoch 381 iteration 20 with loss 0.06827007\n","Training at Epoch 381 iteration 21 with loss 0.06869508\n","Training at Epoch 381 iteration 22 with loss 0.0691161\n","Training at Epoch 381 iteration 23 with loss 0.07039943\n","Training at Epoch 381 iteration 24 with loss 0.07040351\n","Training at Epoch 381 iteration 25 with loss 0.07167672\n","Training at Epoch 381 iteration 26 with loss 0.06953977\n","Training at Epoch 381 iteration 27 with loss 0.06869449\n","Training at Epoch 381 iteration 28 with loss 0.06528343\n","Training at Epoch 381 iteration 29 with loss 0.069545284\n","Training at Epoch 381 iteration 30 with loss 0.069962084\n","Training at Epoch 382 iteration 0 with loss 0.06785152\n","Training at Epoch 382 iteration 1 with loss 0.06954253\n","Training at Epoch 382 iteration 2 with loss 0.068696335\n","Training at Epoch 382 iteration 3 with loss 0.07124243\n","Training at Epoch 382 iteration 4 with loss 0.06869304\n","Training at Epoch 382 iteration 5 with loss 0.06997139\n","Training at Epoch 382 iteration 6 with loss 0.067848265\n","Training at Epoch 382 iteration 7 with loss 0.06954497\n","Training at Epoch 382 iteration 8 with loss 0.06742202\n","Training at Epoch 382 iteration 9 with loss 0.06828031\n","Training at Epoch 382 iteration 10 with loss 0.06784892\n","Training at Epoch 382 iteration 11 with loss 0.07039221\n","Training at Epoch 382 iteration 12 with loss 0.06953557\n","Training at Epoch 382 iteration 13 with loss 0.06869559\n","Training at Epoch 382 iteration 14 with loss 0.0691146\n","Training at Epoch 382 iteration 15 with loss 0.06742381\n","Training at Epoch 382 iteration 16 with loss 0.070406534\n","Training at Epoch 382 iteration 17 with loss 0.069969445\n","Training at Epoch 382 iteration 18 with loss 0.06700747\n","Training at Epoch 382 iteration 19 with loss 0.06912235\n","Training at Epoch 382 iteration 20 with loss 0.06997233\n","Training at Epoch 382 iteration 21 with loss 0.06996519\n","Training at Epoch 382 iteration 22 with loss 0.06996487\n","Training at Epoch 382 iteration 23 with loss 0.069542095\n","Training at Epoch 382 iteration 24 with loss 0.06869283\n","Training at Epoch 382 iteration 25 with loss 0.06827148\n","Training at Epoch 382 iteration 26 with loss 0.06954119\n","Training at Epoch 382 iteration 27 with loss 0.069970466\n","Training at Epoch 382 iteration 28 with loss 0.070395045\n","Training at Epoch 382 iteration 29 with loss 0.06954661\n","Training at Epoch 382 iteration 30 with loss 0.067853704\n","Training at Epoch 383 iteration 0 with loss 0.06659574\n","Training at Epoch 383 iteration 1 with loss 0.069120415\n","Training at Epoch 383 iteration 2 with loss 0.06701664\n","Training at Epoch 383 iteration 3 with loss 0.06701431\n","Training at Epoch 383 iteration 4 with loss 0.07122877\n","Training at Epoch 383 iteration 5 with loss 0.06954412\n","Training at Epoch 383 iteration 6 with loss 0.06996141\n","Training at Epoch 383 iteration 7 with loss 0.06785692\n","Training at Epoch 383 iteration 8 with loss 0.07038227\n","Training at Epoch 383 iteration 9 with loss 0.068275094\n","Training at Epoch 383 iteration 10 with loss 0.06700798\n","Training at Epoch 383 iteration 11 with loss 0.067850634\n","Training at Epoch 383 iteration 12 with loss 0.068703376\n","Training at Epoch 383 iteration 13 with loss 0.06954199\n","Training at Epoch 383 iteration 14 with loss 0.07081126\n","Training at Epoch 383 iteration 15 with loss 0.06784678\n","Training at Epoch 383 iteration 16 with loss 0.06869722\n","Training at Epoch 383 iteration 17 with loss 0.07038924\n","Training at Epoch 383 iteration 18 with loss 0.07038437\n","Training at Epoch 383 iteration 19 with loss 0.068272784\n","Training at Epoch 383 iteration 20 with loss 0.06954553\n","Training at Epoch 383 iteration 21 with loss 0.06784676\n","Training at Epoch 383 iteration 22 with loss 0.06826536\n","Training at Epoch 383 iteration 23 with loss 0.07081827\n","Training at Epoch 383 iteration 24 with loss 0.06869529\n","Training at Epoch 383 iteration 25 with loss 0.07039662\n","Training at Epoch 383 iteration 26 with loss 0.0703964\n","Training at Epoch 383 iteration 27 with loss 0.06954557\n","Training at Epoch 383 iteration 28 with loss 0.07039474\n","Training at Epoch 383 iteration 29 with loss 0.0691223\n","Training at Epoch 383 iteration 30 with loss 0.06997609\n","Training at Epoch 384 iteration 0 with loss 0.06785067\n","Training at Epoch 384 iteration 1 with loss 0.06954522\n","Training at Epoch 384 iteration 2 with loss 0.06786547\n","Training at Epoch 384 iteration 3 with loss 0.06910115\n","Training at Epoch 384 iteration 4 with loss 0.07038967\n","Training at Epoch 384 iteration 5 with loss 0.06911846\n","Training at Epoch 384 iteration 6 with loss 0.06742368\n","Training at Epoch 384 iteration 7 with loss 0.06827335\n","Training at Epoch 384 iteration 8 with loss 0.07081337\n","Training at Epoch 384 iteration 9 with loss 0.067858264\n","Training at Epoch 384 iteration 10 with loss 0.06911702\n","Training at Epoch 384 iteration 11 with loss 0.06996833\n","Training at Epoch 384 iteration 12 with loss 0.0703996\n","Training at Epoch 384 iteration 13 with loss 0.06911757\n","Training at Epoch 384 iteration 14 with loss 0.067430094\n","Training at Epoch 384 iteration 15 with loss 0.06869706\n","Training at Epoch 384 iteration 16 with loss 0.070387855\n","Training at Epoch 384 iteration 17 with loss 0.06954296\n","Training at Epoch 384 iteration 18 with loss 0.07039572\n","Training at Epoch 384 iteration 19 with loss 0.06828301\n","Training at Epoch 384 iteration 20 with loss 0.07079956\n","Training at Epoch 384 iteration 21 with loss 0.06954439\n","Training at Epoch 384 iteration 22 with loss 0.06869818\n","Training at Epoch 384 iteration 23 with loss 0.0678616\n","Training at Epoch 384 iteration 24 with loss 0.067855366\n","Training at Epoch 384 iteration 25 with loss 0.06827357\n","Training at Epoch 384 iteration 26 with loss 0.06870204\n","Training at Epoch 384 iteration 27 with loss 0.06954372\n","Training at Epoch 384 iteration 28 with loss 0.0707979\n","Training at Epoch 384 iteration 29 with loss 0.069124885\n","Training at Epoch 384 iteration 30 with loss 0.06953847\n","Training at Epoch 385 iteration 0 with loss 0.06869939\n","Training at Epoch 385 iteration 1 with loss 0.07079631\n","Training at Epoch 385 iteration 2 with loss 0.06829544\n","Training at Epoch 385 iteration 3 with loss 0.06912433\n","Training at Epoch 385 iteration 4 with loss 0.06786438\n","Training at Epoch 385 iteration 5 with loss 0.06952725\n","Training at Epoch 385 iteration 6 with loss 0.06993946\n","Training at Epoch 385 iteration 7 with loss 0.07120584\n","Training at Epoch 385 iteration 8 with loss 0.06953231\n","Training at Epoch 385 iteration 9 with loss 0.06911601\n","Training at Epoch 385 iteration 10 with loss 0.06913148\n","Training at Epoch 385 iteration 11 with loss 0.069129474\n","Training at Epoch 385 iteration 12 with loss 0.066638745\n","Training at Epoch 385 iteration 13 with loss 0.07117839\n","Training at Epoch 385 iteration 14 with loss 0.0699443\n","Training at Epoch 385 iteration 15 with loss 0.06871017\n","Training at Epoch 385 iteration 16 with loss 0.07035135\n","Training at Epoch 385 iteration 17 with loss 0.06747006\n","Training at Epoch 385 iteration 18 with loss 0.070359126\n","Training at Epoch 385 iteration 19 with loss 0.06788413\n","Training at Epoch 385 iteration 20 with loss 0.06626512\n","Training at Epoch 385 iteration 21 with loss 0.06830079\n","Training at Epoch 385 iteration 22 with loss 0.07033588\n","Training at Epoch 385 iteration 23 with loss 0.06871037\n","Training at Epoch 385 iteration 24 with loss 0.06830285\n","Training at Epoch 385 iteration 25 with loss 0.06872426\n","Training at Epoch 385 iteration 26 with loss 0.0683042\n","Training at Epoch 385 iteration 27 with loss 0.069110885\n","Training at Epoch 385 iteration 28 with loss 0.06912205\n","Training at Epoch 385 iteration 29 with loss 0.06830424\n","Training at Epoch 385 iteration 30 with loss 0.07116478\n","Training at Epoch 386 iteration 0 with loss 0.06953323\n","Training at Epoch 386 iteration 1 with loss 0.067481235\n","Training at Epoch 386 iteration 2 with loss 0.07075423\n","Training at Epoch 386 iteration 3 with loss 0.069131054\n","Training at Epoch 386 iteration 4 with loss 0.06912016\n","Training at Epoch 386 iteration 5 with loss 0.068708695\n","Training at Epoch 386 iteration 6 with loss 0.06994009\n","Training at Epoch 386 iteration 7 with loss 0.06911877\n","Training at Epoch 386 iteration 8 with loss 0.06829423\n","Training at Epoch 386 iteration 9 with loss 0.06993732\n","Training at Epoch 386 iteration 10 with loss 0.06952626\n","Training at Epoch 386 iteration 11 with loss 0.06910956\n","Training at Epoch 386 iteration 12 with loss 0.06544805\n","Training at Epoch 386 iteration 13 with loss 0.06831016\n","Training at Epoch 386 iteration 14 with loss 0.06911752\n","Training at Epoch 386 iteration 15 with loss 0.069931865\n","Training at Epoch 386 iteration 16 with loss 0.06911747\n","Training at Epoch 386 iteration 17 with loss 0.07156858\n","Training at Epoch 386 iteration 18 with loss 0.07033395\n","Training at Epoch 386 iteration 19 with loss 0.06911992\n","Training at Epoch 386 iteration 20 with loss 0.06912156\n","Training at Epoch 386 iteration 21 with loss 0.06790732\n","Training at Epoch 386 iteration 22 with loss 0.07155031\n","Training at Epoch 386 iteration 23 with loss 0.07034115\n","Training at Epoch 386 iteration 24 with loss 0.06790244\n","Training at Epoch 386 iteration 25 with loss 0.06871407\n","Training at Epoch 386 iteration 26 with loss 0.06953094\n","Training at Epoch 386 iteration 27 with loss 0.06711056\n","Training at Epoch 386 iteration 28 with loss 0.069940165\n","Training at Epoch 386 iteration 29 with loss 0.06830364\n","Training at Epoch 386 iteration 30 with loss 0.067911394\n","Training at Epoch 387 iteration 0 with loss 0.06790741\n","Training at Epoch 387 iteration 1 with loss 0.06952462\n","Training at Epoch 387 iteration 2 with loss 0.069113836\n","Training at Epoch 387 iteration 3 with loss 0.06871597\n","Training at Epoch 387 iteration 4 with loss 0.06790718\n","Training at Epoch 387 iteration 5 with loss 0.06952228\n","Training at Epoch 387 iteration 6 with loss 0.06791137\n","Training at Epoch 387 iteration 7 with loss 0.070318535\n","Training at Epoch 387 iteration 8 with loss 0.069122545\n","Training at Epoch 387 iteration 9 with loss 0.070328325\n","Training at Epoch 387 iteration 10 with loss 0.06791415\n","Training at Epoch 387 iteration 11 with loss 0.068303764\n","Training at Epoch 387 iteration 12 with loss 0.06869246\n","Training at Epoch 387 iteration 13 with loss 0.06993143\n","Training at Epoch 387 iteration 14 with loss 0.07033544\n","Training at Epoch 387 iteration 15 with loss 0.07073265\n","Training at Epoch 387 iteration 16 with loss 0.066698544\n","Training at Epoch 387 iteration 17 with loss 0.068700746\n","Training at Epoch 387 iteration 18 with loss 0.06993146\n","Training at Epoch 387 iteration 19 with loss 0.06912228\n","Training at Epoch 387 iteration 20 with loss 0.06669099\n","Training at Epoch 387 iteration 21 with loss 0.0687104\n","Training at Epoch 387 iteration 22 with loss 0.0699472\n","Training at Epoch 387 iteration 23 with loss 0.06831902\n","Training at Epoch 387 iteration 24 with loss 0.06992866\n","Training at Epoch 387 iteration 25 with loss 0.06955852\n","Training at Epoch 387 iteration 26 with loss 0.07033789\n","Training at Epoch 387 iteration 27 with loss 0.069940045\n","Training at Epoch 387 iteration 28 with loss 0.070757404\n","Training at Epoch 387 iteration 29 with loss 0.068715766\n","Training at Epoch 387 iteration 30 with loss 0.068314105\n","Training at Epoch 388 iteration 0 with loss 0.06870964\n","Training at Epoch 388 iteration 1 with loss 0.070734166\n","Training at Epoch 388 iteration 2 with loss 0.070331104\n","Training at Epoch 388 iteration 3 with loss 0.06709472\n","Training at Epoch 388 iteration 4 with loss 0.06952303\n","Training at Epoch 388 iteration 5 with loss 0.069524504\n","Training at Epoch 388 iteration 6 with loss 0.06912595\n","Training at Epoch 388 iteration 7 with loss 0.07112313\n","Training at Epoch 388 iteration 8 with loss 0.07074879\n","Training at Epoch 388 iteration 9 with loss 0.06951861\n","Training at Epoch 388 iteration 10 with loss 0.06832074\n","Training at Epoch 388 iteration 11 with loss 0.069915995\n","Training at Epoch 388 iteration 12 with loss 0.068726145\n","Training at Epoch 388 iteration 13 with loss 0.06832507\n","Training at Epoch 388 iteration 14 with loss 0.06751074\n","Training at Epoch 388 iteration 15 with loss 0.06912172\n","Training at Epoch 388 iteration 16 with loss 0.06871206\n","Training at Epoch 388 iteration 17 with loss 0.06991597\n","Training at Epoch 388 iteration 18 with loss 0.06753196\n","Training at Epoch 388 iteration 19 with loss 0.06873649\n","Training at Epoch 388 iteration 20 with loss 0.06832177\n","Training at Epoch 388 iteration 21 with loss 0.06951898\n","Training at Epoch 388 iteration 22 with loss 0.06872337\n","Training at Epoch 388 iteration 23 with loss 0.07031508\n","Training at Epoch 388 iteration 24 with loss 0.06792559\n","Training at Epoch 388 iteration 25 with loss 0.068316855\n","Training at Epoch 388 iteration 26 with loss 0.06791875\n","Training at Epoch 388 iteration 27 with loss 0.069113016\n","Training at Epoch 388 iteration 28 with loss 0.07072834\n","Training at Epoch 388 iteration 29 with loss 0.06912678\n","Training at Epoch 388 iteration 30 with loss 0.06793201\n","Training at Epoch 389 iteration 0 with loss 0.06792359\n","Training at Epoch 389 iteration 1 with loss 0.07071847\n","Training at Epoch 389 iteration 2 with loss 0.069106504\n","Training at Epoch 389 iteration 3 with loss 0.06992431\n","Training at Epoch 389 iteration 4 with loss 0.069520615\n","Training at Epoch 389 iteration 5 with loss 0.06952278\n","Training at Epoch 389 iteration 6 with loss 0.06990371\n","Training at Epoch 389 iteration 7 with loss 0.06792105\n","Training at Epoch 389 iteration 8 with loss 0.06751656\n","Training at Epoch 389 iteration 9 with loss 0.071909055\n","Training at Epoch 389 iteration 10 with loss 0.068320855\n","Training at Epoch 389 iteration 11 with loss 0.0687041\n","Training at Epoch 389 iteration 12 with loss 0.06950899\n","Training at Epoch 389 iteration 13 with loss 0.06752954\n","Training at Epoch 389 iteration 14 with loss 0.069525555\n","Training at Epoch 389 iteration 15 with loss 0.07031942\n","Training at Epoch 389 iteration 16 with loss 0.07070805\n","Training at Epoch 389 iteration 17 with loss 0.066725686\n","Training at Epoch 389 iteration 18 with loss 0.069119334\n","Training at Epoch 389 iteration 19 with loss 0.06873\n","Training at Epoch 389 iteration 20 with loss 0.0683271\n","Training at Epoch 389 iteration 21 with loss 0.07111236\n","Training at Epoch 389 iteration 22 with loss 0.069921106\n","Training at Epoch 389 iteration 23 with loss 0.068318315\n","Training at Epoch 389 iteration 24 with loss 0.07032779\n","Training at Epoch 389 iteration 25 with loss 0.0679231\n","Training at Epoch 389 iteration 26 with loss 0.06792255\n","Training at Epoch 389 iteration 27 with loss 0.06871134\n","Training at Epoch 389 iteration 28 with loss 0.069139585\n","Training at Epoch 389 iteration 29 with loss 0.06792157\n","Training at Epoch 389 iteration 30 with loss 0.06992398\n","Training at Epoch 390 iteration 0 with loss 0.06831311\n","Training at Epoch 390 iteration 1 with loss 0.06832353\n","Training at Epoch 390 iteration 2 with loss 0.06709842\n","Training at Epoch 390 iteration 3 with loss 0.06912746\n","Training at Epoch 390 iteration 4 with loss 0.07113868\n","Training at Epoch 390 iteration 5 with loss 0.0707312\n","Training at Epoch 390 iteration 6 with loss 0.067918025\n","Training at Epoch 390 iteration 7 with loss 0.07032338\n","Training at Epoch 390 iteration 8 with loss 0.0695242\n","Training at Epoch 390 iteration 9 with loss 0.0699272\n","Training at Epoch 390 iteration 10 with loss 0.068711415\n","Training at Epoch 390 iteration 11 with loss 0.06711532\n","Training at Epoch 390 iteration 12 with loss 0.06994027\n","Training at Epoch 390 iteration 13 with loss 0.0671051\n","Training at Epoch 390 iteration 14 with loss 0.06790884\n","Training at Epoch 390 iteration 15 with loss 0.069919184\n","Training at Epoch 390 iteration 16 with loss 0.07033943\n","Training at Epoch 390 iteration 17 with loss 0.07152773\n","Training at Epoch 390 iteration 18 with loss 0.07032359\n","Training at Epoch 390 iteration 19 with loss 0.06792232\n","Training at Epoch 390 iteration 20 with loss 0.06951872\n","Training at Epoch 390 iteration 21 with loss 0.066294834\n","Training at Epoch 390 iteration 22 with loss 0.06992831\n","Training at Epoch 390 iteration 23 with loss 0.07032568\n","Training at Epoch 390 iteration 24 with loss 0.06951084\n","Training at Epoch 390 iteration 25 with loss 0.068319246\n","Training at Epoch 390 iteration 26 with loss 0.067502\n","Training at Epoch 390 iteration 27 with loss 0.06791191\n","Training at Epoch 390 iteration 28 with loss 0.06952371\n","Training at Epoch 390 iteration 29 with loss 0.070734486\n","Training at Epoch 390 iteration 30 with loss 0.0691229\n","Training at Epoch 391 iteration 0 with loss 0.0699213\n","Training at Epoch 391 iteration 1 with loss 0.07072596\n","Training at Epoch 391 iteration 2 with loss 0.06749959\n","Training at Epoch 391 iteration 3 with loss 0.06710179\n","Training at Epoch 391 iteration 4 with loss 0.06790105\n","Training at Epoch 391 iteration 5 with loss 0.070739865\n","Training at Epoch 391 iteration 6 with loss 0.0687181\n","Training at Epoch 391 iteration 7 with loss 0.0703315\n","Training at Epoch 391 iteration 8 with loss 0.06992496\n","Training at Epoch 391 iteration 9 with loss 0.0691196\n","Training at Epoch 391 iteration 10 with loss 0.06954245\n","Training at Epoch 391 iteration 11 with loss 0.06952387\n","Training at Epoch 391 iteration 12 with loss 0.06870779\n","Training at Epoch 391 iteration 13 with loss 0.069524094\n","Training at Epoch 391 iteration 14 with loss 0.06749187\n","Training at Epoch 391 iteration 15 with loss 0.06790271\n","Training at Epoch 391 iteration 16 with loss 0.06952645\n","Training at Epoch 391 iteration 17 with loss 0.06709813\n","Training at Epoch 391 iteration 18 with loss 0.07076454\n","Training at Epoch 391 iteration 19 with loss 0.069927976\n","Training at Epoch 391 iteration 20 with loss 0.06871344\n","Training at Epoch 391 iteration 21 with loss 0.06830586\n","Training at Epoch 391 iteration 22 with loss 0.069131665\n","Training at Epoch 391 iteration 23 with loss 0.06953104\n","Training at Epoch 391 iteration 24 with loss 0.0695298\n","Training at Epoch 391 iteration 25 with loss 0.06953371\n","Training at Epoch 391 iteration 26 with loss 0.06872584\n","Training at Epoch 391 iteration 27 with loss 0.06870443\n","Training at Epoch 391 iteration 28 with loss 0.069132276\n","Training at Epoch 391 iteration 29 with loss 0.0703379\n","Training at Epoch 391 iteration 30 with loss 0.068296\n","Training at Epoch 392 iteration 0 with loss 0.06830186\n","Training at Epoch 392 iteration 1 with loss 0.06952416\n","Training at Epoch 392 iteration 2 with loss 0.0674933\n","Training at Epoch 392 iteration 3 with loss 0.06993121\n","Training at Epoch 392 iteration 4 with loss 0.06871636\n","Training at Epoch 392 iteration 5 with loss 0.07156195\n","Training at Epoch 392 iteration 6 with loss 0.06911833\n","Training at Epoch 392 iteration 7 with loss 0.06789701\n","Training at Epoch 392 iteration 8 with loss 0.06748114\n","Training at Epoch 392 iteration 9 with loss 0.069117635\n","Training at Epoch 392 iteration 10 with loss 0.06994191\n","Training at Epoch 392 iteration 11 with loss 0.06830688\n","Training at Epoch 392 iteration 12 with loss 0.069122\n","Training at Epoch 392 iteration 13 with loss 0.069537036\n","Training at Epoch 392 iteration 14 with loss 0.06911019\n","Training at Epoch 392 iteration 15 with loss 0.067879\n","Training at Epoch 392 iteration 16 with loss 0.06829594\n","Training at Epoch 392 iteration 17 with loss 0.06952468\n","Training at Epoch 392 iteration 18 with loss 0.069124304\n","Training at Epoch 392 iteration 19 with loss 0.06994645\n","Training at Epoch 392 iteration 20 with loss 0.068712994\n","Training at Epoch 392 iteration 21 with loss 0.07077609\n","Training at Epoch 392 iteration 22 with loss 0.06912099\n","Training at Epoch 392 iteration 23 with loss 0.06953455\n","Training at Epoch 392 iteration 24 with loss 0.06994667\n","Training at Epoch 392 iteration 25 with loss 0.069511786\n","Training at Epoch 392 iteration 26 with loss 0.06911555\n","Training at Epoch 392 iteration 27 with loss 0.06953488\n","Training at Epoch 392 iteration 28 with loss 0.06911929\n","Training at Epoch 392 iteration 29 with loss 0.06912679\n","Training at Epoch 392 iteration 30 with loss 0.06827853\n","Training at Epoch 393 iteration 0 with loss 0.06787505\n","Training at Epoch 393 iteration 1 with loss 0.07035466\n","Training at Epoch 393 iteration 2 with loss 0.06995043\n","Training at Epoch 393 iteration 3 with loss 0.06870727\n","Training at Epoch 393 iteration 4 with loss 0.06621112\n","Training at Epoch 393 iteration 5 with loss 0.068709865\n","Training at Epoch 393 iteration 6 with loss 0.06953569\n","Training at Epoch 393 iteration 7 with loss 0.069127895\n","Training at Epoch 393 iteration 8 with loss 0.068706915\n","Training at Epoch 393 iteration 9 with loss 0.06953398\n","Training at Epoch 393 iteration 10 with loss 0.06953831\n","Training at Epoch 393 iteration 11 with loss 0.06870471\n","Training at Epoch 393 iteration 12 with loss 0.069952026\n","Training at Epoch 393 iteration 13 with loss 0.06912898\n","Training at Epoch 393 iteration 14 with loss 0.06954254\n","Training at Epoch 393 iteration 15 with loss 0.06701887\n","Training at Epoch 393 iteration 16 with loss 0.068709575\n","Training at Epoch 393 iteration 17 with loss 0.06787002\n","Training at Epoch 393 iteration 18 with loss 0.06910445\n","Training at Epoch 393 iteration 19 with loss 0.069141254\n","Training at Epoch 393 iteration 20 with loss 0.0691247\n","Training at Epoch 393 iteration 21 with loss 0.07038307\n","Training at Epoch 393 iteration 22 with loss 0.06954068\n","Training at Epoch 393 iteration 23 with loss 0.070388496\n","Training at Epoch 393 iteration 24 with loss 0.06912083\n","Training at Epoch 393 iteration 25 with loss 0.06997176\n","Training at Epoch 393 iteration 26 with loss 0.068275616\n","Training at Epoch 393 iteration 27 with loss 0.06912936\n","Training at Epoch 393 iteration 28 with loss 0.067853905\n","Training at Epoch 393 iteration 29 with loss 0.071229145\n","Training at Epoch 393 iteration 30 with loss 0.06869536\n","Training at Epoch 394 iteration 0 with loss 0.067024566\n","Training at Epoch 394 iteration 1 with loss 0.068689466\n","Training at Epoch 394 iteration 2 with loss 0.06996232\n","Training at Epoch 394 iteration 3 with loss 0.06869712\n","Training at Epoch 394 iteration 4 with loss 0.067438856\n","Training at Epoch 394 iteration 5 with loss 0.070381954\n","Training at Epoch 394 iteration 6 with loss 0.069543555\n","Training at Epoch 394 iteration 7 with loss 0.06912418\n","Training at Epoch 394 iteration 8 with loss 0.06953484\n","Training at Epoch 394 iteration 9 with loss 0.06785016\n","Training at Epoch 394 iteration 10 with loss 0.06913569\n","Training at Epoch 394 iteration 11 with loss 0.06741567\n","Training at Epoch 394 iteration 12 with loss 0.06826873\n","Training at Epoch 394 iteration 13 with loss 0.071239546\n","Training at Epoch 394 iteration 14 with loss 0.06912038\n","Training at Epoch 394 iteration 15 with loss 0.06997933\n","Training at Epoch 394 iteration 16 with loss 0.06997243\n","Training at Epoch 394 iteration 17 with loss 0.06911932\n","Training at Epoch 394 iteration 18 with loss 0.0682662\n","Training at Epoch 394 iteration 19 with loss 0.06954152\n","Training at Epoch 394 iteration 20 with loss 0.06997259\n","Training at Epoch 394 iteration 21 with loss 0.068281755\n","Training at Epoch 394 iteration 22 with loss 0.06997227\n","Training at Epoch 394 iteration 23 with loss 0.06869525\n","Training at Epoch 394 iteration 24 with loss 0.06996095\n","Training at Epoch 394 iteration 25 with loss 0.06953663\n","Training at Epoch 394 iteration 26 with loss 0.06743654\n","Training at Epoch 394 iteration 27 with loss 0.07081896\n","Training at Epoch 394 iteration 28 with loss 0.067425326\n","Training at Epoch 394 iteration 29 with loss 0.070399046\n","Training at Epoch 394 iteration 30 with loss 0.06912486\n","Training at Epoch 395 iteration 0 with loss 0.07210748\n","Training at Epoch 395 iteration 1 with loss 0.069541745\n","Training at Epoch 395 iteration 2 with loss 0.06827887\n","Training at Epoch 395 iteration 3 with loss 0.06869327\n","Training at Epoch 395 iteration 4 with loss 0.07124965\n","Training at Epoch 395 iteration 5 with loss 0.066989504\n","Training at Epoch 395 iteration 6 with loss 0.06869577\n","Training at Epoch 395 iteration 7 with loss 0.06826694\n","Training at Epoch 395 iteration 8 with loss 0.069545195\n","Training at Epoch 395 iteration 9 with loss 0.06912271\n","Training at Epoch 395 iteration 10 with loss 0.06742867\n","Training at Epoch 395 iteration 11 with loss 0.06954244\n","Training at Epoch 395 iteration 12 with loss 0.068274245\n","Training at Epoch 395 iteration 13 with loss 0.06912127\n","Training at Epoch 395 iteration 14 with loss 0.06868368\n","Training at Epoch 395 iteration 15 with loss 0.067845866\n","Training at Epoch 395 iteration 16 with loss 0.066580236\n","Training at Epoch 395 iteration 17 with loss 0.06785746\n","Training at Epoch 395 iteration 18 with loss 0.06997638\n","Training at Epoch 395 iteration 19 with loss 0.06911019\n","Training at Epoch 395 iteration 20 with loss 0.06787125\n","Training at Epoch 395 iteration 21 with loss 0.0686791\n","Training at Epoch 395 iteration 22 with loss 0.0695481\n","Training at Epoch 395 iteration 23 with loss 0.06655064\n","Training at Epoch 395 iteration 24 with loss 0.069549575\n","Training at Epoch 395 iteration 25 with loss 0.07041165\n","Training at Epoch 395 iteration 26 with loss 0.072540745\n","Training at Epoch 395 iteration 27 with loss 0.071692154\n","Training at Epoch 395 iteration 28 with loss 0.06912816\n","Training at Epoch 395 iteration 29 with loss 0.07040461\n","Training at Epoch 395 iteration 30 with loss 0.069548614\n","Training at Epoch 396 iteration 0 with loss 0.06870769\n","Training at Epoch 396 iteration 1 with loss 0.06826848\n","Training at Epoch 396 iteration 2 with loss 0.06740697\n","Training at Epoch 396 iteration 3 with loss 0.071254134\n","Training at Epoch 396 iteration 4 with loss 0.06955327\n","Training at Epoch 396 iteration 5 with loss 0.06955512\n","Training at Epoch 396 iteration 6 with loss 0.06955361\n","Training at Epoch 396 iteration 7 with loss 0.06997064\n","Training at Epoch 396 iteration 8 with loss 0.068267085\n","Training at Epoch 396 iteration 9 with loss 0.06954453\n","Training at Epoch 396 iteration 10 with loss 0.067850396\n","Training at Epoch 396 iteration 11 with loss 0.06954412\n","Training at Epoch 396 iteration 12 with loss 0.06912114\n","Training at Epoch 396 iteration 13 with loss 0.07081758\n","Training at Epoch 396 iteration 14 with loss 0.06742696\n","Training at Epoch 396 iteration 15 with loss 0.06954236\n","Training at Epoch 396 iteration 16 with loss 0.069120325\n","Training at Epoch 396 iteration 17 with loss 0.06785087\n","Training at Epoch 396 iteration 18 with loss 0.06826887\n","Training at Epoch 396 iteration 19 with loss 0.06869803\n","Training at Epoch 396 iteration 20 with loss 0.06827963\n","Training at Epoch 396 iteration 21 with loss 0.06828754\n","Training at Epoch 396 iteration 22 with loss 0.06911494\n","Training at Epoch 396 iteration 23 with loss 0.06912247\n","Training at Epoch 396 iteration 24 with loss 0.07082516\n","Training at Epoch 396 iteration 25 with loss 0.06997442\n","Training at Epoch 396 iteration 26 with loss 0.06827962\n","Training at Epoch 396 iteration 27 with loss 0.06869762\n","Training at Epoch 396 iteration 28 with loss 0.07124486\n","Training at Epoch 396 iteration 29 with loss 0.068703994\n","Training at Epoch 396 iteration 30 with loss 0.069536686\n","Training at Epoch 397 iteration 0 with loss 0.06955069\n","Training at Epoch 397 iteration 1 with loss 0.06912362\n","Training at Epoch 397 iteration 2 with loss 0.06954234\n","Training at Epoch 397 iteration 3 with loss 0.06912665\n","Training at Epoch 397 iteration 4 with loss 0.06912477\n","Training at Epoch 397 iteration 5 with loss 0.06911953\n","Training at Epoch 397 iteration 6 with loss 0.06870355\n","Training at Epoch 397 iteration 7 with loss 0.067848\n","Training at Epoch 397 iteration 8 with loss 0.06996293\n","Training at Epoch 397 iteration 9 with loss 0.06911976\n","Training at Epoch 397 iteration 10 with loss 0.072476536\n","Training at Epoch 397 iteration 11 with loss 0.06743573\n","Training at Epoch 397 iteration 12 with loss 0.06996552\n","Training at Epoch 397 iteration 13 with loss 0.06871229\n","Training at Epoch 397 iteration 14 with loss 0.071224496\n","Training at Epoch 397 iteration 15 with loss 0.069118954\n","Training at Epoch 397 iteration 16 with loss 0.06995596\n","Training at Epoch 397 iteration 17 with loss 0.06743817\n","Training at Epoch 397 iteration 18 with loss 0.06785451\n","Training at Epoch 397 iteration 19 with loss 0.06828039\n","Training at Epoch 397 iteration 20 with loss 0.070790574\n","Training at Epoch 397 iteration 21 with loss 0.06953945\n","Training at Epoch 397 iteration 22 with loss 0.06870178\n","Training at Epoch 397 iteration 23 with loss 0.06704065\n","Training at Epoch 397 iteration 24 with loss 0.069951504\n","Training at Epoch 397 iteration 25 with loss 0.069958135\n","Training at Epoch 397 iteration 26 with loss 0.070369884\n","Training at Epoch 397 iteration 27 with loss 0.06828771\n","Training at Epoch 397 iteration 28 with loss 0.06828956\n","Training at Epoch 397 iteration 29 with loss 0.06703316\n","Training at Epoch 397 iteration 30 with loss 0.06827666\n","Training at Epoch 398 iteration 0 with loss 0.071198\n","Training at Epoch 398 iteration 1 with loss 0.070362\n","Training at Epoch 398 iteration 2 with loss 0.06870486\n","Training at Epoch 398 iteration 3 with loss 0.06828992\n","Training at Epoch 398 iteration 4 with loss 0.068704695\n","Training at Epoch 398 iteration 5 with loss 0.06746222\n","Training at Epoch 398 iteration 6 with loss 0.066630684\n","Training at Epoch 398 iteration 7 with loss 0.07119252\n","Training at Epoch 398 iteration 8 with loss 0.0678603\n","Training at Epoch 398 iteration 9 with loss 0.06870593\n","Training at Epoch 398 iteration 10 with loss 0.07119774\n","Training at Epoch 398 iteration 11 with loss 0.06744471\n","Training at Epoch 398 iteration 12 with loss 0.069531634\n","Training at Epoch 398 iteration 13 with loss 0.06870465\n","Training at Epoch 398 iteration 14 with loss 0.06828718\n","Training at Epoch 398 iteration 15 with loss 0.06953542\n","Training at Epoch 398 iteration 16 with loss 0.06787529\n","Training at Epoch 398 iteration 17 with loss 0.06703973\n","Training at Epoch 398 iteration 18 with loss 0.06828825\n","Training at Epoch 398 iteration 19 with loss 0.07036687\n","Training at Epoch 398 iteration 20 with loss 0.06996028\n","Training at Epoch 398 iteration 21 with loss 0.069955036\n","Training at Epoch 398 iteration 22 with loss 0.06828658\n","Training at Epoch 398 iteration 23 with loss 0.06870377\n","Training at Epoch 398 iteration 24 with loss 0.07037692\n","Training at Epoch 398 iteration 25 with loss 0.07036995\n","Training at Epoch 398 iteration 26 with loss 0.06870328\n","Training at Epoch 398 iteration 27 with loss 0.06953824\n","Training at Epoch 398 iteration 28 with loss 0.07079217\n","Training at Epoch 398 iteration 29 with loss 0.06827976\n","Training at Epoch 398 iteration 30 with loss 0.068705216\n","Training at Epoch 399 iteration 0 with loss 0.06871329\n","Training at Epoch 399 iteration 1 with loss 0.06953838\n","Training at Epoch 399 iteration 2 with loss 0.06870143\n","Training at Epoch 399 iteration 3 with loss 0.06870569\n","Training at Epoch 399 iteration 4 with loss 0.069534436\n","Training at Epoch 399 iteration 5 with loss 0.06995916\n","Training at Epoch 399 iteration 6 with loss 0.070375055\n","Training at Epoch 399 iteration 7 with loss 0.06828221\n","Training at Epoch 399 iteration 8 with loss 0.0682833\n","Training at Epoch 399 iteration 9 with loss 0.068283334\n","Training at Epoch 399 iteration 10 with loss 0.06996377\n","Training at Epoch 399 iteration 11 with loss 0.06912197\n","Training at Epoch 399 iteration 12 with loss 0.069535635\n","Training at Epoch 399 iteration 13 with loss 0.069115736\n","Training at Epoch 399 iteration 14 with loss 0.06828259\n","Training at Epoch 399 iteration 15 with loss 0.06954299\n","Training at Epoch 399 iteration 16 with loss 0.07079564\n","Training at Epoch 399 iteration 17 with loss 0.071624614\n","Training at Epoch 399 iteration 18 with loss 0.06786743\n","Training at Epoch 399 iteration 19 with loss 0.0687022\n","Training at Epoch 399 iteration 20 with loss 0.06953723\n","Training at Epoch 399 iteration 21 with loss 0.070362285\n","Training at Epoch 399 iteration 22 with loss 0.06829013\n","Training at Epoch 399 iteration 23 with loss 0.06870474\n","Training at Epoch 399 iteration 24 with loss 0.068703786\n","Training at Epoch 399 iteration 25 with loss 0.06828764\n","Training at Epoch 399 iteration 26 with loss 0.07160303\n","Training at Epoch 399 iteration 27 with loss 0.06828922\n","Training at Epoch 399 iteration 28 with loss 0.067460954\n","Training at Epoch 399 iteration 29 with loss 0.06871054\n","Training at Epoch 399 iteration 30 with loss 0.067472234\n","Training at Epoch 400 iteration 0 with loss 0.067885\n","Training at Epoch 400 iteration 1 with loss 0.06994043\n","Training at Epoch 400 iteration 2 with loss 0.07036109\n","Training at Epoch 400 iteration 3 with loss 0.067878455\n","Training at Epoch 400 iteration 4 with loss 0.06953216\n","Training at Epoch 400 iteration 5 with loss 0.069121145\n","Training at Epoch 400 iteration 6 with loss 0.06911759\n","Training at Epoch 400 iteration 7 with loss 0.070345834\n","Training at Epoch 400 iteration 8 with loss 0.06829813\n","Training at Epoch 400 iteration 9 with loss 0.069123335\n","Training at Epoch 400 iteration 10 with loss 0.06789064\n","Training at Epoch 400 iteration 11 with loss 0.0687073\n","Training at Epoch 400 iteration 12 with loss 0.06789637\n","Training at Epoch 400 iteration 13 with loss 0.07034484\n","Training at Epoch 400 iteration 14 with loss 0.06871647\n","Training at Epoch 400 iteration 15 with loss 0.06870793\n","Training at Epoch 400 iteration 16 with loss 0.06788732\n","Training at Epoch 400 iteration 17 with loss 0.070767894\n","Training at Epoch 400 iteration 18 with loss 0.069119975\n","Training at Epoch 400 iteration 19 with loss 0.06912056\n","Training at Epoch 400 iteration 20 with loss 0.06911655\n","Training at Epoch 400 iteration 21 with loss 0.06788336\n","Training at Epoch 400 iteration 22 with loss 0.070356034\n","Training at Epoch 400 iteration 23 with loss 0.06912036\n","Training at Epoch 400 iteration 24 with loss 0.06871186\n","Training at Epoch 400 iteration 25 with loss 0.0707648\n","Training at Epoch 400 iteration 26 with loss 0.06911957\n","Training at Epoch 400 iteration 27 with loss 0.068298325\n","Training at Epoch 400 iteration 28 with loss 0.06870884\n","Training at Epoch 400 iteration 29 with loss 0.070354946\n","Training at Epoch 400 iteration 30 with loss 0.06870811\n","Training at Epoch 401 iteration 0 with loss 0.06953059\n","Training at Epoch 401 iteration 1 with loss 0.06994146\n","Training at Epoch 401 iteration 2 with loss 0.072408125\n","Training at Epoch 401 iteration 3 with loss 0.06871098\n","Training at Epoch 401 iteration 4 with loss 0.06665611\n","Training at Epoch 401 iteration 5 with loss 0.0687087\n","Training at Epoch 401 iteration 6 with loss 0.06870895\n","Training at Epoch 401 iteration 7 with loss 0.06952993\n","Training at Epoch 401 iteration 8 with loss 0.067479104\n","Training at Epoch 401 iteration 9 with loss 0.06829764\n","Training at Epoch 401 iteration 10 with loss 0.06788518\n","Training at Epoch 401 iteration 11 with loss 0.06870712\n","Training at Epoch 401 iteration 12 with loss 0.06870692\n","Training at Epoch 401 iteration 13 with loss 0.0703517\n","Training at Epoch 401 iteration 14 with loss 0.07036039\n","Training at Epoch 401 iteration 15 with loss 0.06870439\n","Training at Epoch 401 iteration 16 with loss 0.06871174\n","Training at Epoch 401 iteration 17 with loss 0.06953268\n","Training at Epoch 401 iteration 18 with loss 0.069531485\n","Training at Epoch 401 iteration 19 with loss 0.070766166\n","Training at Epoch 401 iteration 20 with loss 0.07117001\n","Training at Epoch 401 iteration 21 with loss 0.0670694\n","Training at Epoch 401 iteration 22 with loss 0.07117391\n","Training at Epoch 401 iteration 23 with loss 0.0662487\n","Training at Epoch 401 iteration 24 with loss 0.06911974\n","Training at Epoch 401 iteration 25 with loss 0.06952949\n","Training at Epoch 401 iteration 26 with loss 0.0695326\n","Training at Epoch 401 iteration 27 with loss 0.06870939\n","Training at Epoch 401 iteration 28 with loss 0.06912075\n","Training at Epoch 401 iteration 29 with loss 0.06912339\n","Training at Epoch 401 iteration 30 with loss 0.067899354\n","Training at Epoch 402 iteration 0 with loss 0.06953084\n","Training at Epoch 402 iteration 1 with loss 0.06789277\n","Training at Epoch 402 iteration 2 with loss 0.06953468\n","Training at Epoch 402 iteration 3 with loss 0.06707485\n","Training at Epoch 402 iteration 4 with loss 0.06952633\n","Training at Epoch 402 iteration 5 with loss 0.067894414\n","Training at Epoch 402 iteration 6 with loss 0.06871171\n","Training at Epoch 402 iteration 7 with loss 0.07116656\n","Training at Epoch 402 iteration 8 with loss 0.06789148\n","Training at Epoch 402 iteration 9 with loss 0.06788678\n","Training at Epoch 402 iteration 10 with loss 0.069119945\n","Training at Epoch 402 iteration 11 with loss 0.07160367\n","Training at Epoch 402 iteration 12 with loss 0.06912162\n","Training at Epoch 402 iteration 13 with loss 0.07036022\n","Training at Epoch 402 iteration 14 with loss 0.06911704\n","Training at Epoch 402 iteration 15 with loss 0.067881055\n","Training at Epoch 402 iteration 16 with loss 0.069119886\n","Training at Epoch 402 iteration 17 with loss 0.06911713\n","Training at Epoch 402 iteration 18 with loss 0.06952755\n","Training at Epoch 402 iteration 19 with loss 0.0691216\n","Training at Epoch 402 iteration 20 with loss 0.06829033\n","Training at Epoch 402 iteration 21 with loss 0.06994658\n","Training at Epoch 402 iteration 22 with loss 0.0691203\n","Training at Epoch 402 iteration 23 with loss 0.06829627\n","Training at Epoch 402 iteration 24 with loss 0.06912062\n","Training at Epoch 402 iteration 25 with loss 0.068293594\n","Training at Epoch 402 iteration 26 with loss 0.06912197\n","Training at Epoch 402 iteration 27 with loss 0.06829192\n","Training at Epoch 402 iteration 28 with loss 0.07036112\n","Training at Epoch 402 iteration 29 with loss 0.06953223\n","Training at Epoch 402 iteration 30 with loss 0.07119417\n","Training at Epoch 403 iteration 0 with loss 0.0703628\n","Training at Epoch 403 iteration 1 with loss 0.06704594\n","Training at Epoch 403 iteration 2 with loss 0.06871266\n","Training at Epoch 403 iteration 3 with loss 0.06911562\n","Training at Epoch 403 iteration 4 with loss 0.07036479\n","Training at Epoch 403 iteration 5 with loss 0.071609\n","Training at Epoch 403 iteration 6 with loss 0.06912257\n","Training at Epoch 403 iteration 7 with loss 0.06829224\n","Training at Epoch 403 iteration 8 with loss 0.06787507\n","Training at Epoch 403 iteration 9 with loss 0.06994928\n","Training at Epoch 403 iteration 10 with loss 0.068709604\n","Training at Epoch 403 iteration 11 with loss 0.068707965\n","Training at Epoch 403 iteration 12 with loss 0.069531925\n","Training at Epoch 403 iteration 13 with loss 0.06870365\n","Training at Epoch 403 iteration 14 with loss 0.06994472\n","Training at Epoch 403 iteration 15 with loss 0.06870748\n","Training at Epoch 403 iteration 16 with loss 0.07077247\n","Training at Epoch 403 iteration 17 with loss 0.070769295\n","Training at Epoch 403 iteration 18 with loss 0.06788133\n","Training at Epoch 403 iteration 19 with loss 0.07118093\n","Training at Epoch 403 iteration 20 with loss 0.06505969\n","Training at Epoch 403 iteration 21 with loss 0.06952544\n","Training at Epoch 403 iteration 22 with loss 0.070741266\n","Training at Epoch 403 iteration 23 with loss 0.06709136\n","Training at Epoch 403 iteration 24 with loss 0.069121234\n","Training at Epoch 403 iteration 25 with loss 0.06993166\n","Training at Epoch 403 iteration 26 with loss 0.06952877\n","Training at Epoch 403 iteration 27 with loss 0.06790157\n","Training at Epoch 403 iteration 28 with loss 0.06830834\n","Training at Epoch 403 iteration 29 with loss 0.0687138\n","Training at Epoch 403 iteration 30 with loss 0.06911874\n","Training at Epoch 404 iteration 0 with loss 0.06993054\n","Training at Epoch 404 iteration 1 with loss 0.06830855\n","Training at Epoch 404 iteration 2 with loss 0.069118686\n","Training at Epoch 404 iteration 3 with loss 0.06749501\n","Training at Epoch 404 iteration 4 with loss 0.071553335\n","Training at Epoch 404 iteration 5 with loss 0.07074164\n","Training at Epoch 404 iteration 6 with loss 0.06992951\n","Training at Epoch 404 iteration 7 with loss 0.06952414\n","Training at Epoch 404 iteration 8 with loss 0.06871228\n","Training at Epoch 404 iteration 9 with loss 0.06993087\n","Training at Epoch 404 iteration 10 with loss 0.0695206\n","Training at Epoch 404 iteration 11 with loss 0.06911944\n","Training at Epoch 404 iteration 12 with loss 0.069519624\n","Training at Epoch 404 iteration 13 with loss 0.069921926\n","Training at Epoch 404 iteration 14 with loss 0.068722315\n","Training at Epoch 404 iteration 15 with loss 0.06871914\n","Training at Epoch 404 iteration 16 with loss 0.06872032\n","Training at Epoch 404 iteration 17 with loss 0.068718985\n","Training at Epoch 404 iteration 18 with loss 0.06831779\n","Training at Epoch 404 iteration 19 with loss 0.067116015\n","Training at Epoch 404 iteration 20 with loss 0.068720266\n","Training at Epoch 404 iteration 21 with loss 0.06872077\n","Training at Epoch 404 iteration 22 with loss 0.06712657\n","Training at Epoch 404 iteration 23 with loss 0.06991505\n","Training at Epoch 404 iteration 24 with loss 0.07031855\n","Training at Epoch 404 iteration 25 with loss 0.069917336\n","Training at Epoch 404 iteration 26 with loss 0.06792043\n","Training at Epoch 404 iteration 27 with loss 0.06831701\n","Training at Epoch 404 iteration 28 with loss 0.06911936\n","Training at Epoch 404 iteration 29 with loss 0.06911509\n","Training at Epoch 404 iteration 30 with loss 0.069922976\n","Training at Epoch 405 iteration 0 with loss 0.07111989\n","Training at Epoch 405 iteration 1 with loss 0.068319224\n","Training at Epoch 405 iteration 2 with loss 0.0699197\n","Training at Epoch 405 iteration 3 with loss 0.07111732\n","Training at Epoch 405 iteration 4 with loss 0.06912259\n","Training at Epoch 405 iteration 5 with loss 0.06872253\n","Training at Epoch 405 iteration 6 with loss 0.070320085\n","Training at Epoch 405 iteration 7 with loss 0.0687191\n","Training at Epoch 405 iteration 8 with loss 0.06952013\n","Training at Epoch 405 iteration 9 with loss 0.06831922\n","Training at Epoch 405 iteration 10 with loss 0.07070547\n","Training at Epoch 405 iteration 11 with loss 0.06832796\n","Training at Epoch 405 iteration 12 with loss 0.07030694\n","Training at Epoch 405 iteration 13 with loss 0.0699141\n","Training at Epoch 405 iteration 14 with loss 0.06674561\n","Training at Epoch 405 iteration 15 with loss 0.07030637\n","Training at Epoch 405 iteration 16 with loss 0.06912063\n","Training at Epoch 405 iteration 17 with loss 0.070700556\n","Training at Epoch 405 iteration 18 with loss 0.067542\n","Training at Epoch 405 iteration 19 with loss 0.06753676\n","Training at Epoch 405 iteration 20 with loss 0.06951063\n","Training at Epoch 405 iteration 21 with loss 0.068336286\n","Training at Epoch 405 iteration 22 with loss 0.0683351\n","Training at Epoch 405 iteration 23 with loss 0.067552924\n","Training at Epoch 405 iteration 24 with loss 0.06911769\n","Training at Epoch 405 iteration 25 with loss 0.069119275\n","Training at Epoch 405 iteration 26 with loss 0.06990268\n","Training at Epoch 405 iteration 27 with loss 0.069118686\n","Training at Epoch 405 iteration 28 with loss 0.06755287\n","Training at Epoch 405 iteration 29 with loss 0.06833752\n","Training at Epoch 405 iteration 30 with loss 0.06911893\n","Training at Epoch 406 iteration 0 with loss 0.067154855\n","Training at Epoch 406 iteration 1 with loss 0.069119275\n","Training at Epoch 406 iteration 2 with loss 0.07068909\n","Training at Epoch 406 iteration 3 with loss 0.06911977\n","Training at Epoch 406 iteration 4 with loss 0.0691192\n","Training at Epoch 406 iteration 5 with loss 0.07029722\n","Training at Epoch 406 iteration 6 with loss 0.07147487\n","Training at Epoch 406 iteration 7 with loss 0.070300534\n","Training at Epoch 406 iteration 8 with loss 0.06951177\n","Training at Epoch 406 iteration 9 with loss 0.06912575\n","Training at Epoch 406 iteration 10 with loss 0.067956984\n","Training at Epoch 406 iteration 11 with loss 0.06989561\n","Training at Epoch 406 iteration 12 with loss 0.06756624\n","Training at Epoch 406 iteration 13 with loss 0.06640097\n","Training at Epoch 406 iteration 14 with loss 0.0667897\n","Training at Epoch 406 iteration 15 with loss 0.06911928\n","Training at Epoch 406 iteration 16 with loss 0.069895945\n","Training at Epoch 406 iteration 17 with loss 0.0698965\n","Training at Epoch 406 iteration 18 with loss 0.0698929\n","Training at Epoch 406 iteration 19 with loss 0.069123544\n","Training at Epoch 406 iteration 20 with loss 0.06911875\n","Training at Epoch 406 iteration 21 with loss 0.06872789\n","Training at Epoch 406 iteration 22 with loss 0.06990127\n","Training at Epoch 406 iteration 23 with loss 0.06834195\n","Training at Epoch 406 iteration 24 with loss 0.070287995\n","Training at Epoch 406 iteration 25 with loss 0.06795511\n","Training at Epoch 406 iteration 26 with loss 0.06911908\n","Training at Epoch 406 iteration 27 with loss 0.0679492\n","Training at Epoch 406 iteration 28 with loss 0.07106714\n","Training at Epoch 406 iteration 29 with loss 0.06872963\n","Training at Epoch 406 iteration 30 with loss 0.06872927\n","Training at Epoch 407 iteration 0 with loss 0.06990205\n","Training at Epoch 407 iteration 1 with loss 0.06990069\n","Training at Epoch 407 iteration 2 with loss 0.069118515\n","Training at Epoch 407 iteration 3 with loss 0.07029027\n","Training at Epoch 407 iteration 4 with loss 0.06834197\n","Training at Epoch 407 iteration 5 with loss 0.06872822\n","Training at Epoch 407 iteration 6 with loss 0.07029261\n","Training at Epoch 407 iteration 7 with loss 0.06872914\n","Training at Epoch 407 iteration 8 with loss 0.06911433\n","Training at Epoch 407 iteration 9 with loss 0.06716573\n","Training at Epoch 407 iteration 10 with loss 0.06872641\n","Training at Epoch 407 iteration 11 with loss 0.06872754\n","Training at Epoch 407 iteration 12 with loss 0.068335965\n","Training at Epoch 407 iteration 13 with loss 0.0706868\n","Training at Epoch 407 iteration 14 with loss 0.06951\n","Training at Epoch 407 iteration 15 with loss 0.06951353\n","Training at Epoch 407 iteration 16 with loss 0.070686124\n","Training at Epoch 407 iteration 17 with loss 0.06911904\n","Training at Epoch 407 iteration 18 with loss 0.06833581\n","Training at Epoch 407 iteration 19 with loss 0.06755098\n","Training at Epoch 407 iteration 20 with loss 0.068726465\n","Training at Epoch 407 iteration 21 with loss 0.069904916\n","Training at Epoch 407 iteration 22 with loss 0.069511615\n","Training at Epoch 407 iteration 23 with loss 0.0683336\n","Training at Epoch 407 iteration 24 with loss 0.06990632\n","Training at Epoch 407 iteration 25 with loss 0.06872732\n","Training at Epoch 407 iteration 26 with loss 0.06833395\n","Training at Epoch 407 iteration 27 with loss 0.070297495\n","Training at Epoch 407 iteration 28 with loss 0.0691187\n"]}],"source":["print('--- Go for Training ---')\n","torch.backends.cudnn.benchmark = True\n","for epo in range(train_epoch):\n","    model.train()\n","    for i, (d, p, d_mask, p_mask, label) in enumerate(training_generator):\n","        \n","        label = Variable(torch.from_numpy(np.array(label)).float())\n","        if use_cuda:\n","            d = d.cuda()\n","            p = p.cuda()\n","            d_mask = d_mask.cuda()\n","            p_mask = p_mask.cuda()\n","            label = label.cuda()\n","\n","        \n","        score = model(d, p, d_mask, p_mask)\n","        loss_fct = torch.nn.BCELoss()\n","        m = torch.nn.Sigmoid()\n","        n = torch.squeeze(m(score))\n","        # print(d.isnan().any())\n","        # print(p.isnan().any())\n","        # print(p)\n","        \n","        loss = loss_fct(n, label)\n","        loss = loss / accumulation_steps\n","        loss_history.append(loss)\n","        loss.backward()\n","\n","        # only updates weights after gradients are accumulated\n","        if (i+1) % accumulation_steps == 0:\n","          opt.step()\n","          opt.zero_grad()\n","          \n","  \n","        print('Training at Epoch ' + str(epo + 1) + ' iteration ' + str(i) + ' with loss ' + str(loss.cpu().detach().numpy()))\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4iLFn0DLBI-"},"outputs":[],"source":["lh = list(filter(lambda x: x < 1, loss_history))\n","plt.plot(lh)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45872,"status":"ok","timestamp":1646960990582,"user":{"displayName":"cong liu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"05614906434306765320"},"user_tz":480},"id":"0gognPLspBsy","outputId":"b54890d0-4cf8-42d4-93ae-2ebc0f6158e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Let's use cpu!\n","--- Data Preparation ---\n","tensor([0.5449, 0.5420, 0.5875, 0.5028, 0.5609, 0.4888, 0.5067, 0.4737, 0.4319,\n","        0.5757, 0.5445, 0.4892, 0.5802, 0.5591, 0.4825, 0.5260, 0.4977, 0.4802,\n","        0.4988, 0.4939, 0.5904, 0.5076, 0.5060, 0.5795, 0.4732, 0.5112, 0.5063,\n","        0.4594, 0.5629, 0.4810, 0.4885, 0.5361], grad_fn=<SqueezeBackward0>)\n"]}],"source":["#@title unit testing to check if the variables change\n","# from torchtest.torchtest.torchtest import assert_vars_change\n","config = BIN_config_DBPE()\n","\n","lr = 5e-6\n","BATCH_SIZE = config['batch_size']\n","train_epoch = 20\n","max_d = config['max_dna_seq']\n","max_p = config['max_protein_seq']\n","\n","loss_history = []\n","\n","model = BIN_Interaction_Flat(**config)\n","\n","if use_cuda:\n","  model = model.cuda()\n","\n","if torch.cuda.device_count() > 1:\n","  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n","  model = nn.DataParallel(model, dim = 0)\n","elif torch.cuda.device_count() < 1:\n","  print(\"Let's use cpu!\")\n","\n","opt = torch.optim.Adam(model.parameters(), lr = lr)\n","#opt = torch.optim.SGD(model.parameters(), lr = lr, momentum=0.9)\n","\n","\n","\n","\n","\n","\n","# check if variables change\n","assert_vars_change(\n","    model=model,\n","    loss_fn=loss_fct,\n","    optim=opt,\n","    batch=next(iter(training_generator)),\n","    device = \"cpu\")\n","## the result shows that variables do change, which is normal\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8fFXWwLEKsIw"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jm_p6FSpJ-V5"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fe8fiwHpg5h8"},"outputs":[],"source":[""]}],"metadata":{"colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Copy of example_gpu.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}